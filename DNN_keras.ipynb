{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DNN_재미있눙 :).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPaYP+8b+qVDVVDeHjHUz6M",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minh-A/Keras_DeepLearning/blob/main/DNN_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNRoJNFIJWS8"
      },
      "source": [
        "# 데이터 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VUZuOH0MpZe",
        "outputId": "a487e970-bf56-43b9-b3a7-cefae174a9b7"
      },
      "source": [
        "# 구글드라이브 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-8ZaYlfMp5A"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time, datetime  # 시간관련 패키지\n",
        "# 전체 데이터 표준화\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import model_selection as ms\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy\n",
        "#시간을 계산하는 함수\n",
        "def clock(start):\n",
        "    sec = time.time() - start #현재시간 - 시스템초기시간\n",
        "    times = str(datetime.timedelta(seconds = sec)).split(\".\") # 시간:분:초로 변환\n",
        "    times = times[0]\n",
        "    return times"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYp9bwLzMp70"
      },
      "source": [
        "cf = pd.read_csv(\"/content/drive/Shared drives/데이터분석 및 인공지능 연구실 드라이브/1043-RP/Data/cf.csv\")\n",
        "eo = pd.read_csv(\"/content/drive/Shared drives/데이터분석 및 인공지능 연구실 드라이브/1043-RP/Data/eo.csv\")\n",
        "fwc = pd.read_csv(\"/content/drive/Shared drives/데이터분석 및 인공지능 연구실 드라이브/1043-RP/Data/fwc.csv\")\n",
        "fwe = pd.read_csv(\"/content/drive/Shared drives/데이터분석 및 인공지능 연구실 드라이브/1043-RP/Data/fwe.csv\")\n",
        "nc = pd.read_csv(\"/content/drive/Shared drives/데이터분석 및 인공지능 연구실 드라이브/1043-RP/Data/nc.csv\")\n",
        "normal = pd.read_csv(\"/content/drive/Shared drives/데이터분석 및 인공지능 연구실 드라이브/1043-RP/Data/normal.csv\")\n",
        "rl = pd.read_csv(\"/content/drive/Shared drives/데이터분석 및 인공지능 연구실 드라이브/1043-RP/Data/rl.csv\")\n",
        "ro = pd.read_csv(\"/content/drive/Shared drives/데이터분석 및 인공지능 연구실 드라이브/1043-RP/Data/ro.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcHP47HKMp-O"
      },
      "source": [
        "normal['rorl'] = 0\n",
        "normal['detect'] = 0\n",
        "normal['target'] = 1\n",
        "\n",
        "cf['rorl'] = 0\n",
        "cf['detect'] = 1\n",
        "cf['target'] = 2\n",
        "\n",
        "ro['rorl'] = 3\n",
        "ro['detect'] = 1\n",
        "ro['target'] = 3\n",
        "\n",
        "eo['rorl'] = 0\n",
        "eo['detect'] = 1\n",
        "eo['target'] = 4\n",
        "\n",
        "fwc['rorl'] = 0\n",
        "fwc['detect'] = 1\n",
        "fwc['target'] = 5\n",
        "\n",
        "nc['rorl'] = 0\n",
        "nc['detect'] = 1\n",
        "nc['target'] = 6\n",
        "\n",
        "fwe['rorl'] = 0\n",
        "fwe['detect'] = 1\n",
        "fwe['target'] = 7\n",
        "\n",
        "rl['rorl'] = 8\n",
        "rl['detect'] = 1\n",
        "rl['target'] = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "klspxq_MMqAo",
        "outputId": "2dc9c759-c408-4363-898a-1c1fbf671108"
      },
      "source": [
        "data = pd.concat([normal,cf,ro,eo,fwc,nc,fwe, rl], axis=0)\n",
        "#data = data.drop(['Unnamed: 0','Time','cf','eo','fwc','fwe','nc','rl','ro'],axis=1)\n",
        "data = data.drop(['Time'],axis=1)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TWE_set</th>\n",
              "      <th>TEI</th>\n",
              "      <th>TWEI</th>\n",
              "      <th>TEO</th>\n",
              "      <th>TWEO</th>\n",
              "      <th>TCI</th>\n",
              "      <th>TWCI</th>\n",
              "      <th>TCO</th>\n",
              "      <th>TWCO</th>\n",
              "      <th>TSI</th>\n",
              "      <th>TSO</th>\n",
              "      <th>TBI</th>\n",
              "      <th>TBO</th>\n",
              "      <th>Cond Tons</th>\n",
              "      <th>Cooling Tons</th>\n",
              "      <th>Shared Cond Tons</th>\n",
              "      <th>Cond Energy Balance</th>\n",
              "      <th>Evap Tons</th>\n",
              "      <th>Shared Evap Tons</th>\n",
              "      <th>Building Tons</th>\n",
              "      <th>Evap Energy Balance</th>\n",
              "      <th>kW</th>\n",
              "      <th>COP</th>\n",
              "      <th>kW/Ton</th>\n",
              "      <th>FWC</th>\n",
              "      <th>FWE</th>\n",
              "      <th>TEA</th>\n",
              "      <th>TCA</th>\n",
              "      <th>TRE</th>\n",
              "      <th>PRE</th>\n",
              "      <th>TRC</th>\n",
              "      <th>PRC</th>\n",
              "      <th>TRC_sub</th>\n",
              "      <th>T_suc</th>\n",
              "      <th>Tsh_suc</th>\n",
              "      <th>TR_dis</th>\n",
              "      <th>Tsh_dis</th>\n",
              "      <th>P_lift</th>\n",
              "      <th>Amps</th>\n",
              "      <th>RLA%</th>\n",
              "      <th>Heat Balance (kW)</th>\n",
              "      <th>Heat Balance%</th>\n",
              "      <th>Tolerance%</th>\n",
              "      <th>Unit Status</th>\n",
              "      <th>Active Fault</th>\n",
              "      <th>TO_sump</th>\n",
              "      <th>TO_feed</th>\n",
              "      <th>PO_feed</th>\n",
              "      <th>PO_net</th>\n",
              "      <th>TWCD</th>\n",
              "      <th>TWED</th>\n",
              "      <th>VSS</th>\n",
              "      <th>VSL</th>\n",
              "      <th>VH</th>\n",
              "      <th>VM</th>\n",
              "      <th>VC</th>\n",
              "      <th>VE</th>\n",
              "      <th>VW</th>\n",
              "      <th>TWI</th>\n",
              "      <th>TWO</th>\n",
              "      <th>THI</th>\n",
              "      <th>THO</th>\n",
              "      <th>FWW</th>\n",
              "      <th>FWH</th>\n",
              "      <th>FWB</th>\n",
              "      <th>rorl</th>\n",
              "      <th>detect</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>62.28</td>\n",
              "      <td>62.3</td>\n",
              "      <td>63.20</td>\n",
              "      <td>63.1</td>\n",
              "      <td>64.28</td>\n",
              "      <td>64.2</td>\n",
              "      <td>65.14</td>\n",
              "      <td>65.0</td>\n",
              "      <td>66.18</td>\n",
              "      <td>67.78</td>\n",
              "      <td>64.75</td>\n",
              "      <td>65.76</td>\n",
              "      <td>1.383000e-46</td>\n",
              "      <td>1.671000e-46</td>\n",
              "      <td>2.580000e-46</td>\n",
              "      <td>5.634000e-46</td>\n",
              "      <td>1.196000e-46</td>\n",
              "      <td>1.995000e-46</td>\n",
              "      <td>1.308000e-46</td>\n",
              "      <td>4.499000e-46</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>4.206000e-46</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>61.5</td>\n",
              "      <td>63.1</td>\n",
              "      <td>61.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>80.8</td>\n",
              "      <td>17.8</td>\n",
              "      <td>71.1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>23.27</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>103.7</td>\n",
              "      <td>85.3</td>\n",
              "      <td>57.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.23</td>\n",
              "      <td>70.15</td>\n",
              "      <td>67.01</td>\n",
              "      <td>71.33</td>\n",
              "      <td>8.145000e-46</td>\n",
              "      <td>7.262000e-46</td>\n",
              "      <td>8.761000e-46</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>62.24</td>\n",
              "      <td>62.3</td>\n",
              "      <td>63.20</td>\n",
              "      <td>63.1</td>\n",
              "      <td>64.28</td>\n",
              "      <td>64.2</td>\n",
              "      <td>65.14</td>\n",
              "      <td>65.1</td>\n",
              "      <td>66.18</td>\n",
              "      <td>67.78</td>\n",
              "      <td>64.71</td>\n",
              "      <td>65.76</td>\n",
              "      <td>1.383000e-46</td>\n",
              "      <td>1.671000e-46</td>\n",
              "      <td>2.580000e-46</td>\n",
              "      <td>5.634000e-46</td>\n",
              "      <td>1.249000e-46</td>\n",
              "      <td>1.942000e-46</td>\n",
              "      <td>1.361000e-46</td>\n",
              "      <td>4.552000e-46</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>4.391000e-46</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>63.1</td>\n",
              "      <td>61.7</td>\n",
              "      <td>63.1</td>\n",
              "      <td>61.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>80.8</td>\n",
              "      <td>17.8</td>\n",
              "      <td>70.9</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>23.27</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>103.5</td>\n",
              "      <td>85.4</td>\n",
              "      <td>57.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>65.23</td>\n",
              "      <td>70.19</td>\n",
              "      <td>67.01</td>\n",
              "      <td>71.29</td>\n",
              "      <td>8.078000e-46</td>\n",
              "      <td>7.627000e-46</td>\n",
              "      <td>9.552000e-46</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50</td>\n",
              "      <td>62.24</td>\n",
              "      <td>63.3</td>\n",
              "      <td>63.16</td>\n",
              "      <td>63.3</td>\n",
              "      <td>64.32</td>\n",
              "      <td>65.3</td>\n",
              "      <td>65.10</td>\n",
              "      <td>64.7</td>\n",
              "      <td>66.14</td>\n",
              "      <td>67.82</td>\n",
              "      <td>64.75</td>\n",
              "      <td>65.76</td>\n",
              "      <td>5.290000e+00</td>\n",
              "      <td>7.071000e+00</td>\n",
              "      <td>1.147000e+01</td>\n",
              "      <td>2.383000e+01</td>\n",
              "      <td>5.175000e+00</td>\n",
              "      <td>8.861000e+00</td>\n",
              "      <td>5.659000e+00</td>\n",
              "      <td>1.969000e+01</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>1.820000e+01</td>\n",
              "      <td>3.249000e-46</td>\n",
              "      <td>1.636000e+02</td>\n",
              "      <td>1.340000e+02</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>63.0</td>\n",
              "      <td>61.5</td>\n",
              "      <td>65.2</td>\n",
              "      <td>64.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>80.8</td>\n",
              "      <td>17.8</td>\n",
              "      <td>71.1</td>\n",
              "      <td>5.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.4048</td>\n",
              "      <td>-2.176</td>\n",
              "      <td>23.27</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>103.3</td>\n",
              "      <td>85.3</td>\n",
              "      <td>56.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>65.27</td>\n",
              "      <td>70.15</td>\n",
              "      <td>67.87</td>\n",
              "      <td>70.96</td>\n",
              "      <td>3.474000e+01</td>\n",
              "      <td>4.389000e+01</td>\n",
              "      <td>3.720000e+01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50</td>\n",
              "      <td>62.52</td>\n",
              "      <td>64.8</td>\n",
              "      <td>63.25</td>\n",
              "      <td>63.7</td>\n",
              "      <td>64.57</td>\n",
              "      <td>65.1</td>\n",
              "      <td>65.10</td>\n",
              "      <td>65.4</td>\n",
              "      <td>66.14</td>\n",
              "      <td>67.49</td>\n",
              "      <td>64.71</td>\n",
              "      <td>65.64</td>\n",
              "      <td>5.547000e+00</td>\n",
              "      <td>1.085000e+01</td>\n",
              "      <td>1.417000e+01</td>\n",
              "      <td>3.056000e+01</td>\n",
              "      <td>6.132000e+00</td>\n",
              "      <td>1.243000e+01</td>\n",
              "      <td>7.909000e+00</td>\n",
              "      <td>2.647000e+01</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>2.156000e+01</td>\n",
              "      <td>2.742000e-46</td>\n",
              "      <td>2.510000e+02</td>\n",
              "      <td>2.037000e+02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.7</td>\n",
              "      <td>63.8</td>\n",
              "      <td>64.5</td>\n",
              "      <td>63.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81.2</td>\n",
              "      <td>16.7</td>\n",
              "      <td>71.1</td>\n",
              "      <td>6.6</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0570</td>\n",
              "      <td>10.550</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>103.1</td>\n",
              "      <td>85.3</td>\n",
              "      <td>59.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>65.27</td>\n",
              "      <td>69.95</td>\n",
              "      <td>68.40</td>\n",
              "      <td>68.87</td>\n",
              "      <td>5.563000e+01</td>\n",
              "      <td>1.898000e+02</td>\n",
              "      <td>3.077000e+01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>63.01</td>\n",
              "      <td>64.4</td>\n",
              "      <td>63.49</td>\n",
              "      <td>64.7</td>\n",
              "      <td>64.69</td>\n",
              "      <td>64.5</td>\n",
              "      <td>65.14</td>\n",
              "      <td>64.7</td>\n",
              "      <td>65.89</td>\n",
              "      <td>66.75</td>\n",
              "      <td>64.55</td>\n",
              "      <td>65.31</td>\n",
              "      <td>4.873000e+00</td>\n",
              "      <td>8.157000e+00</td>\n",
              "      <td>9.385000e+00</td>\n",
              "      <td>2.242000e+01</td>\n",
              "      <td>4.236000e+00</td>\n",
              "      <td>9.377000e+00</td>\n",
              "      <td>6.825000e+00</td>\n",
              "      <td>2.044000e+01</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>1.489000e+01</td>\n",
              "      <td>3.970000e-46</td>\n",
              "      <td>2.608000e+02</td>\n",
              "      <td>2.133000e+02</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>63.9</td>\n",
              "      <td>62.7</td>\n",
              "      <td>64.1</td>\n",
              "      <td>63.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81.3</td>\n",
              "      <td>17.6</td>\n",
              "      <td>70.9</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.2410</td>\n",
              "      <td>-13.080</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>102.8</td>\n",
              "      <td>85.3</td>\n",
              "      <td>58.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>65.39</td>\n",
              "      <td>69.01</td>\n",
              "      <td>67.01</td>\n",
              "      <td>67.73</td>\n",
              "      <td>5.415000e+01</td>\n",
              "      <td>1.638000e+02</td>\n",
              "      <td>3.575000e+01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5186</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.1</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.40</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.85</td>\n",
              "      <td>57.00</td>\n",
              "      <td>5.248000e-47</td>\n",
              "      <td>2.419000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>4.042000e-48</td>\n",
              "      <td>1.984000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.4</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>57.3</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.7</td>\n",
              "      <td>17.8</td>\n",
              "      <td>97.3</td>\n",
              "      <td>39.8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>106.3</td>\n",
              "      <td>106.5</td>\n",
              "      <td>49.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.60</td>\n",
              "      <td>57.18</td>\n",
              "      <td>57.20</td>\n",
              "      <td>5.806000e-46</td>\n",
              "      <td>4.762000e-46</td>\n",
              "      <td>3.771000e-45</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5187</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>56.9</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.8</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.36</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.29</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>56.96</td>\n",
              "      <td>4.588000e-47</td>\n",
              "      <td>1.100000e-47</td>\n",
              "      <td>4.557000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>9.272000e-48</td>\n",
              "      <td>1.671000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>57.3</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.6</td>\n",
              "      <td>17.8</td>\n",
              "      <td>97.3</td>\n",
              "      <td>40.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>106.3</td>\n",
              "      <td>106.7</td>\n",
              "      <td>49.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.60</td>\n",
              "      <td>57.18</td>\n",
              "      <td>57.25</td>\n",
              "      <td>2.641000e-46</td>\n",
              "      <td>2.225000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5188</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.1</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.2</td>\n",
              "      <td>57.40</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>57.00</td>\n",
              "      <td>5.248000e-47</td>\n",
              "      <td>2.419000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>1.456000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.7</td>\n",
              "      <td>56.9</td>\n",
              "      <td>53.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.3</td>\n",
              "      <td>17.5</td>\n",
              "      <td>97.0</td>\n",
              "      <td>39.9</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>106.1</td>\n",
              "      <td>106.5</td>\n",
              "      <td>49.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.64</td>\n",
              "      <td>57.18</td>\n",
              "      <td>57.29</td>\n",
              "      <td>5.806000e-46</td>\n",
              "      <td>3.494000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5189</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.1</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.04</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.40</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>57.00</td>\n",
              "      <td>5.907000e-47</td>\n",
              "      <td>2.419000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-4.109000e-48</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>1.456000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>56.9</td>\n",
              "      <td>53.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.3</td>\n",
              "      <td>17.4</td>\n",
              "      <td>96.8</td>\n",
              "      <td>40.1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>105.9</td>\n",
              "      <td>106.5</td>\n",
              "      <td>49.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.60</td>\n",
              "      <td>57.14</td>\n",
              "      <td>57.29</td>\n",
              "      <td>5.806000e-46</td>\n",
              "      <td>3.494000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5190</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.2</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.8</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.36</td>\n",
              "      <td>57.2</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>57.00</td>\n",
              "      <td>4.588000e-47</td>\n",
              "      <td>1.759000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>1.456000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>57.3</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.1</td>\n",
              "      <td>17.5</td>\n",
              "      <td>96.8</td>\n",
              "      <td>39.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>105.9</td>\n",
              "      <td>106.3</td>\n",
              "      <td>49.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.64</td>\n",
              "      <td>57.10</td>\n",
              "      <td>57.33</td>\n",
              "      <td>4.222000e-46</td>\n",
              "      <td>3.494000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150539 rows × 68 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      TWE_set    TEI  TWEI    TEO  ...           FWB  rorl  detect  target\n",
              "0          50  62.28  62.3  63.20  ...  8.761000e-46     0       0       1\n",
              "1          50  62.24  62.3  63.20  ...  9.552000e-46     0       0       1\n",
              "2          50  62.24  63.3  63.16  ...  3.720000e+01     0       0       1\n",
              "3          50  62.52  64.8  63.25  ...  3.077000e+01     0       0       1\n",
              "4          50  63.01  64.4  63.49  ...  3.575000e+01     0       0       1\n",
              "...       ...    ...   ...    ...  ...           ...   ...     ...     ...\n",
              "5186       40  56.83  57.1  56.82  ...  3.771000e-45     8       1       3\n",
              "5187       40  56.83  56.9  56.82  ...  3.644000e-45     8       1       3\n",
              "5188       40  56.83  57.1  56.82  ...  3.644000e-45     8       1       3\n",
              "5189       40  56.83  57.1  56.82  ...  3.644000e-45     8       1       3\n",
              "5190       40  56.83  57.2  56.82  ...  3.644000e-45     8       1       3\n",
              "\n",
              "[150539 rows x 68 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFT0FZyZMqDX"
      },
      "source": [
        "#X = data.iloc[:,:-3]\n",
        "#y_detect = data.iloc[:,-3]\n",
        "#y_target = data.iloc[:,-2]\n",
        "#y_rorl = data.iloc[:,-1]\n",
        "\n",
        "# 심각도 제거용\n",
        "X = data.iloc[:,:-3]\n",
        "y_detect = data.iloc[:,-2]\n",
        "y_target = data.iloc[:,-1]\n",
        "y_rorl = data.iloc[:,-3]\n",
        "\n",
        "# 표준화\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "\n",
        "# 정규화\n",
        "#transformer = MinMaxScaler()\n",
        "#transformer.fit(X)\n",
        "#X_scaled = transformer.transform(X)\n",
        "\n",
        "# train, test 분리\n",
        "#detect\n",
        "X_train, X_test, y_train, y_test = ms.train_test_split(X_scaled, y_detect, \n",
        "                                                      test_size = 0.2, random_state = 100, stratify = y_target )\n",
        "#target\n",
        "X2_train, X2_test, target_train, target_test = ms.train_test_split(X_scaled, y_target, \n",
        "                                                      test_size = 0.2, random_state = 100, stratify= y_target)\n",
        "#rorl\n",
        "X3_train, X3_test, rorl_train, rorl_test = ms.train_test_split(X_scaled, y_rorl, \n",
        "                                                      test_size = 0.2, random_state = 100, stratify = y_target )\n",
        "#######\n",
        "#rorl에서 0인거 빼고 training 해야함\n",
        "# rorl_all에 X+y\n",
        "X3_train = pd.DataFrame(X3_train)\n",
        "rorl_train = pd.DataFrame(rorl_train)\n",
        "rorl_train = rorl_train.reset_index(drop=True)\n",
        "rorl_all = pd.concat([X3_train,rorl_train], axis=1)\n",
        "# train에서 0인거 제거\n",
        "rorl_all = rorl_all.loc[rorl_all['rorl']!= 0]\n",
        "# X3_train / rorl_train으로 나누기\n",
        "X3_train = rorl_all.iloc[:,:-1]\n",
        "rorl_train = rorl_all.iloc[:,-1]\n",
        "X3_train = np.array(X3_train)\n",
        "\n",
        "# 사용할 변수(3,8만 존재) : X3_train, rorl_train\n",
        "\n",
        "#test에서 0제거\n",
        "#lg_test_all 생성\n",
        "X3_test = pd.DataFrame(X3_test)\n",
        "rorl_test = pd.DataFrame(rorl_test)\n",
        "rorl_test = rorl_test.reset_index(drop=True)\n",
        "lg_test_all = pd.concat([X3_test,rorl_test], axis = 1)\n",
        "# test에서 0인거 제거\n",
        "lg_test_all = lg_test_all.loc[lg_test_all['rorl']!=0]\n",
        "# X/y로 나누기\n",
        "lg_x_test = lg_test_all.iloc[:,:-1]\n",
        "lg_y_test = lg_test_all.iloc[:,-1]\n",
        "lg_x_test = np.array(lg_x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0EdsLWaMqI0",
        "outputId": "cf9d470b-7a1a-4666-c84a-d524ecba857a"
      },
      "source": [
        "rorl_train.loc[rorl_train == 3] = 0\n",
        "rorl_train.loc[rorl_train == 8] = 1\n",
        "lg_y_test.loc[lg_y_test == 3] = 0\n",
        "lg_y_test.loc[lg_y_test == 8] = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoCuUJk3JZtX"
      },
      "source": [
        "# 데이터 셋 ro,rl 시각도 4단계 전체"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4M9xr7ZMqOZ",
        "outputId": "d065cce0-338f-4fb6-c7dc-68c20895e915"
      },
      "source": [
        "start = time.time()\n",
        "# 라이브러리\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='normal',activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "model.fit(X3_train, rorl_train, epochs=10, batch_size=64)\n",
        "\n",
        "# 모델 예측 (예측 Test 파일 넣으세용) - Accuracy 구하는 용도\n",
        "#pred = model.predict(lg_x_test)\n",
        "#print('정확도 :', metrics.accuracy_score(lg_y_test, pred))\n",
        "\n",
        "##### 모델 평가(여기에 Test 파일 넣으세용) #####\n",
        "loss_and_metrics = model.evaluate(lg_x_test, lg_y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.7093 - accuracy: 0.4988\n",
            "Epoch 2/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6959 - accuracy: 0.5024\n",
            "Epoch 3/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6962 - accuracy: 0.4947\n",
            "Epoch 4/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6951 - accuracy: 0.5002\n",
            "Epoch 5/10\n",
            "520/520 [==============================] - 2s 3ms/step - loss: 0.6950 - accuracy: 0.4967\n",
            "Epoch 6/10\n",
            "520/520 [==============================] - 2s 3ms/step - loss: 0.6954 - accuracy: 0.4913\n",
            "Epoch 7/10\n",
            "520/520 [==============================] - 2s 3ms/step - loss: 0.6944 - accuracy: 0.4980\n",
            "Epoch 8/10\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.4973\n",
            "Epoch 9/10\n",
            "520/520 [==============================] - 2s 3ms/step - loss: 0.6944 - accuracy: 0.5002\n",
            "Epoch 10/10\n",
            "520/520 [==============================] - 2s 3ms/step - loss: 0.6941 - accuracy: 0.4999\n",
            "260/260 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5022\n",
            "loss_and_metrics : [0.6932076215744019, 0.5021671056747437]\n",
            "0:00:16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "o2z8U5rwVRxp",
        "outputId": "60b30602-9c93-4925-a76e-aa0489b17222"
      },
      "source": [
        "start = time.time()\n",
        "# 라이브러리\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='normal',activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='softmax'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='linear'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='linear'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "history = model.fit(X3_train, rorl_train, epochs=10, batch_size=64, validation_data=(lg_x_test, lg_y_test))\n",
        "\n",
        "# 모델 예측 (예측 Test 파일 넣으세용) - Accuracy 구하는 용도\n",
        "#pred = model.predict(lg_x_test)\n",
        "#print('정확도 :', metrics.accuracy_score(lg_y_test, pred))\n",
        "\n",
        "##### 모델 평가(여기에 Test 파일 넣으세용) #####\n",
        "\n",
        "print('\\nAccuracy: {:.4f}'.format(model.evaluate(lg_x_test, lg_y_test)[1]))\n",
        "\n",
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "\n",
        "x_len = numpy.arange(len(y_loss))\n",
        "plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n",
        "plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6971 - accuracy: 0.4991 - val_loss: 0.6932 - val_accuracy: 0.4966\n",
            "Epoch 2/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6936 - accuracy: 0.4961 - val_loss: 0.6933 - val_accuracy: 0.5022\n",
            "Epoch 3/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.5022\n",
            "Epoch 4/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.4966 - val_loss: 0.6934 - val_accuracy: 0.4977\n",
            "Epoch 5/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.4926 - val_loss: 0.6932 - val_accuracy: 0.4976\n",
            "Epoch 6/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.5022\n",
            "Epoch 7/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.4971 - val_loss: 0.6932 - val_accuracy: 0.4976\n",
            "Epoch 8/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.5031 - val_loss: 0.6933 - val_accuracy: 0.4978\n",
            "Epoch 9/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6931 - val_accuracy: 0.5023\n",
            "Epoch 10/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4997 - val_loss: 0.6956 - val_accuracy: 0.4978\n",
            "260/260 [==============================] - 0s 1ms/step - loss: 0.6956 - accuracy: 0.4978\n",
            "\n",
            "Accuracy: 0.4978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEGCAYAAACtqQjWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXyU1b348c83IQQhgMiqRAUFVISQCQGMiCZVkIpXquJ2ay3VSrXSVr2l1Xtb9dryq7ZuV0V73btg0atCqSIDIhGUoCwNyCKIgBJcQAoJAQMJ+f7+OM8kk5BAlnkyS77v1+t5zcyZZzlzIPOdszzniKpijDHGREJStDNgjDEmcVhQMcYYEzEWVIwxxkSMBRVjjDERY0HFGGNMxLSJdgaiqVu3btqnT58mHbtv3z46dOgQ2QzFMSuPmqw8qllZ1JQI5bFixYqvVbV7Xe+16qDSp08fli9f3qRj8/Pzyc3NjWyG4piVR01WHtWsLGpKhPIQkU/re8+av4wxxkSMBRVjjDERY0HFGGNMxLTqPhVjzJGVl5dTVFREWVlZk8/RuXNn1q9fH8Fcxbd4Ko927dqRnp5OSkpKg4+xoGKMqVdRUREdO3akT58+iEiTzrF37146duwY4ZzFr3gpD1Vl165dFBUV0bdv3wYfZ81fxph6lZWV0bVr1yYHFBO/RISuXbs2upZqQaUJCgpg+vSTKCiIdk6M8Z8FlNarKf/21vzVSAUF8K1vQVlZX6ZPhwULICcn2rkyxpjYYDWVRsrPh4MHAYQDB9xrY0zk7dq1i8zMTDIzM+nVqxe9e/euen3Q/RHWa/ny5fz0pz/1LW979uzhiSeeqPf9tLQ0364d66ym0ki5udC2LZSVQXKye22MibyuXbtSWFgIwD333ENaWho///nPq96vqKigTZu6v8Kys7PJzs72LW+hoPLjH//Yt2vEK6upNFJOjmvySk2t4MILrenLmNoKCuB3v8OXPseJEydy0003MWLECH7xi1/wwQcfkJOTQyAQ4Oyzz2bDhg2Amwrl4osvBlxAuv7668nNzeWUU07h0UcfrfPc77zzTlVNKBAIsHfvXgD+8Ic/MGzYMDIyMrj77rsBuOOOO/jkk0/IzMxkypQpDcp7YWEhZ511Fjk5OVx66aXs3r0bgEcffZSBAweSkZHB1VdffcS8xAOrqTTB2WfD4MElbN9+XLSzYkyLufVW8CoO9SouhtWrobISkpIgIwPS0o4hObnu/TMz4ZFHGpePoqIilixZQnJyMiUlJSxevJg2bdrw1ltv8Z//+Z+8+uqrhx3z0UcfsXDhQvbu3ctpp53GzTfffNi9Fw888ADTpk1j5MiRlJaW0q5dO+bNm8fHH3/MBx98gKpyySWXsGjRIu677z7WrFlTVZNqiOuuu47HHnuMrKws/vCHP/Df//3fPPLII9x3331s2bKF1NRU9uzZU29e4oWvNRURGSsiG0Rkk4jcUc8+V4rIOhFZKyIvhqXfLyJrvO2qsPTFIlLobZ+LyCwvXUTkUe9aq0Uky8/P1r9/KWvWhPpXjDHggkplpXteWeleR9oVV1xBsheliouLueKKKxg0aBC33XYba9eurfOYcePGkZqaSrdu3ejRowdfffXVYfuMHDmS22+/nUcffZQ9e/bQpk0b5s2bx7x58wgEAmRlZfHRRx/x8ccfNzrPxcXF7Nmzh/POOw+A73//+yxatAiAjIwMvvvd7/LXv/61qjmvrrzEC99yKiLJwDRgNFAELBOR2aq6Lmyf/sCdwEhV3S0iPbz0cUAWkAmkAvki8qaqlqjqqLDjXwX+7r38NtDf20YAT3qPvujXby/l5bB2LQQCfl3FmNjRkBpFQQGcf777sdW2LUyfDoMGfRPRm/3Cp43/9a9/TV5eHjNnzmTr1q31zv6bmppa9Tw5OZmKigqmTZvG008/DcCcOXO44447GDduHHPmzGHkyJEEg0FUlTvvvJMf/ehHNc63devWiH2eN954g0WLFvGPf/yDqVOn8uGHH9aZl9NPPz1i1/STnzWV4cAmVd2sqgeBGcD4WvvcCExT1d0AqrrDSx8ILFLVClXdB6wGxoYfKCKdgG8Bs7yk8cCf1VkKHCsix/vxwcDVVAD++U+/rmBM/An1Of7mNy0z3L64uJjevXsD8MILLzTq2FtuuYXCwkIKCws54YQT+OSTTxg8eDC//OUvGTZsGB999BEXXnghzz33HKWl7u99+/bt7Nixg44dOzaqn6Nz58506dKFxYsXA/CXv/yF8847j8rKSrZt20ZeXh73338/xcXFlJaW1pmXeOFnnao3sC3sdRGH1xwGAIjIe0AycI+qzgVWAXeLyINAeyAPWFfr2O8AC1S15AjX6w18EX6QiEwCJgH07NmT/CaOCe7cuZRjjqng9de/5JRTNjXpHImktLS0yWWZiBKlPDp37tzoTuJBg9wGsHcvHDp0qNkdzQcOHCAlJYXy8nK++eabqvPdcsst3HTTTdx7772MGTMGVWXv3r3s37+fiooK9u7dW3Vs6JjKykpKS0sPy9Pvf/97Fi9eTFJSEqeffjrnnHMOqampXHbZZYwY4b66OnTowNNPP80pp5zC8OHDGThwIKNHj+a3v/1tjXPt37+/KtgBTJ48mSeeeIJbb72V/fv307dvX5544gn27NnDNddcQ0lJCarKj370I5KTk+vMS7Q668vKyhr3f1lVfdmACcAzYa+/Bzxea5/XgZlACtAXFxSO9d77L6AQmA9MB26tdeybwOW1znVO2OsFQPaR8jh06FBtqoULF+o556iOHNnkUySUhQsXRjsLMSVRymPdunXNPkdJSUkEcpI44q086vo/ACzXer5X/Wz+2g6cGPY63UsLVwTMVtVyVd0CbMT1iaCqU1U1U1VHA+K9B4CIdMM1r73RyOtFVCDgRsOEOiaNMaa18zOoLAP6i0hfEWkLXA3MrrXPLCAXqgLFAGCziCSLSFcvPQPIAOaFHTcBeF1Vw2c6mw1c540COwsoVtUaTV+RFgjAvn3QhMEgxhiTkHzrU1HVChGZDARx/SXPqepaEbkXV3Wa7b03RkTWAYeAKaq6S0TaAYu9ycxKgGtVtSLs9FcD99W65BzgImATsB/4gV+fLSQ06uuf/4TTTvP7asYYE/t8HfysqnNwX/bhaXeFPVfgdm8L36cMNwKsvvPm1pGmwC3Ny3HjDBwIKSkuqHg3whpjTKtm07Q0Q9u2MHiwDSs2xpgQCyrNFAjAypXgBpwZY0zrZkGlmQIB2LULioqinRNjEk9eXh7BYLBG2iOPPMLNN99c7zG5ubksX74cgIsuuqhqPq1w99xzDw888MARrz1r1izWrau+Pe6uu+7irbfeakz2IyI/P58lS5bU+d4LL7zA5MmTWzhHR2ZBpZnCO+uNMZF1zTXXMGPGjBppM2bM4JprrmnQ8XPmzOHYY49t0rVrB5V7772XCy64oEnnao4jBZVYZEGlmTIyQMSCijFVIjj3/YQJE3jjjTeqFuXaunUrn3/+OaNGjeLmm28mOzubM888s2pK+tr69OnD119/DcDUqVMZMGAA55xzTtUU+QBPP/00w4YNY8iQIVx++eXs37+fJUuWMHv2bKZMmUJmZiaffPIJEydO5JVXXgFgwYIFBAIBBg8ezPXXX8+BAweqrnf33XeTlZXF4MGD651e5Y477qia7j60RszOnTu5/PLLGTZsGMOGDeO9995j69at/PGPf+Thhx8mMzOzapqXo3nooYcYNGgQgwYN4hFv0rZ9+/Yxbtw4hgwZwqBBg3jppZfqzUtzxM/UlzEqLc0NJ7agYhJeE+e+PyYtjabOfX/ccccxfPhw3nzzTcaPH8+MGTO48sorERGmTp3Kcccdx6FDhzj//PNZvXo1GRkZdZ5nxYoVzJgxg8LCQioqKsjKymLo0KEAXHbZZdx4440A/OpXv+LZZ5/lJz/5CZdccgkXX3wxEyZMqHGusrIyJk6cyIIFCxgwYADXXXcdTz75JLfeeisA3bp1Y+XKlTzxxBM88MADPPPMMzWO37VrFzNnzuSjjz5CRKqa5372s59x2223cc455/DZZ59x4YUXsn79em666abDFig7khUrVvD888/z/vvvo6qMGDGC8847j82bN3PCCSfwxhvunvHi4uJ689IcVlOJgFBnvTGtng9z34c3gYU3fb388stkZWURCARYu3Ztjaaq2hYvXsyll15K+/bt6dSpE5dccknVe2vWrGHUqFEMHjyY6dOn1zt9fsiGDRvo27cvAwYMAGpOYw8uSAEMHTq0ztmMO3fuTLt27bjhhht47bXXaN++PQBvvfUWkydPJjMzk0suuYSSkpKqiSwb49133+XSSy+lQ4cOpKWlcdlll7F48WIGDx7M/Pnz+eUvf8nixYvp3LlzvXlpDqupREAgAH/7m+uw79o12rkxxidNnPv+m0GDmjX1/fjx47nttttYuXIl+/fvZ+jQoWzZsoUHHniAZcuW0aVLFyZOnEhZWdnRT1aHiRMnMmvWLIYMGcILL7zQ7IlAQ9Psh6bYB7jwwgv56quvyM7O5uGHH+aDDz5gwYIFvPLKKzz++OO8/fbbVFZWsnTpUt8W5BowYAArV65kzpw5/OpXv+L888/nrrvuqjMvzWE1lQiwznpjPD7MfZ+WlkZeXh7XX399VS2lpKSEDh060LlzZ7766ivefPPNI57j3HPPZdasWVUzHP/jH/+oem/v3r0cf/zxlJeXM3369Kr0+qa3P+2009i6dSubNrnZyUPT2B9JMBiksLCQZ555htLSUoqLi7nooot4+OGHWbVqFQBjxozhscceqzomtKpkY6fZHzVqFLNmzWL//v3s27ePmTNnMmrUKD7//HPat2/Ptddey5QpU1i5cmW9eWkOq6lEQHhQicLgEGNiS05OxBdSueaaa7j00kurmsGGDBlCIBDg9NNP58QTT2TkyJFHPD4rK4urrrqKIUOG0KNHD4YNG1b13m9+8xtGjBhB9+7dGTFiRNUX+NVXX82NN97Io48+WtVBD9CuXTuef/55rrjiCioqKhg2bBg33XRTgz9LaWkp3/3udykrK0NVeeihhwC3Vv0tt9xCRkYGFRUVnHvuufzxj3/k3/7t35gwYQJ///vfeeyxxxg1alSN873wwgvMmjWr6vXSpUuZOHEiw4cPB+CHP/whgUCAYDDIlClTSEpKIiUlhSeffJK9e/cyfvz4w/LSHKKt+K697OxsDY1nb6z8/Pwaq8ydfDKMHAkvvlj/MYmsdnm0dolSHuvXr+eMM85o1jn27t0b0ZUf4128lUdd/wdEZIWqZte1vzV/RYh11htjjAWViAkEYONGaMJgDWOMSRgWVCIkEHDzf61eHe2cGBNZrbmJvLVryr+9BZUIycpyjzYCzCSSdu3asWvXLgssrZCqsmvXrkYPcbbRXxHSuzd062ZBxSSW9PR0ioqK2LlzZ5PPUVZW5tu9F/EonsqjXbt2pKenN+oYX4OKiIwF/ge38uMzqlp7tUZE5ErgHkCBVar67176/cA4b7ffqOpLXroAvwWuwK0W+aSqPioiucDfgS3eMa+p6r0+fbTDiFhnvUk8KSkp9O3bt1nnyM/PJxAad28Svjx8CyoikgxMA0YDRcAyEZmtquvC9ukP3AmMVNXdItLDSx8HZAGZQCqQLyJvqmoJMBE4EThdVStDx3gWq+rFfn2mowkE4OGHq28mNsaY1sbPPpXhwCZV3ayqB4EZwPha+9wITFPV3QCqusNLHwgsUtUKVd0HrAbGeu/dDNyrqpW1jom6QADKy+EIUxAZY0xC87P5qzewLex1ETCi1j4DAETkPVwT2T2qOhdYBdwtIg8C7YE8IPRVfSpwlYhcCuwEfqqqH3vv5YjIKuBz4OeqetjMcCIyCZgE0LNnzybP81NaWnrYseXlxwAj+NvfPmLPni+bdN54VVd5tGZWHtWsLGpK9PKIdkd9G6A/kAukA4tEZLCqzhORYcASXOAowPWfgGsOK1PVbBG5DHgOGAWsBE5W1VIRuQiY5Z27BlV9CngK3B31Tb3rua47pisr4cc/hm++OZ3c3NObdN54lSh3kEeKlUc1K4uaEr08/Gz+2o7r+whJ99LCFQGzVbVcVbcAG/ECgapOVdVMVR0NiPde6JjXvOczgQxv/xJVLfWezwFSRKRb5D9W/ZKSYMgQ66w3xrRefgaVZUB/EekrIm2Bq4HZtfaZhaul4AWAAcBmEUkWka5eegYucMwLOybPe34eXrARkV7eyDBEZDjus+3y56PVLxCAVauql5QwxpjWxLegoqoVwGQgCKwHXlbVtSJyr4iEVsgJArtEZB2wEJiiqruAFGCxl/4UcK13PoD7gMtF5EPgd8APvfQJwBqvT+VR4GqNwh1bgYCbqsWbFdsYY1oVX/tUvGaoObXS7gp7rsDt3ha+TxluBFhd59xD9f0r4emPA483P9fNE35nvbcwnDHGtBo2TUuEDRwIKSl2Z70xpnWyoBJhbdvCoEHWWW+MaZ0sqPggEHA1FZuDzxjT2lhQ8UEgAF9/DdtrD6A2xpgEZ0HFBzYNvjGmtbKg4oOMDDdrsQUVY0xrY0HFB2lpbjixddYbY1obCyo+CXXWG2NMa2JBxSeBAHz2Gexq8YlijDEmeiyo+CTUWV9YGN18GGNMS7Kg4pPQaqHWBGaMaU0sqPika1c48UTrrDfGtC4WVHxknfXGmNbGgoqPAgHYsAH27Yt2TowxpmVYUPFRVpab/2v16mjnxBhjWoYFFR9ZZ70xprXxNaiIyFgR2SAim0Tkjnr2uVJE1onIWhF5MSz9fhFZ421XhaWLiEwVkY0isl5EfhqW/qh3rdUikuXnZ2uI9HTXYW+d9caY1sK3lR9FJBmYBowGioBlIjJbVdeF7dMfuBMYqaq7RaSHlz4OyAIygVQgX0TeVNUSYCJwInC6qlaGjgG+DfT3thHAk95j1IhYZ70xpnXxs6YyHNikqptV9SAwAxhfa58bgWmquhtAVXd46QOBRapaoar7gNXAWO+9m4F7VbWy1jHjgT+rsxQ4VkSO9+vDNVRWFqxZA+Xl0c6JMcb4z8816nsD28JeF3F4zWEAgIi8ByQD96jqXGAVcLeIPAi0B/KAUA3nVOAqEbkU2An8VFU/rud6vYEvwi8oIpOASQA9e/YkPz+/SR+utLS0Qcempvbg4MGB/OlPy+jXL3GHgTW0PFoLK49qVhY1JXp5+BlUGnr9/kAukA4sEpHBqjpPRIYBS3CBowA45B2TCpSparaIXAY8B4xq6AVV9SngKYDs7GzNzc1tUsbz8/NpyLHHHw+/+Q0kJw+jiZeKCw0tj9bCyqOalUVNiV4efjZ/bcf1fYSke2nhioDZqlquqluAjbggg6pOVdVMVR0NiPde6JjXvOczgYxGXK/F9e8PHTpYv4oxpnXwM6gsA/qLSF8RaQtcDcyutc8sXC0FEemGaw7bLCLJItLVS8/ABY55Ycfkec/PozrYzAau80aBnQUUq2qNpq9oSEqCIUMsqBhjWgffmr9UtUJEJgNBXH/Jc6q6VkTuBZar6mzvvTEisg7XvDVFVXeJSDtgsYgAlADXqmqFd+r7gOkichtQCvzQS58DXARsAvYDP/DrszVWVha88AJUVrogY4wxicrXPhVVnYP7sg9PuyvsuQK3e1v4PmW4EWB1nXMPMK6OdAVuaX6uIy8QgMcfh08+cc1hxhiTqOx3cwsI3VlvN0EaYxKdBZUWcOaZkJJi/SrGmMRnQaUFtG3rAosFFWNMorOg0kKyslxQUY12Towxxj8WVFpIIAA7d8Lnn0c7J8YY4x8LKi3EOuuNMa2BBZUWMmSIm7XY+lWMMYnMgkoLSUtz96hYUDHGJDILKi0o1FlvjDGJyoJKCwoE4NNP4V//inZOjDHGHxZUWpCtWW+MSXQWVFqQBRVjTEwoKIDf/c49Rli0F+lqVbp1g/R0CyrGmCgqKIDcXKiogNRUWLAAcnIidnqrqbQw66w3xkRVMAgHD7q1OA4ehAgvbWxBpYUFArBhA+xL3OXqjTGxLC3NPSYluYkJI7y0sa9BRUTGisgGEdkkInfUs8+VIrJORNaKyIth6feLyBpvuyos/QUR2SIihd6W6aXnikhxWPpddV0v2gIB9wNh9epo58QY0ypt3eqave65J+JNX+Bjn4qIJAPTgNG4deWXichsVV0Xtk9/4E5gpKruFpEeXvo4IAvIBFKBfBF5U1VLvEOnqOordVx2sape7NdnioTwzvoI/1saY8zRBYNw/vnw61/7cno/ayrDgU2qullVDwIzgPG19rkRmKaquwFUdYeXPhBYpKoVqroPWA2M9TGvLebEE+G446xfxRgTBZs3w6ZNMNa/r1M/g0pvYFvY6yIvLdwAYICIvCciS0Uk9ElXAWNFpL2IdAPygBPDjpsqIqtF5GERSQ1LzxGRVSLypoicGeHPExEi1llvjImSYNA9Xnihb5eI9pDiNkB/IBdIBxaJyGBVnSciw4AlwE6gADjkHXMn8CXQFngK+CVwL7ASOFlVS0XkImCWd+4aRGQSMAmgZ8+e5Ddx5ENpaWmTj+3a9RTeeSedt95aTJs2ibHASnPKIxFZeVSzsqgpmuUx6K9/pUOvXry/fbt/63Coqi8bkAMEw17fCdxZa58/Aj8Ie70AGFbHuV4ELqojPRd4vZ7rbwW6HSmPQ4cO1aZauHBhk4998UVVUC0sbPIpYk5zyiMRWXlUs7KoKWrlceCAalqa6k03NftUwHKt53vVz+avZUB/EekrIm2Bq4HZtfaZ5QUGvGauAcBmEUkWka5eegaQAczzXh/vPQrwHWCN97qXl4aIDMc17e3y8fM1md1Zb4xpcQUFUFrqa9MX+Nj8paoVIjIZCALJwHOqulZE7sVFudnee2NEZB2ueWuKqu4SkXbAYi9GlADXqmqFd+rpItIdEKAQuMlLnwDcLCIVwDfA1V5EjTn9+0P79i6oTJwY7dwYY1qFYBDatIFvfcvXy/jap6Kqc4A5tdLuCnuuwO3eFr5PGW4EWF3nrLNEVPVx4PFmZrlFJCdDZqbVVIwxLWjuXHcfQ6dOvl7G7qiPkkAACgvdjZDGGOOrr75yv2J9HEocYkElSgIB2LsXPvkk2jkxxiS8+fPdo8/9KWBBJWqss94Y02KCQejevfqLx0cWVKLkzDMhJcWCijHGZ5WVLqiMHu0mkfSZBZUoSU11gcWCijHGV4WFsHNni/SngAWVqAoEYOVKiM2Bz8aYhBCammXMmBa5XIOCioj8TEQ6ifOsiKwUkZbJYQILBNwPCL9mSzDGGObOdfcw9OzZIpdraE3lenXTzo8BugDfA+7zLVethHXWG2N8VVICS5a0yKivkIYGFfEeLwL+oqprw9JMEw0Z4mYttqBijPHFwoVuLfoW6k+BhgeVFSIyDxdUgiLSEbDb9pqpY0c3ZYsFFWOML4JBt3zw2We32CUbOk3LDbhVGDer6n4ROQ74gX/Zaj0CAVi6NNq5MMYkHFXXn5KX59aibyENrankABtUdY+IXAv8Cij2L1utRyAAn34K//pXtHNijEkomzbBli0t2p8CDQ8qTwL7RWQI8B/AJ8CffctVKxLqrC8sjG4+jDEJJjSUuAX7U6DhQaXCm1F4PPC4qk4DOvqXrdbDRoAZY3wRDMKpp7qtBTU0qOwVkTtxQ4nfEJEkIMW/bLUe3btDeroFFWNMBB04AG+/3eJNX9DwoHIVcAB3v8qXuPXk/+BbrlqZ0J31xhgTEe+9B/v3x25Q8QLJdKCziFwMlKnqUftURGSsiGwQkU0ickc9+1wpIutEZK2IvBiWfr+IrPG2q8LSXxCRLSJS6G2ZXrqIyKPetVaLSFZDPlssCARgwwb3f8AYY5otGHQz1ubltfilGzpNy5XAB8AVwJXA+yIy4SjHJAPTgG/jVnG8RkQG1tqnP3AnMFJVzwRu9dLHAVm4YcwjgJ+LSPhyZVNUNdPbQl3c3wb6e9sk3OCCuBAIuIlEV6+Odk6MMQkhGISRI93NcC2soc1f/wUMU9Xvq+p1wHDg10c5ZjiwSVU3q+pBYAauoz/cjcA0Vd0NoKo7vPSBwCJVrVDVfcBq4GhDGMYDf1ZnKXCsiBzfwM8XVVlencr6VYwxzfbFF7BqVVSavqDhNz8mhX3hA+zi6AGpN7At7HURrtYRbgCAiLwHJAP3qOpcYBVwt4g8CLQH8oB1YcdNFZG7gAXAHap6oJ7r9Qa+CL+giEzC1WTo2bMn+fn5R/kYdSstLW3ysbWpQqdOI3njjZ2cccbGiJyzpUWyPBKBlUc1K4ua/C6PnnPncgawvFs3SqNQ7g0NKnNFJAj8zXt9FTAnQtfvD+TiOv8XichgVZ0nIsOAJcBOoAA45B1zJ/Al0BZ4CvglcG9DL6iqT3nHkZ2drbm5uU3KeH5+Pk09ti7DhsGXX55Abu4JETtnS4p0ecQ7K49qVhY1+V4eTz0FPXuSff31LbIoV20N7aifgvsizvC2p1T1l0c5bDtwYtjrdC8tXBEwW1XLVXULsBEXZFDVqV6fyWjc5JUbvfQvvCauA8DzuGa2hl4vZgUC8OGHUF4e7ZwYY+LWoUMwb55bOyUKAQUasUiXqr6qqrd728wGHLIM6C8ifUWkLXA1MLvWPrNwtRREpBuuOWyziCSLSFcvPRTI5nmvj/ceBfgOsMY712zgOm8U2FlAsarWaPqKZYEAHDwI69dHOyfGmLi1ciXs2hW1/hQ4SvOXiOwF6lqXUABV1U51vAfuzQoRmQwEcf0lz6nqWhG5F1iuqrO998aIyDpc89YUVd0lIu2AxS5uUAJcq6oV3qmni0h3Lw+FwE1e+hzcLMqbgP3E2YSX4Z31GRnRzYsxJk4Fg249jRZa5bEuRwwqqtqs8WiqOodafS+qelfYcwVu97bwfcpwI8DqOue36klX4Jbm5Dea+veH9u1dUPn+96OdG2NMXAoG3S/U7t2jlgVboz5GJCe7RbvsznpjTJMUF0NBQVSbvsCCSkwJBNxsxZW2/JkxprEWLHAd9RZUTEggAHv3wubN0c6JMSbuBIPuDvqcnKhmw4JKDLE7640xTaLqgsr557s5v6LIgkoMOfNMaN4N4X0AACAASURBVNPGgooxppE2bnRLyEa56QssqMSU1FQXWKyz3hjTKHPnukcLKqa2QMDVVLSuu4OMMaYuwSAMGAB9+0Y7JxZUYk0gADt2uIlGjTHmqMrKID8/JmopYEEl5lhnvTGmUd59F775xoKKqduQIW6WBQsqxpgGmTsX2raFGJkJ2oJKjOnYEfr1s856Y0wDBYMwahR06BDtnAAWVGJSqLPeGGOOaPt2WLMmZpq+wIJKTAoEYOtW2L072jkxxsS0efPcowUVcyShzvrCwujmwxgT4+bOheOPh8GDo52TKhZUYlAg4B6tCcwYU69Dh2D+fFdLcWtPxQQLKjGoe3fo3ds6640xR7B8uWsjj6GmL/A5qIjIWBHZICKbROSOeva5UkTWichaEXkxLP1+EVnjbVfVcdyjIlIa9nqiiOwUkUJv+6E/n6plWGe9MeaI5s51NZTRo6OdkxqOuPJjc4hIMjANGA0UActEZLaqrgvbpz9wJzBSVXeLSA8vfRyQBWQCqUC+iLypqiXe+9lAlzou+5KqTvbrM7WkrCyYMwf273crQhpjTA3BIGRnQ9eu0c5JDX7WVIYDm1R1s6oeBGYA42vtcyMwTVV3A6jqDi99ILBIVStUdR+wGhgLVcHqD8AvfMx71AUCbrGuDz+Mdk6MMTFn9254/30YOzbaOTmMbzUVoDewLex1ETCi1j4DAETkPSAZuEdV5wKrgLtF5EGgPZAHhGo4k4HZqvqFHN45dbmInAtsBG5T1W21dxCRScAkgJ49e5Kfn9+kD1daWtrkYxuirCwVyOGllzbyzTef+3adSPG7POKNlUc1K4uaIlEe3d95hzMrK1nZvTslsVa2qurLBkwAngl7/T3g8Vr7vA7MBFKAvrggdKz33n8BhcB8YDpwK3AC8C7QxtunNOxcXYFU7/mPgLePlsehQ4dqUy1cuLDJxzZEZaVqly6qN97o62Uixu/yiDdWHtWsLGqKSHnccINq586q5eXNP1cTAMu1nu9VP5u/tgMnhr1O99LCFeFqHeWqugVXw+gPoKpTVTVTVUcD4r0XAPoBm0RkK9BeRDZ5++9S1QPeeZ8BhvrzsVqGiHXWG2PqEFrl8YIL3Kp+McbPoLIM6C8ifUWkLXA1MLvWPrOAXAAR6YZrDtssIski0tVLzwAygHmq+oaq9lLVPqraB9ivqv28/Y4PO+8lwHr/PlrLyMpyfSrl5dHOiTEmZqxfD0VFMTeUOMS3MKeqFSIyGQji+kueU9W1InIvruo023tvjIisAw4BU1R1l4i0AxZ7fSYlwLWqWnGUS/5URC4BKoB/ARN9+WAtKBCAAwfgo49i6oZZY0w0BYPusbUFFQBVnQPMqZV2V9hzBW73tvB9ynAjwI52/rSw53fihicnjPA76y2oGGMAd3/K6afDSSdFOyd1sjvqY9iAAe4eFbuz3hgDuMW4Fi2KyaHEIRZUYlhyMmRkWGe9McazaJFbPjhGm77AgkrMy8pysxVXVkY7J8aYqAsGITUVzj032jmplwWVGBcIQEkJbNkS7ZwYY6Ju7lwXUGJ47iYLKjEu1Flv/SrGtHLbtrnhxDHcnwIWVGLeoEHu/ibrVzGmlYvxocQhFlRiXGoqDBxoQcWYVi8YdAstDTzq3RZRZUElDmRlWVAxplWrqIjJVR7rYkElDgQC8NVX8MUX0c6JMSYqPvgAiotjvj8FLKjEBeusN6aVCwYhKclNIhnjLKjEgSFD3KM1gRnTSgWDMHw4dKlrwdvYYkElDnTqBP36WVAxplXatcs1f8X4qK8QCypxwjrrjWml3nrLraESB/0pYEElbgQC7q763bujnRNjTIsKBl2z17Bh0c5Jg1hQiROhzvrCwujmwxjTgsJXeUxOjnZuGsSCSpwIX1vFGNNKrFkDn38eN/0p4HNQEZGxIrJBRDaJyB317HOliKwTkbUi8mJY+v0issbbrqrjuEdFpDTsdaqIvORd630R6ePHZ4qWHj3ghBMsqBjTqsTJ1CzhfFv5UUSSgWnAaKAIWCYis1V1Xdg+/XGrNY5U1d0i0sNLHwdkAZlAKpAvIm+qaon3fjZQe2zdDcBuVe0nIlcD9wOHBaN4Zp31xrQywSCceSakp0c7Jw3mZ01lOLBJVTer6kFgBjC+1j43AtNUdTeAqu7w0gcCi1S1QlX3AauBsVAVrP4A/KLWucYDf/KevwKcLxLj8xk0UiDgJindvz/aOTHG+G7fPrcoVxzVUsDfNep7A9vCXhcBI2rtMwBARN4DkoF7VHUusAq4W0QeBNoDeUCohjMZmK2qX9SKGVXXU9UKESkGugJfh+8kIpOASQA9e/YkPz+/SR+utLS0ycc2VUpKNyorB/HCCysYOHBvi177aKJRHrHMyqOalUVNDS2P45YuJePgQVb16sXuOCo/P4NKQ6/fH8gF0oFFIjJYVeeJyDBgCbATKAAOicgJwBXe/k2iqk8BTwFkZ2drbm7TTpWfn09Tj22qPn3grrsgKWkoLXzpo4pGecQyK49qVhY1Nbg8Zs6EY45hyE9+Au3a+Z6vSPGz+Ws7cGLY63QvLVwRrtZRrqpbgI24IIOqTlXVTFUdDYj3XgDoB2wSka1AexHZVPt6ItIG6Azs8uODRcvJJ7vh6tavYkwrEAzCeefFVUABf4PKMqC/iPQVkbbA1cDsWvvMwqt1iEg3XHPYZhFJFpGuXnoGkAHMU9U3VLWXqvZR1T7AflXt551rNvB97/kE4G1VVf8+XssTcf0qFlSMSXBbt8KGDXHXnwI+Nn95/RqTgSCuv+Q5VV0rIvcCy1V1tvfeGBFZBxwCpqjqLhFpByz2+kxKgGtVteIol3wW+ItXc/kXLoglnEAAHn8cysshJSXauTHG+CI0lDhOpmYJ52ufiqrOAebUSrsr7LkCt3tb+D5luBFgRzt/Wq1jrmhmlmNeIAAHDsBHH8HgwdHOjTHGF8EgnHQSnHZatHPSaHZHfZyxO+uNSXDl5bBgQVys8lgXCypx5rTT4JhjLKgYk7CWLoWSkrjsTwELKnEnOdkt2mVBxZgEFQy6P/Tzz492TprEgkocCo0Aq6yMdk6MMREXDMJZZ8Gxx0Y7J01iQSUOBQKudrxlS7RzYoyJqJ07YcWKuG36Agsqcck6641JUPPnuzVULKiYljRoELRpY0HFmIQTDELXrjB0aLRz0mQWVOJQu3YwcKAFFWMSiirMmwejR8fNKo91saASp2y6FmMSzOrV8OWXcd30BRZU4lYg4P7/ffFFtHNijImIuXPd45gx0c1HM1lQiVPWWW9MggkGISPDrRsexyyoxKnMTPdoQcWYBFBaCu++G/dNX2BBJW516gS9e8Pf/gYFBdHOjTGmWfLz3ZxfFlRMtBQUuD6VtWshLw/eeSfaOTLGNNncudC+PZxzTrRz0mwWVOJUfr4bgQhuKvzRo+Hb34YHH4RVq2wKF2PiSjDofh2mpkY7J83ma1ARkbEiskFENonIHfXsc6WIrBORtSLyYlj6/SKyxtuuCkt/VkRWichqEXlFRNK89IkislNECr3th35+tmjLzXX//5KToW1bGD8ePv0Ufv5z19/Sqxdccw08+6xLN8bEqM2bYdOmhGj6Ah8X6RKRZGAaMBq3Fv0yEZmtquvC9ukP3AmMVNXdItLDSx8HZAGZQCqQLyJvqmoJcJv3iIg8BEwG7vNO+ZKqTvbrM8WSnBy35EJ+vgswOTkufft2l/7WW26bMcOl9+sHF1zgtrw8OO64aOXcGFNDaJVHCypHNRzYpKqbAURkBjAeWBe2z43ANFXdDaCqO7z0gcAibwnhChFZDYwFXg4LKAIcAyTUOvSNkZNTHUxCeveG665zmyqsX18dYP76V/jjH926P0OHugAzejScfba7S98YEwVz50KfPtC/f7RzEhGi6s93sohMAMaq6g+9198DRoTXJERkFrARGIlbx/4eVZ0rImOAu3G1nPbAB7jg86B33PPARbgANU5V94vIROB3wE7vnLep6rY68jUJmATQs2fPoTNCP+UbqbS0lLS0tKPvGEMqKoT16zuycmUXVqzowrp1nTh0KIm2bQ+RkVFMVtZuhg7dTb9+pSQ1smE0HsvDT1Ye1awsagovDykvZ+T48Xw1ejQf33ZblHPWcHl5eStUNbvON1XVlw2YADwT9vp7wOO19nkdmAmkAH2BbcCx3nv/BRQC84HpwK21jk0GngB+4L3uCqR6z38EvH20PA4dOlSbauHChU0+NlaUlKi+/rrqrbeqDhqk6uo2ql27ql5xher//q/qJ5807FyJUB6RZOVRzcqiphrlkZ/v/uhmzoxafpoCWK71fK/62VG/HTgx7HW6lxauCJitquWqugVXw+gPoKpTVTVTVUcD4r1XRVUPATOAy73Xu1T1gPf2M0D8TvPZQjp2hHHj4OGH4cMP4fPPXRPZv/0bLFkCP/oRnHoqnHIKTJoEL78MX38d7VzHvoICmD79JLt/yBxdMOimHP/Wt6Kdk4jxs09lGdBfRPrigsnVwL/X2mcWcA3wvIh0AwYAm71O/mNVdZeIZAAZwDyvH+VUVd3kPb8E+AhARI5X1dBMWJcA6338bAnp+OPhu991myps3Oj6YubPh5degqefdv0xgUB1p/8550BhofsSTU09vI8nXqm6odolJY3btm1zsxyo9uX55+Hyy91ovB49oGfPmlsCjB41zTV3ruvU7NQp2jmJGN+CiqpWiMhkIIhrqnpOVdeKyL24qtNs770xIrIOOARM8QJJO2CxixuUANd650sC/iQinXC1l1XAzd4lfyoilwAVwL+AiX59ttZABE47zW233AIVFbB8eXWn/8MPw+9/DykpcOgQVFb25YUX3DDmvn3dj6+UFLf5/TwpyeW3oAAWLnQrsZ5xRuMDQmgrLnaP5eVHL6c2baBzZ/ed0KmTO9a1wAqHDsGsWa6GV5fOnV1wqSvg1E63LokE9NVX7hfI1KnRzklE+VlTQVXnAHNqpd0V9lyB270tfJ8y3Aiw2uerxHXq13WtO3HDk40P2rRxX9ZnnQW/+hXs2weLF7u/h3ffBRAqKmD69OjceJmcDMMOFZBLPv9FLks5cpWpXbvqQBDaTjrp8LS6tvAgkprqAlpIQQGcfz4cOFBJamoSCxbAkCGwY4f7DgnfwtPWroW334bdu+vOb/v2dQebugLRscdWB9naQ85NDJk/3z0myFDiEF+DiklcHTrA2LHuC7b2l+hZZ7naS0WF+7VfXu7vc/bto9dL/8MPtt5FEpWUk8L9I16j30/H1RkMOnZ0N4z6IXT/0HPPbeX660+p+jLv08dtR3PwoAs2dQWhUCDasgWWLnXLmdc1eLNtW/dZv/7avZ+UBMOGuYCTmureDz025Hlj9g1/npLi8ploTaPNEepvS02FnGAQunevnnI8Qfg2pDgeZGdn6/Lly5t0bH5+Prm5uZHNUGPNnes6+iZMgJF1VuBaREEBPPfc5hpfor7bswdefx1efdWVQ1kZimsTrXLGGe5Oz7w8OO889wfcQlri/8ehQy5w1FXzeestWLmyet/0dOjWzfUTHTzottrPDx3yK6eKiHDeeS64nXJK9XbSSf4F+GgqKXHBf8sWd8P8li2wYoULsqpKsihfp/Ri64AxLL3lr/Tu7f6NQv9OIke/RjSJSL1Diq2mEk/KytywrPnzYeZM2LDBpT/yiKtCX3utW+CnR48WzVZODhw48Bk5Oaf4e6EdO+Dvf4fXXnPVgfJyd7fnjTfCaadR+R9T4OBBpE0bkm68wf0l//nP8MQT7vhBg2oGmTifViA5ubrZq7ZQM9zBg+5L++WXj15TOHTIFemRAk/o+ZHeCz1fsMD1cakKqq6Jr6DAvR+SlAQnnlgz0IRvXbtG6As2wm2B5eXw2Wc1g0b4465dNffv3Nk1YYb62zL0nxx7cCcPr72QP99cc9+2bakKMuHBJvx5r16uSToWxWi2DOA6Jz780AWR+fNdJ8Y337j/Tenp7q9N1T0uXlw93UMg4ILM2LHuDyiefwoWFbkg8tpr7jNWVrpvm1tvdUOrhg0jdKdmclbW4V8c5eXuJ+LChW579ll47DFXZkOGuACTmwvnnus6IxJEfdP4HElystsiNbtCXl7NptG//x1GjHCrlW7efPj2xhtu5u1wHTvWH3BOPvkoI+hUXVXujTfc+PiKCve38PbbRy0Q1eqmxrqCxrZtNfsOU1Jc82bfvnDFFe7xlFOqH7t0qdnfdnFSECrguaIx/A7333z7dvcY/nzZMjfYo6ysZv6SklxgCQ82tQNQ795wzDGN+ReLDGv+irXmr6Ki6iCyYIH7nw0wcKCbU+WCC9yv7DVrav4UnT/ffRsEg25bssT9EaWluTHwF17otlNPjXyeiXB5bNrkgsirr8IHH7i0QYPgsstcIBk8uOk/Xw8edH+poSCzZIn7i01KcsE4VJM555xmDfOMiebRGNDYptF9+2Dr1rqDzubN1V+uQiU92cHQnkVkdivijLRt9E0p4oTKIo7bX0SH3UUkfbkdCa8WhRx3HFx0EQeyctiWnsP6NoPZ/Fmbw4LH/v01D+vVqzqg1Q4aJ5zgAnJDy+OBFdfTWYsbtMqeKvzrX/UHntDz4uLDj+3ate7aTkmJC+7f+U7TKm5Hav6yoBLtoFJS4n5OhgJJqEmrZ8/qIHLBBe5/Q21HqtKXlLhfZKEgs2WLSz/11OoAk5fnfgpGQLPKQ9UFyVAg+fBDl56d7YLIZZfBgAERyedhDhyA99+vDjIFBS7wJCe7CdJCQWbkyEaN67WgUq3RZVFR4aosoW9Mb9NtRZRvKaJyWxFtd24n6VBFjcMO0Jbt9KaIdIpI56uUdA50S6dL2318/9P/pg3lVJJMYfscTi7bSM9KVy0qpQPLGMaKlBw+653D7tNz6H5GtxqBo08f13wVCYvfeINR3/mOm1L8d7+LzEmBvXtdcKkv8BQVucEdISLud+iCBY0PLNanEkvKy92v71AQef9915jdvr1rgpk0yQWTQYOO/mu8rhklQzp1cj9DvvMd96W9aVN1gPnTn1w/Q0qK+7IMBZkhQ2j0pF9NpepqDKGmrY8/dp/3nHNcH9Gll7peXL+lprpyP/dcuPtu17wYCtYLF8JDD8H997smx2HDqoPM2WdH7lsmkRUUcNL06VQN/zp40E3dUCtg1Ni++OLwcenHHIOkp9M2PR1Gn1v90ztsq+zQjQOfJbF3M+zaDJ95tZv334cXyCWXfPLJZefxOXwrT8ns8imBsgL6flXAORsLyF3zB2RrBWwFNvVz+W2TA71zoO0gIvV1eew//+kCZ4SHEnfsCKef7rb6HDjgbgl46CFXxAcPuv/qkRxgYzUVv2sqqq72EQoi+fnuJ4WI+yU+erTbcnJa7hbrAwfgvfeqg8yqVS69Rw/X0X/hhY3u8G9QeRw65K776qtuoMG2bdVTVFx2mQuAdfU6R9O+fa6JbOFC92+3bJn7QkhJcWOnc3NdkMnJqdEZ0epqKqqu/eXzz6t/Li9dCs8+i1ZUICKuz6quG3HS0lxvfR2Bomrr0qXJTZ61By3U+8t8/37X/1ZQUL199ZV7r0MHGD68+ofcWWe5YVpNsH38eHq//bbrzY9Cf2eDy+MIrPmrHr4FlR07quc3eest9+sLXF06FERiaVGTL7+EefNcgJk3r3qCr6ys6lrMUTr86y2PgwfdF/Jrr7kexx07XPC88ELXtHXxxbFTDg1RWuru9gw1l61Y4X7yhX6J5+VBt258umgRJ19/vWu6bKnan18OHHC1h+3bawaN2s9rd0LUlp3t/r1rB4zOnX3/CE0a/KXqOnjCg0xhYfXY6/79q4NMTo5rXThax4oq3/TuzTHZ2TB7dtM/UDM1dzCcBZV6RCyo7N/vRiaFAknol3+XLu4nQahv5BSfh9xGQmWl6zwM3QNTUNCgDv8a5fHNN+7Y116Df/zD3VOSluZmr7z8crfucaLMO1JcXDPIhN8cEhK687JzZ/drvbHP27f358aFykr3A+JIgWL79rpnEU1Ndf18J5xQ8zH8+bZtcNFFVB44QFJqatN+Esea/fvdfEXhgSY0mCYt7fDaTNeuNY//+GPXPzhtGvz4xy2f/wixoFKPJgeVd99l+4MP0rtXLzfr4rvvVtclR46sDiJZWQ0bEhLL6uvw79evOsC0b8+WP/+Zvqee6jrZ58xxf3xdurh1ji+7zJVJa1gJ7K673Nw1lZUuEOTludFqxcUuuBYXV2+h1xUVRz5ncvLhwaYhQWnjRvdztHdvF5hqB40vvjh8gjMR1wR5pGBxwgmudtmQQFdQwObnnuOU66+P/4BSF1X3NxEeZFatqq7NDBhQszbz/POuz/Dll93Y4zhlQaUeTQoqBQVuSG/oj/HUU11fwAUXwKhRru01UdXu8H/77aomj6q72Y87Dq66ygWS885zfQ+tiddg3eBf56quZhcecBr7vKSk7vlaauvYse4AEZ7Wq1fE/81aXf/Svn2H12bChl0pIMccE9c1tyMFFd8W6YqHrUmLdP2//6eanOwW1klOdq9bq7Iy1RtuUBVx5ZGUpPrb30Y7V9G3ZIl+8sMfqi5Z0jLXO3RIdc8e1U8/VV29WnXRItXrrnP/HqH/p3ff3TJ5qUOrX6SrslJ10ya38l3obyXOvzs4wiJdNqS4sXJzoW1b90u0bVv3urVKTYUbboAXX6z+ZZ5Aiw01WU4Onx04wCkt9Ss0Kam66SukTRv4v/+rbpZNsJlw44qIa9G47TZ4/fWE/+6I82EpUeDNf7H1+uvjuvoaMVYesSk0T8tvfmP/LrGilfytWE2lKVr6l2iss/KITUe6OdZERyv4W/G1piIiY0Vkg4hsEpE76tnnShFZJyJrReTFsPT7RWSNt10Vlv6siKwSkdUi8oqIpHnpqSLyknet90Wkj5+fzRhjzOF8CyreOvPTgG/jVnG8RkQG1tqnP261xpGqeiZwq5c+DsgCMoERwM+9JYQBblPVIaqaAXwGTPbSbwB2q2o/4GHgfr8+mzHGmLr5WVMZDmxS1c2qehCYAYyvtc+NwDRV3Q2gqt5dRAwEFqlqharuA1YDY719SgDELWB/DG6EHt65/+Q9fwU439vHGGNMC/GzT6U3sC3sdRGu1hFuAICIvAckA/eo6lxgFXC3iDwItAfygHWhg0TkeeAiL+0/al9PVStEpBjoCtS4HVhEJgGTAHr27El+fn6TPlxpaWmTj01EVh41WXlUs7KoKdHLI9od9W2A/kAukA4sEpHBqjpPRIYBS4CdQAFQtdipqv7Aa157DLgKeL6hF1TVp4CnwN382NSbslrdDV1HYeVRk5VHNSuLmhK9PPxs/toOnBj2Ot1LC1cEzFbVclXdAmzEBRlUdaqqZqrqaNzN2hvDD1TVQ7gmtctrX09E2gCdgVqLehpjjPGTnzWVZUB/EemL+8K/Gvj3WvvMAq4BnheRbrjmsM1eLeRYVd0lIhlABjDP6yM5VVU3ec8vAT7yzjUb+D6uVjMBeNu787NeK1as+FpEPm3i5+tGraa1Vs7KoyYrj2pWFjUlQnmcXN8bvgUVr19jMhDE9Zc8p6prReRe3C3+s733xojIOlzz1hQvkLQDFnv97CXAtd75koA/eSPBBNf3crN3yWeBv4jIJuBfuCB2tDx2b+rnE5HlWt/cN62QlUdNVh7VrCxqSvTyaNUTSjZHov/HaCwrj5qsPKpZWdSU6OVh07QYY4yJGAsqTfdUtDMQY6w8arLyqGZlUVNCl4c1fxljjIkYq6kYY4yJGAsqxhhjIsaCShM0ZPbl1kJEThSRhWEzTf8s2nmKNhFJFpF/isjr0c5LtInIsd5s4h+JyHoRSdw5349CRG7z/kbWiMjfvFsnEo4FlUZqyOzLrUwF8B+qOhA4C7illZcHwM+A9dHORIz4H2Cuqp4ODKGVlouI9AZ+CmSr6iDcvXtHvZcuHllQabyGzL7caqjqF6q60nu+F/el0Tu6uYoeEUkHxgHPRDsv0SYinYFzcTcmo6oHVXVPdHMVVW2AY7xppNoDn0c5P76woNJ4dc2+3Gq/RMN5C6MFgPejm5OoegT4BVAZ7YzEgL64CWGf95oDnxGRDtHOVDSo6nbgAdwaUF8Axao6L7q58ocFFRMR3gqcrwK3hta8aW1E5GJgh6quiHZeYkQb3GJ7T6pqANgHtMo+SBHpgmvR6AucAHQQkWujmyt/WFBpvIbMvtyqiEgKLqBMV9XXop2fKBoJXCIiW3HNot8Skb9GN0tRVQQUqWqo5voKLsi0RhcAW1R1p6qWA68BZ0c5T76woNJ4VbMvi0hbXGfb7CjnKWq82aKfBdar6kPRzk80qeqdqpquqn1w/y/eVtWE/DXaEKr6JbBNRE7zks4nbLG9VuYz4CwRae/9zZxPgg5aiPYiXXGnvtmXo5ytaBoJfA/4UEQKvbT/VNU5UcyTiR0/AaZ7P8A2Az+Icn6iQlXfF5FXgJW4EZP/JEGna7FpWowxxkSMNX8ZY4yJGAsqxhhjIsaCijHGmIixoGKMMSZiLKgYY4yJGAsqxsQpEcm1mZBNrLGgYowxJmIsqBjjMxG5VkQ+EJFCEflfb72VUhF52FtfY4GIdPf2zRSRpSKyWkRmenNGISL9ROQtEVklIitF5FTv9Glh65VM9+7WNiZqLKgY4yMROQO4ChipqpnAIeC7QAdguaqeCbwD3O0d8mfgl6qaAXwYlj4dmKaqQ3BzRn3hpQeAW3Fr+5yCm+HAmKixaVqM8df5wFBgmVeJOAbYgZsa/yVvn78Cr3nrjxyrqu946X8C/k9EOgK9VXUmgKqWAXjn+0BVi7zXhUAf4F3/P5YxdbOgYoy/BPiTqt5ZI1Hk17X2a+p8SQfCnh/C/qZNlFnzlzH+WgBMEJEeACJynIicjPvbm+Dt8+/Au6paDOwWkVFe+veAd7wVNYtE5DveOVJFpH2LfgpjGsh+XSNiNwAAAHxJREFU1RjjI1VdJyK/AuaJSBJQDtyCW7BquPfeDly/C8D3gT96QSN8Vt/vAf8rIvd657iiBT+GMQ1msxQbEwUiUqqqadHOhzGRZs1fxhhjIsZqKsYYYyLGairGGGMixoKKMcaYiLGgYowxJmIsqBhjjIkYCyrGGGMi5v8D3y4qHQ0cjPQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0:00:12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "U3ipki3vWGzO",
        "outputId": "19128bd5-b374-40b6-9bea-f3609ca34fb2"
      },
      "source": [
        "start = time.time()\n",
        "# 라이브러리\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='normal',activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='linear'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='softmax'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='linear'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "hist = model.fit(X3_train, rorl_train, epochs=10, batch_size=64)\n",
        "\n",
        "    \n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(lg_x_test, lg_y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6940 - accuracy: 0.4971\n",
            "Epoch 2/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.5023\n",
            "Epoch 3/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6936 - accuracy: 0.4943\n",
            "Epoch 4/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.5019\n",
            "Epoch 5/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.5000\n",
            "Epoch 6/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.5001\n",
            "Epoch 7/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4972\n",
            "Epoch 8/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4998\n",
            "Epoch 9/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4953\n",
            "Epoch 10/10\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "260/260 [==============================] - 0s 993us/step - loss: 0.6932 - accuracy: 0.4965\n",
            "loss_and_metrics : [0.6932438015937805, 0.49650853872299194]\n",
            "0:00:11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgAegL5UMqRV",
        "outputId": "c249bd56-fb82-4399-e58b-d72059c2c097"
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn import metrics   \n",
        "\n",
        "#분류->object 회귀\n",
        "start = time.time()\n",
        "\n",
        "# 부스팅 타입은 default값인 gbdt로, 학습률은 0.01로 지정 하였음\n",
        "LGB = lgb.LGBMClassifier(objective=\"regression\", boosting_type='gbdt', learning_rate = 0.01)\n",
        "\n",
        "param_list = {\"n_estimators\": list(range(10, 300, 10)),\n",
        "              \"max_depth\": list(range(4, 21, 4)),\n",
        "              \"max_features\": list(range(3, 13, 2)),\n",
        "              \"min_samples_split\": list(range(3, 13, 2))}\n",
        "\n",
        "# 하이퍼파라미터 최적화\n",
        "LGB_random_search = RandomizedSearchCV(estimator = LGB,\n",
        "                                        param_distributions = param_list,\n",
        "                                        n_iter = 10,      # 10번반복하는 lightgbm 구현 : 성능개선 시도\n",
        "                                        cv = 3,           # cross-validation 3번 반복\n",
        "                                        n_jobs = 10,\n",
        "                                        random_state=42)\n",
        "\n",
        "LGB_random_search.fit(X3_train, rorl_train)\n",
        "y_pred = LGB_random_search.predict(lg_x_test)\n",
        "\n",
        "#성능평가\n",
        "print('정확도 :', metrics.accuracy_score(lg_y_test, y_pred))\n",
        "\n",
        "print( LGB_random_search.best_params_ )\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정확도 : 0.48338550445461115\n",
            "{'n_estimators': 30, 'min_samples_split': 5, 'max_features': 9, 'max_depth': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0UmcqaZcVBD"
      },
      "source": [
        "RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxQU4yxPMqUM"
      },
      "source": [
        "# 데이터 변환\n",
        "# LTSM : 입력을 (샘플 수, 타입스텝, 특성 수)로 형태 변환\n",
        "x_train = np.array(X3_train).reshape((X3_train.shape[0], X3_train.shape[1], 1))\n",
        "y_train = np.array(rorl_train).reshape((rorl_train.shape[0], 1))\n",
        "x_test = np.array(lg_x_test).reshape((lg_x_test.shape[0], lg_x_test.shape[1],1))\n",
        "y_test = np.array(lg_y_test).reshape((lg_y_test.shape[0], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "sKTxMz79MqXO",
        "outputId": "6410d07b-361b-4f48-da7d-673e248cfd18"
      },
      "source": [
        "start = time.time()\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Activation, Dropout, LSTM\n",
        "\n",
        "# RNN 모델 구성\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(65, input_shape = (65,1), return_sequences = True ,activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(130, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(65, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(32, return_sequences = True, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "hist = model.fit(x_train, y_train, epochs=10, batch_size=64 )\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "520/520 [==============================] - 39s 74ms/step - loss: 0.6949 - accuracy: 0.5009\n",
            "Epoch 2/10\n",
            "520/520 [==============================] - 40s 77ms/step - loss: 0.6939 - accuracy: 0.4988\n",
            "Epoch 3/10\n",
            "520/520 [==============================] - 40s 77ms/step - loss: 0.6936 - accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "520/520 [==============================] - 40s 77ms/step - loss: 0.6935 - accuracy: 0.4988\n",
            "Epoch 5/10\n",
            "520/520 [==============================] - 40s 77ms/step - loss: 0.6935 - accuracy: 0.4998\n",
            "Epoch 6/10\n",
            "520/520 [==============================] - 40s 77ms/step - loss: 0.6935 - accuracy: 0.4997\n",
            "Epoch 7/10\n",
            "520/520 [==============================] - 40s 78ms/step - loss: 0.6935 - accuracy: 0.5000\n",
            "Epoch 8/10\n",
            "520/520 [==============================] - 40s 77ms/step - loss: 0.6934 - accuracy: 0.4978\n",
            "Epoch 9/10\n",
            "520/520 [==============================] - 40s 77ms/step - loss: 0.6933 - accuracy: 0.5067\n",
            "Epoch 10/10\n",
            "520/520 [==============================] - 43s 82ms/step - loss: 0.6933 - accuracy: 0.5023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "260/260 [==============================] - 5s 18ms/step - loss: 0.6933 - accuracy: 0.5022\n",
            "loss_and_metrics : [0.6933420300483704, 0.5021671056747437]\n",
            "0:06:51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHi-SIe9Y3c_",
        "outputId": "cba5d364-a506-41d0-c028-38b68db3e121"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_2 (SimpleRNN)     (None, 65, 65)            4355      \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 65, 65)            0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 65, 130)           8580      \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 65, 130)           0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 65, 65)            8515      \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 65, 65)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 65, 32)            12544     \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 65, 1)             33        \n",
            "=================================================================\n",
            "Total params: 34,027\n",
            "Trainable params: 34,027\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1qjFfmiia8s"
      },
      "source": [
        "# 데이터셋 ro,rl 심각도 1,2단계 없애기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-4gfkQCgcke",
        "outputId": "ee03a919-90a2-4c0d-c50f-c56350260255"
      },
      "source": [
        "# 데이터 로드\n",
        "\n",
        "\n",
        "# 정상데이터 (normal1,normal2)\n",
        "start = time.time()\n",
        "n = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/1_Benchmark Tests/normal.xls\",sheet_name=\"Complete Data Set\")\n",
        "n1 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/1_Benchmark Tests/normal1.xls\",sheet_name=\"Complete Data Set\")\n",
        "n2 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/1_Benchmark Tests/normal2.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"normal\", clock(start) )\n",
        "\n",
        "# 고장데이터\n",
        "# condenser fouling\n",
        "start = time.time()\n",
        "cf12 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/2_Condenser fouling/cf12.xls\",sheet_name=\"Complete Data Set\")\n",
        "cf20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/2_Condenser fouling/cf20.xls\",sheet_name=\"Complete Data Set\")\n",
        "cf30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/2_Condenser fouling/cf30.xls\",sheet_name=\"Complete Data Set\")\n",
        "cf45 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/2_Condenser fouling/cf45.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"cf\", clock(start) )\n",
        "\n",
        "# Refrigerant overcharge\n",
        "start = time.time()\n",
        "#ro10 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro10.xls\",sheet_name=\"Complete Data Set\")\n",
        "#ro20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro20.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro30.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro40 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"ro\",  clock(start) )\n",
        "\n",
        "#Excess oil\n",
        "start = time.time()\n",
        "eo14 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/4_Excess oil/eo14.xls\",sheet_name=\"Complete Data Set\")\n",
        "eo32 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/4_Excess oil/eo32.xls\",sheet_name=\"Complete Data Set\")\n",
        "eo50 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/4_Excess oil/eo50.xls\",sheet_name=\"Complete Data Set\")\n",
        "eo68 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/4_Excess oil/eo68--unsteady test1.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"eo\", clock(start) )\n",
        "\n",
        "# Reduced condenser water flow\n",
        "start = time.time()\n",
        "fwc10 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/5_Reduced condenser water flow/fwc10.xls\",sheet_name=\"Complete Data Set\")\n",
        "fwc20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/5_Reduced condenser water flow/fwc20.xls\",sheet_name=\"Complete Data Set\")\n",
        "fwc30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/5_Reduced condenser water flow/fwc30.xls\",sheet_name=\"Complete Data Set\")\n",
        "fwc40 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/5_Reduced condenser water flow/fwc40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"fwc\",  clock(start) )\n",
        "\n",
        "# Non condensables in refrigerant\n",
        "start = time.time()\n",
        "nc1 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/6_Non-condensables in refrigerant/nc1.xls\",sheet_name=\"Complete Data Set\")\n",
        "nc2 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/6_Non-condensables in refrigerant/nc2.xls\",sheet_name=\"Complete Data Set\")\n",
        "nc3 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/6_Non-condensables in refrigerant/nc3.xls\",sheet_name=\"Complete Data Set\")\n",
        "nc5 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/6_Non-condensables in refrigerant/nc5.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"nc\",  clock(start) )\n",
        "\n",
        "# 7_reduced evaporator water flow\n",
        "start = time.time()\n",
        "fwe10 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/7_Reduced evaporator water flow/fwe10.xls\",sheet_name=\"Complete Data Set\")\n",
        "fwe20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/7_Reduced evaporator water flow/fwe20.xls\",sheet_name=\"Complete Data Set\")\n",
        "fwe30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/7_Reduced evaporator water flow/fwe30.xls\",sheet_name=\"Complete Data Set\")\n",
        "fwe40 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/7_Reduced evaporator water flow/fwe40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"fwe\",  clock(start) )\n",
        "\n",
        "# 8_refrigerant leak\n",
        "start = time.time()\n",
        "#rl10 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl10.xls\",sheet_name=\"Complete Data Set\")\n",
        "#rl20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl20.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl30.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl40 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl40.xls\",sheet_name=\"Complete Data Set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normal 0:00:07\n",
            "cf 0:00:08\n",
            "ro 0:00:04\n",
            "eo 0:00:09\n",
            "fwc 0:00:08\n",
            "nc 0:00:08\n",
            "fwe 0:00:08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh5xgfXAjMej"
      },
      "source": [
        "normal = pd.concat([n,n1,n2],axis=0)\n",
        "cf = pd.concat([cf12,cf20,cf30,cf45],axis=0)\n",
        "ro = pd.concat([ro30,ro40], axis=0)\n",
        "eo= pd.concat([eo14,eo32,eo50,eo68], axis=0)\n",
        "fwc= pd.concat([fwc10,fwc20,fwc30,fwc40], axis=0)\n",
        "nc= pd.concat([nc1,nc2,nc3,nc5], axis=0)\n",
        "fwe= pd.concat([fwe10,fwe20,fwe30,fwe40], axis=0)\n",
        "rl= pd.concat([rl30,rl40], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHWE3CQRMqjb"
      },
      "source": [
        "normal['rorl'] = 0\n",
        "normal['detect'] = 0\n",
        "normal['target'] = 1\n",
        "\n",
        "cf['rorl'] = 0\n",
        "cf['detect'] = 1\n",
        "cf['target'] = 2\n",
        "\n",
        "ro['rorl'] = 3\n",
        "ro['detect'] = 1\n",
        "ro['target'] = 3\n",
        "\n",
        "eo['rorl'] = 0\n",
        "eo['detect'] = 1\n",
        "eo['target'] = 4\n",
        "\n",
        "fwc['rorl'] = 0\n",
        "fwc['detect'] = 1\n",
        "fwc['target'] = 5\n",
        "\n",
        "nc['rorl'] = 0\n",
        "nc['detect'] = 1\n",
        "nc['target'] = 6\n",
        "\n",
        "fwe['rorl'] = 0\n",
        "fwe['detect'] = 1\n",
        "fwe['target'] = 7\n",
        "\n",
        "rl['rorl'] = 8\n",
        "rl['detect'] = 1\n",
        "rl['target'] = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk6_D4VcW8UJ",
        "outputId": "bed8c6d7-8322-4ce3-86aa-8a60f0fdcca1"
      },
      "source": [
        "start = time.time()\n",
        "# 라이브러리\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='normal',activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "model.fit(X3_train, rorl_train, epochs=10, batch_size=64)\n",
        "\n",
        "# 모델 예측 (예측 Test 파일 넣으세용) - Accuracy 구하는 용도\n",
        "#pred = model.predict(lg_x_test)\n",
        "#print('정확도 :', metrics.accuracy_score(lg_y_test, pred))\n",
        "\n",
        "##### 모델 평가(여기에 Test 파일 넣으세용) #####\n",
        "loss_and_metrics = model.evaluate(lg_x_test, lg_y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.7096 - accuracy: 0.4965\n",
            "Epoch 2/10\n",
            "260/260 [==============================] - 0s 2ms/step - loss: 0.6973 - accuracy: 0.5104\n",
            "Epoch 3/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.6111 - accuracy: 0.6358\n",
            "Epoch 4/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.4872 - accuracy: 0.7920\n",
            "Epoch 5/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.3861 - accuracy: 0.8482\n",
            "Epoch 6/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.3901 - accuracy: 0.8417\n",
            "Epoch 7/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.3818 - accuracy: 0.8461\n",
            "Epoch 8/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.3500 - accuracy: 0.8608\n",
            "Epoch 9/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.3575 - accuracy: 0.8576\n",
            "Epoch 10/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.3405 - accuracy: 0.8658\n",
            "130/130 [==============================] - 0s 999us/step - loss: 0.1391 - accuracy: 0.9892\n",
            "loss_and_metrics : [0.13914000988006592, 0.9891644716262817]\n",
            "0:00:06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "mHzZ9ZKsW8Uo",
        "outputId": "45411ad8-f73e-4053-bd86-e7281f160c4b"
      },
      "source": [
        "start = time.time()\n",
        "# 라이브러리\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='normal',activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='softmax'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='linear'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='linear'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "history = model.fit(X3_train, rorl_train, epochs=10, batch_size=64, validation_data=(lg_x_test, lg_y_test))\n",
        "\n",
        "# 모델 예측 (예측 Test 파일 넣으세용) - Accuracy 구하는 용도\n",
        "#pred = model.predict(lg_x_test)\n",
        "#print('정확도 :', metrics.accuracy_score(lg_y_test, pred))\n",
        "\n",
        "##### 모델 평가(여기에 Test 파일 넣으세용) #####\n",
        "\n",
        "print('\\nAccuracy: {:.4f}'.format(model.evaluate(lg_x_test, lg_y_test)[1]))\n",
        "\n",
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "\n",
        "x_len = numpy.arange(len(y_loss))\n",
        "plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n",
        "plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "260/260 [==============================] - 1s 3ms/step - loss: 0.6977 - accuracy: 0.4913 - val_loss: 0.6938 - val_accuracy: 0.4878\n",
            "Epoch 2/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.6923 - accuracy: 0.5113 - val_loss: 0.6971 - val_accuracy: 0.4878\n",
            "Epoch 3/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.6855 - accuracy: 0.5250 - val_loss: 0.6292 - val_accuracy: 0.8083\n",
            "Epoch 4/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.4289 - accuracy: 0.7787 - val_loss: 0.1710 - val_accuracy: 0.9376\n",
            "Epoch 5/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.2749 - accuracy: 0.8470 - val_loss: 0.0390 - val_accuracy: 0.9841\n",
            "Epoch 6/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.2365 - accuracy: 0.8623 - val_loss: 0.0319 - val_accuracy: 0.9892\n",
            "Epoch 7/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.2268 - accuracy: 0.8629 - val_loss: 0.0241 - val_accuracy: 0.9911\n",
            "Epoch 8/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.2260 - accuracy: 0.8598 - val_loss: 0.0199 - val_accuracy: 0.9913\n",
            "Epoch 9/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.2104 - accuracy: 0.8694 - val_loss: 0.0242 - val_accuracy: 0.9913\n",
            "Epoch 10/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.2120 - accuracy: 0.8635 - val_loss: 0.0182 - val_accuracy: 0.9887\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 0.0182 - accuracy: 0.9887\n",
            "\n",
            "Accuracy: 0.9887\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU5fX48c/Jzr4piAQFFVRkJ0gjskxxwQ1EQUFFEZVqRRGVFix1rT/tF4u4UK2yuVARFxAFixUTRUVREBc2QUAMWkQUSIAEkpzfH09CFiYhmczNZGbO+/Wa18yduffOycMwZ+6ziqpijDEmesWEOgBjjDGhZYnAGGOinCUCY4yJcpYIjDEmylkiMMaYKBcX6gAq66ijjtJWrVoFdOzevXupU6dOcAMKY1YeJVl5FLGyKCkSymPFihW/qOrR/l4Lu0TQqlUrPv/884COTU9Pp2/fvsENKIxZeZRk5VHEyqKkSCgPEfm+rNesasgYY6KcJQJjjIlylgiMMSbKhV0bgTGmbAcPHiQjI4Ps7OwqnadBgwasXbs2SFGFv3Aqj6SkJJKTk4mPj6/wMZ4mAhHpDzwGxALTVPXhUq8/CvgKNmsDTVW1oZcxGRPJMjIyqFevHq1atUJEAj5PZmYm9erVC2Jk4S1cykNV2blzJxkZGbRu3brCx3mWCEQkFpgKnA1kAJ+JyAJVXVO4j6qOLbb/LUAXr+IxJhpkZ2dXOQmY8CUiNGnShB07dlTqOC/bCE4HNqrqJlU9AMwBBpaz/zDgJa+CWbYMZs8+jmXLvHqHSgTy0EOEPhATqSwJRLdA/v29rBpqAfxQbDsD6OFvRxE5HmgNvOdFIMuWwfg+yzjjYBp/mukj9fZUOnaEunWLbnXqlNxOSoIKl2d+Phw4ANnZkJPjbv4er1oFd90FubmQmAhLlkBqqhd/sjHGVFhNaSweCryqqnn+XhSRUcAogGbNmpGenl6pky+b/Av/OTicBHLIy4tj6qQ/8hXHkkgOSWSTSI6fxznUjt1PrZhskiSHWpJNEjkkkEOSZpOgOcTrARLyc4jLP1jpP1j372frE0+wOSen0scGS1ZWVqXLMpJFQnk0aNCAzMzMKp8nLy8voPPs3LmTAQMGALB9+3ZiY2M56qijAEhLSyMhIaHMY1euXMlLL73EpEmTAgv6CHbt2sUrr7zCDTfc4Pf15s2b89NPP/l9LdDyCJXs7OxKfZbFq4VpRCQVuFdVzy3YngCgqg/52fcL4GZV/fhI501JSdHKjiz+/qaHSH76L8Ry+N+qMTHkxSeRH5dIblwiuXFJHIxJ5KAkkiNJ5EgiOZrIfk1if34i+/MT2ZuXxN7cRPbmJpJ5IJF9mlSQPhJx6eLwxzkkchIbmMpo4jlADEo2SbzY+m7eOW0stRsn0aABFbrVrl2Jq5VyRMJoyWCKhPJYu3Ytp556apXPE4zG0XvvvZe6dety5513HnouNzeXuLjQ/P7csmULF154Id98843f1+vWrUtWVpbf18KlsbiQv8+BiKxQ1RR/+3v5L/IZ0EZEWgPbcL/6ryi9k4icAjQCPKs0P/7qvuTNTCLvQA7EJxC7YD6ceSYkJiJxcYcKoezfKuU7cACyskre9u49/LlFi/rw+3dPow/pbKANf6g7mxs230X/bdO4t/6jPH/gIvZkCkfKzXFxUL9+xZJGWbevvnJtJomJVjsV7ZYtg/R06NvXm8/CiBEjSEpK4osvvqBnz54MHTqUMWPGkJ2dTa1atZg5cyYnn3wy6enpPPLII7z11lvce++9bN26lU2bNrF161Zuu+02br311sPO/f777zNmzBjA1Y1/8MEH1KtXj0mTJjF37lxycnIYNGgQ9913H+PHj+e7776jc+fOnH322RW68li1ahU33ngjWVlZtGnThhkzZtCoUSMef/xxnn76aeLi4mjXrh1z5swpM5Zw4FkiUNVcERkNLMZ1H52hqqtF5H7gc1VdULDrUGCOerlmZmoqsWlL2DRjBieMHBn0T3tCAjRu7G7l+d3voN9HqSw/kEpCAtzxzmDI+i8tx4xh+tqBTD/nHPInTyGr5ans3k2lblu2wJ49Rdv5+RWJvDWzZ1tTRaS67TbXLFWe3bvdj4L8fIiJgY4d3Q+FvLxaxMYevn/nzjBlSuVjycjI4OOPPyY2NpY9e/awdOlS4uLiePfdd7nrrrt47bXXDjtm3bp1pKWlkZmZycknn8xNN910WN/4Rx55hKlTp9KzZ0+ysrJISkrinXfeYcOGDSxfvhxVZcCAAXzwwQc8/PDDfPPNN6w6UqEUc/XVV/PEE0/QtWtXJk2axH333ceUKVN4+OGH2bx5M4mJiezatavMWMKFp9doqroIWFTqubtLbd/rZQyHpKayNSeHE0L4jZea6r50S/76Ohu+/BL++U+45x5iOnek/i23UP+ee2jZskFA76PqrkjKShrz5sF//gMgHDjg4rFEEJ2K/2jIz3fbDQL72JVryJAhxBZklt27d3PNNdewYcMGRISDB/23sV1wwQUkJiaSmJhI06ZN2b59O8nJySX26dmzJ7fffjtXXnkll1xyCcnJybzzzju88847dOnieqNnZWWxYcMGjjvuuErFvHv3bnbt2kWfPn3IzMzkmmuuYciQIQB07NiRK6+8kosvvpiLL764zFjCRU1pLI4aqal+vnTj42HMGBg2DCZOdD+5Zs+G//f/4Npr3U+1ShAp6v3UosXhr3fo4L78s7MVESHMq8VNGSryy33ZMujXz1VvJiS4j11qKmRm7g9qtUbxKZz/+te/4vP5mDdvHlu2bCmzXSYxMfHQ49jYWHJzc5k6dSrPPvssAIsWLWL8+PFccMEFLFq0iJ49e7J48WJUlQkTJvCHP/yhxPm2bNkStL9n4cKFfPDBB7z55ps8+OCDfP31135jOeWUU4L2nl6yuYZqkqZN4Zln4LPP4KST4PrroUePoI85SE2F996D1q33kpQE7dsH9fQmjBRepT7wQPVVEe7evZsWBb9QZs2aValjb775ZlatWsWqVas49thj+e677+jQoQN//vOf6d69O+vWrePcc89lxowZhxp+t23bxs8//0y9evUq1fOnQYMGNGrUiKVLlwLwwgsv0KdPH/Lz8/nhhx/w+Xz8/e9/Z/fu3WRlZfmNJVxYIqiJunWDDz+EF1+EH3+EM86Aq692j4MkNRXuvHM9WVkwY0bQTmvCUGoqTJhQfdWDf/rTn5gwYQJdunQhNze3SueaMmUK7du3p2PHjsTHx3PeeedxzjnncMUVV5CamkqHDh0YPHgwmZmZNGnShJ49e9K+fXvGjRt32Ln27dtHcnLyodvkyZN57rnnGDduHKmpqaxatYq7776bvLw8rrrqKjp06ECXLl249dZbadiwod9YwoaqhtWtW7duGqi0tLSAjw2ZzEzVCRNUExJU69RRffhh1ezsoJw6LS1NzzxT9fjjVQ8eDMopw1pYfj5KWbNmTVDOs2fPnqCcJ1KEW3n4+xzgOun4/V61K4Karm5d11awZo2rzB0/3tXlvPUWR+xnWgF33gnffw9+Om0YY6KEJYJwceKJ8MYbrrtPXBxcdBGcfz6sX1+l0150EbRpA5MmBSWvGGPCkCWCcHPuua7j9+TJ8PHH7urgzjvdIIIAxMTAHXfAihXwwQdBjtUYExYsEYSj+HgYOxa+/RauucYlhbZtYebMio4kK+Hqq+Goo+Af//AgVmNMjWeJIJw1awbTpsHy5XDCCVA4avrTTyt1mlq1YPRoePNNCKMeb8aYILFEEAlSUlx30+efhx9+cHNZjBgBZcyk6M8f/+im3p482bswjTE1kyWCSBETA8OHu8bj8ePhpZdcddGkSW7Y6BEcfbTLHc8/D9u3ex+uiUw+n4/FixeXeG7KlCncdNNNZR7Tt29fCmcUPv/88w/N3VPcvffeyyOPPFLue8+fP581aw4tgMjdd9/Nu+++W5nwgyI9PZ2PP/Y/kfKsWbMYPXp0NUd0ZJYIIk29em4FtG++cRMa/elPrkF50aIjHjp2rMsZU6d6H6aJTMOGDWPOnDklnpszZw7Dhg2r0PGLFi2iYcPAli0vnQjuv/9+zjrrrIDOVRXlJYKayhJBpGrTxlX6v/22u1q44AJ3+/bbMg9p2xYGDnSJYN++aozVhFYQl08dPHgwCxcu5EDBVeiWLVv48ccf6dWrFzfddBMpKSmcdtpp3HPPPX6Pb9WqFb/88gsADz74IG3btuXMM89kfbFu0s8++yzdu3enU6dOXHrppezbt4+PP/6YBQsWMG7cODp37sx3333HiBEjePXVVwFYsmQJXbp0oUOHDowcOZKcggWhWrVqxT333EPXrl3p0KFDmdNC3HPPPbRr146OHTseWl9hx44dXHrppXTv3p3u3bvz0UcfsWXLFp5++mkeffRROnfufGh6iiOZPHky7du3p3379kwpmCRq7969XHDBBXTq1In27dvz8ssvAzB+/PjDYqkqm3Qu0vXv77qbPvEE3Hefuzq47TY45xyOe/llSi9IcOedMH8+zJrl2g1MGKvCPNS18vIIZB7qxo0bc/rpp/P2228zcOBA5syZw2WXXYaI8OCDD9K4cWPy8vLo168fX331FR07dvR7nhUrVjBnzhxWrVpFbm4uXbt2pVu3bgBccsklh1YZmzhxItOnT+eWW25hwIABXHjhhQwePLjEubKzsxkxYgRLliyhbdu2XH311Tz11FPcdtttABx11FGsXLmSf/7znzzyyCNMmzatxPE7d+7kzTff5Ntvv0VEDlVdjRkzhrFjx3LmmWeydetWzj33XNauXcuNN9542II85VmxYgUzZ87k008/RVXp0aMHffr0YdOmTRx77LEsXLgQcHM07dy5k3nz5rFu3boSsVSVXRFEg4QEN1jg22/hqqtcu8HZZ9N6+nQ3WrnYL8EzznBtzZMnQ57fhUNNRPE3D3UVFa8eKl4tNHfuXLp27UqXLl1YvXp1iWqc0pYuXcqgQYOoXbs29evXP7T8JcA333xDr1696NChA7Nnz2b16tXlxrN+/Xpat25N27ZtAbjmmmv4oNigmUsuuQSAbt26+Z2htEGDBiQlJXHdddfx+uuvU7t2bQDeffddRo8eTefOnRkwYAB79uwpc4Wz8nz44YcMGjSIOnXqULduXS655BKWLl1Khw4d+O9//8uf//xnli5dSoMGDcqMparsiiCaHHOMm2EuKQmeegpRpfSCBCLuqmDwYDeQueD/iAlHVZiHen8VlmYcOHAgY8eOZeXKlezbt49u3bqxefNmHnnkET777DMaNWrEiBEjyM7ODuj8I0aMYP78+XTq1IlZs2ZVeZ3pwumuC6e6Bjj33HPZvn07KSkpTJs2jbS0NJYvX86rr77Kk08+yXvvvUd+fj6ffPKJZwvQtG3blpUrV7Jo0SImTpxIv379uPvuu1m+fDlLliwpEUtV2RVBNBo+HOLi3ArOCQmUXpDg4ovdsIQjdNIwkcCDeajr1q2Lz+dj5MiRh64G9uzZQ506dWjQoAHbt2/n7bffLvccvXv3Zv78+ezfv5/MzEzefPPNQ69lZmbSvHlzDh48yOzZsw89X9Y00yeffDJbtmxh48aNQNF00uVZvHgxq1atYtq0aWRlZbFnzx7OP/98Hn30Ub788ksAzjnnHJ544olDxxSufFbZ6a579erF/Pnz2bdvH3v37mXevHn06tWLH3/8kdq1a3PVVVcxbtw4Vq5cSVZWFrt37z4slqqyK4JolJoKjz6K3HIL3H//Yf/5Y2Ph9tvdILOPP3bVRSaC+V0tqWqGDRvGoEGDDlURderUiS5dunDKKafQsmVLevbsWe7xXbt25fLLL6dTp040bdqU7t27H3rtgQceoEePHhx99NH06NHj0Jfu0KFDueGGG3j88ccPNRIDJCUlMXPmTIYMGUJubi7du3fnxhtvrPDfkpmZyZAhQzh48CCqyuSCwTaPP/44N998Mx07diQ3N5fevXvz9NNPc9FFFzF48GDeeOMNnnjiCXr16lXifLNmzWL+/PmHtj/55BNGjBjB6aefDsD1119Ply5dWLx4MePGjSMmJob4+HieeuopMjMzGThwINnZ2SViqSrRMJtpLCUlRQv7HFdWenp6mashRZ3sbPLr1yfmllv8zi2xdy+0bOkuFl5/vfrDC4VI+HysXbuWU089tcrnyaxC1VAkCrfy8Pc5EJEVqprib3+rGopWSUnsPu00SEvz+3KdOq7X0Pz5sGFDNcdmjKlWlgii2K7OnV33wt9+8/v66NFufrtHH63mwIwx1crTRCAi/UVkvYhsFJHxZexzmYisEZHVIvJvL+MxJe3q0sUtQlDG/NPHHOPalWfOhB07qjk4E7Bwq+41wRXIv79niUBEYoGpwHlAO2CYiLQrtU8bYALQU1VPA27zKh5zuD2nnOK6kpZRPQSu0Tg7G556qhoDMwFLSkpi586dlgyilKqyc+fOSndp9bLX0OnARlXdBCAic4CBQPFRJDcAU1X1NwBV/dnDeEwpmpAAPXuWmwjatXMzUzz5JIwb56asNjVXcnIyGRkZ7KjiJVx2drZn/ePDUTiVR1JSEsnJyZU6xstE0AL4odh2BtCj1D5tAUTkIyAWuFdV/1P6RCIyChgF0KxZs4AHkGRlZVV58EkkycrKYlOrVpywZAkfvfEGBxs08LvfWWc1ZOHCzkycuJ6LLqr41Nbhxj4fRbKysqhbt26ow6gxwq08vv/++8odUNaq9lW9AYOBacW2hwNPltrnLWAeEA+0xiWOhuWdt1u3bhqotLS0gI+NRGlpaaoffaQKqq++WuZ++fmq3bqptm2rmpdXffFVN/t8FLGyKCkSygP4XMv4XvWysXgb0LLYdnLBc8VlAAtU9aCqbga+Bdp4GJMprXt311e0nOqhwmknvv0W3nqrGmMzxlQLLxPBZ0AbEWktIgnAUGBBqX3mA30BROQoXFXRJg9jMqXFx8OZZ5abCMDNPXTccTbthDGRyLNEoKq5wGhgMbAWmKuqq0XkfhEpnEpwMbBTRNYAacA4Vd3pVUymDD4frFlT7tJkcXFu4ZqlSyu9JLIxpobzdByBqi5S1baqeqKqPljw3N2quqDgsarq7araTlU7qOqc8s9oPOHzufsjNJRedx00aOB3RgpjTBizkcUGunZ1S1weoXqoXj248UZ47TXYZBV4xkQMSwTG1fv07n3ERABwyy1udtKKTHVvjAkPlgiM4/O5bkE//ljubi1awBVXwPTp8Ouv1RSbMcZTlgiMU9hOUIGrgjvucIvbP/20xzEZY6qFJQLjdOoEDRtWKBF06ADnnguPPw45OdUQmzHGU5YIjBMbC336VCgRgBtgtn27W+LWGBPeLBGYIj6f6w60desRd+3Xz11E/OMfbiZrY0z4skRgilSinaBw2ok1a+A/h00TaIwJJ5YITJH27aFJkwpXD11+uetFZNNOGBPeLBGYIjExbrX6Ck7FHB8Pt90G770HK1d6GpkxxkOWCExJPh98/z1s3lyh3W+4wY04tmknjAlflghMSX37uvsKVg81aACjRsHLL1eojdkYUwNZIjAltWsHTZtWOBEAjBnjGo8fe8zDuIwxnrFEYEoScVcFaWkV7hfasqVrOH7mGdi1y9vwjDHBZ4nAHM7ng23bYOPGCh9yxx2QlQXPPuthXMYYT1giMIerxHiCQl26uEFmjz0GBw54FJcxxhOWCMzh2raF5s0rlQjADTDbts01HBtjwoclAnM4EXdVUIl2AnAT0Z12mhtgZtNOGBM+LBEY/3w+N6vcunUVPkTEtRV89RW8+66HsRljgsoSgfEvgHYCcIvWHHOMTTthTDjxNBGISH8RWS8iG0VkvJ/XR4jIDhFZVXC73st4TCWccILrF1rJRJCYCLfeCu+8464MjDE1n2eJQERiganAeUA7YJiItPOz68uq2rngNs2reEwlFbYTpKdDfn6lDv3DH6BOHZt2wphw4eUVwenARlXdpKoHgDnAQA/fzwSbzwe//AKrV1fqsMaN4brr4N//howMj2IzxgRNnIfnbgH8UGw7A+jhZ79LRaQ38C0wVlV/KL2DiIwCRgE0a9aM9ArOjllaVlZWwMdGoiOVR2JSEqnAhmeeYdull1bq3L/7XRJPPtmDceN+4A9/2FS1QKuJfT6KWFmUFPHloaqe3IDBwLRi28OBJ0vt0wRILHj8B+C9I523W7duGqi0tLSAj41EFSqP1q1VL744oPNfdplq/fqqu3cHdHi1s89HESuLkiKhPIDPtYzvVS+rhrYBLYttJxc8VzwJ7VTVwuXPpwHdPIzHBMLng/ffr3Q7AbgBZnv2wPTpHsRljAkaLxPBZ0AbEWktIgnAUGBB8R1EpHmxzQHAWg/jMYHw+eC33+DLLyt9aPfu0Ls3TJkCBw96EJsxJig8SwSqmguMBhbjvuDnqupqEblfRAYU7HariKwWkS+BW4ERXsVjAhTgeIJCd97p1il49dUgxmSMCSpPxxGo6iJVbauqJ6rqgwXP3a2qCwoeT1DV01S1k6r6VLXiw1hN9WjRAtq0qfDylaVdcAGcfLJNO2FMTWYji82R9e0LH3wAeXmVPjQmxk07sXJlwLnEGOMxSwTmyHw+2L0bvvgioMOHD3eLntm0E8bUTJYIzJFVch3j0pKSYPRoWLSo0mPTjDHVwBKBObLmzeGUUwJOBAA33QS1asHkyUGMyxgTFJYITMX4fLB0acD9QI86Cq69Fl58EX76KcixGWOqxBKBqRifzy1KvGJFwKcYO9blkSefDGJcxpgqs0RgKqaK7QQAJ50EgwbBU0/B3r3BCcsYU3WWCEzFHH00tG9fpUQAboDZb7/BzJlBissYU2WWCEzF+Xzw0Udw4EDAp0hNhTPOcI3GAQxLMMZ4wBKBqTifD/btg+XLq3SaO++EzZth3rwgxWWMqRJLBKbi+vRxK5dVsXpowADXXjBpkk07YUxNYInAVFzjxtCpU5UTQWws3H67u7D46KMgxWaMCZglAlM5Ph98/DFkZ1fpNNdcA02awF13wUMPwbJlQYrPGFNplghM5fh8kJMDn3xSpdPUru2qiJYuhYkToV8/SwbGhIolAlM5vXu7KUWrWD0EbuYKcIufHThgs5MaEyqWCEzlNGgAXbsGJRFceCHEx7vHIkVj1owx1csSgak8n89VDe3bV6XTpKa6q4BOndyYgio2OxhjAmSJwFSez+cmDfr44yqf6owz4MMPoW1buOIK2L49CPEZYyrFEoGpvDPPdH1Ag1A9BFC3Lrzyilv75oorbMSxMdXNEoGpvHr1ICUlqK27HTq4WUnfew/uvz9opzXGVICniUBE+ovIehHZKCLjy9nvUhFREUnxMh4TRD6fGxGWlRW0U157rRtf8MAD8N//Bu20xpgj8CwRiEgsMBU4D2gHDBORdn72qweMAT71KhbjAZ8PcnODOjRYBKZOhVNPhSuvhB9/DNqpjTHl8PKK4HRgo6puUtUDwBxgoJ/9HgD+DlifkXDSs6fr+xmkdoJCdeq49oK9e2HYMJdrjDHeivPw3C2AH4ptZwA9iu8gIl2Blqq6UETGlXUiERkFjAJo1qwZ6QHWTWdlZQV8bCSqanl0Oflk5I03WNm/f/CCKjBmTDMeeuhURoz4nuuv3xz08/tjn48iVhYlRXx5qKonN2AwMK3Y9nDgyWLbMUA60KpgOx1IOdJ5u3XrpoFKS0sL+NhIVOXymDhRNTZWdffuoMRT2vXXq4LqokWenP4w9vkoYmVRUiSUB/C5lvG96mXV0DagZbHt5ILnCtUD2gPpIrIF+B2wwBqMw4jP5/p6Ll3qyekffxw6doThw+GHH468vzEmMF4mgs+ANiLSWkQSgKHAgsIXVXW3qh6lqq1UtRXwCTBAVT/3MCYTTKmpkJAQ9HaCQrVqufaCnBwYOtSNYTPGBJ9niUBVc4HRwGJgLTBXVVeLyP0iMsCr9zXVqFYtlww8SgTgRhw/+6wbxPyXv3j2NsZENS8bi1HVRcCiUs/dXca+fb2MxXjE54P77nMr0jdq5MlbDB0K77/vVjTr1QsuusiTtzEmalXoikBExohIfXGmi8hKETnH6+BMGPD53HqTH3zg6ds8+ih06eIGnH3/vadvZUzUqWjV0EhV3QOcAzTC9QB62LOoTPjo0QOSkjytHgL3Fq+84tqmL7vMrV9gjAmOiiYCKbg/H3hBVVcXe85Es8REN7jM40QAcOKJMGOGm9niz3/2/O2MiRoVTQQrROQdXCJYXDAtRL53YZmw4vPBV1/BL794/laXXgq33AJTpsC8eZ6/nTFRoaKJ4DpgPNBdVfcB8cC1nkVlwovP5+7ff79a3m7SJOje3U1St2lTtbylMRGtookgFVivqrtE5CpgIrDbu7BMWOne3U0SVA3VQ+Bqo+bOdZPUXXaZG2dgjAlcRRPBU8A+EekE3AF8BzzvWVQmvMTHu8VqqikRALRqBbNmwYoVcMcd1fa2xkSkiiaC3IK5Kgbi5guaipsiwhjH54M1a6p1rcmBA+H2293U1XPnVtvbGhNxKpoIMkVkAq7b6EIRicG1Exjj9O3r7qt5hsaHH3aDm6+/HjZsqNa3NiZiVDQRXA7k4MYT/A83gdwkz6Iy4adbN7eEZTUngvh4ePlldz9kCOzfX61vb0xEqFAiKPjynw00EJELgWxVtTYCUyQuzs3/UI3tBIVatoQXXoAvv4Tbbqv2tzcm7FV0ionLgOXAEOAy4FMRGexlYCYM+Xywfn1I1pg8/3w3yOyZZ+Df/672tzcmrFW0augvuDEE16jq1bhlKP/qXVgmLBWOJwjRSk5/+5u7KBk1CtatC0kIxoSliiaCGFX9udj2zkoca6JF587QsGFIqofA1U699BLUru3aC/btC0kYxoSdin6Z/0dEFovICBEZASyk1PTSxhAbC717hywRALRoAS++CKtXw+jRIQvDmLBS0cbiccAzQMeC2zOqatN+mcP5fPDddyFdW/Kcc9wiNjNnwnPPhSwMY8JGhat3VPU1Vb294GbTfRn/CtsJQnhVAHDvvS6Um25yVwfGmLKVmwhEJFNE9vi5ZYrInuoK0oSRDh2gSZOQJ4LYWNd7qH591/4NltQAABZbSURBVF6QlRXScIyp0cpNBKpaT1Xr+7nVU9X61RWkCSMxMdCnT8gTAcAxx7hksH69uzJQDXVExtRM1vPHBJ/P59aT3Lw51JHw+9/DPfe4BuTp00MdjTE1k6eJQET6i8h6EdkoIuP9vH6jiHwtIqtE5EMRaedlPKaa1JB2gkJ/+QucdZZb0Oarr0IdjTE1j2eJQERiganAeUA7YJifL/p/q2oHVe0M/B8w2at4TDVq1w6aNq0xiSA2FmbPhkaNXHtBZmaoIzKmZvHyiuB0YKOqblLVA8Ac3DTWh6hq8QbnOoDV4kYCETcbaVpajamYb9oU5syBjRvdyOMaEpYxNUKch+duARTvTJ4B9Ci9k4jcDNwOJAC/93ciERkFjAJo1qwZ6QFOYZCVlRXwsZHIy/I4tkUL2m7bxqezZ7M/OdmT9wjEyJHHMW3aCRxzzLcMHFhyTiT7fBSxsigp4stDVT25AYOBacW2h+MWtSlr/yuA54503m7dummg0tLSAj42EnlaHuvWqYLqv/7l3XsEIC9P9bzzVBMSVFesKPmafT6KWFmUFAnlAXyuZXyvelk1tA1oWWw7ueC5sswBLvYwHlOd2raF5s1rTDtBoZgYeP55V1U0ZAjstpW3jfE0EXwGtBGR1iKSAAwFFhTfQUTaFNu8ALA1piJFDWwnKHTUUW4xm61b4brralx4xlQ7zxKBquYCo4HFwFpgrqquFpH7RWRAwW6jRWS1iKzCtRNc41U8JgR8PreGcQ2cE/qMM+Chh+C11+CJJ0IdjTGh5WVjMaq6iFKzlKrq3cUej/Hy/U2IFR9PcOqpoY3FjzvugA8+gDvvhDp14JNPjiMx0a2BbEw0sZHFxjsnngjJySFbqOZIRGDWLDc10g03wPTprenXD5YtC3VkxlQvSwTGOyLuqiA9vcZWxDduDIMGufBUhf37i64U8vJCHZ0x1cMSgfGWzwc7dtTouaCHD4datUBEiY2FFSvcvHnJyXDzzS6PWVIwkcwSgfFWDZt3yJ/UVFiyBK67bjNLl8Ivv7hRyD17usVtfD449lg3g+mSJZCbG+qIjQkuSwTGW61auVsNTgTgksGVV24lNRXq1YPLL4dXX3UXM3Pnup6wzz/vJq9r3txNU/HOO3DwYKgjN6bqLBEY7/l88P77kJ8f6kgqrU4dN/Ds5ZddUnjtNTj7bHjpJTj3XLfmwXXXwX/+AwcOhDpaYwJjicB4z+eDX38N+zmga9eGSy5xi938/DPMnw/nnQevvOLujzkGrr0WFi60pGDCiyUC470waCeorFq1YOBAt+DNzz/DggVw0UUwbx5ceKGbwuLqq+HNNyE7O9TRGlM+SwTGe8nJcNJJEZUIiktKcknguefcQOqFC12X1DffhAEDXFK46ip3BbF/f6ijNeZwlghM9fD5oqJzfmIinH++6220fTu8/TZcdpm7HzTIJYVhw+D112HfvlBHa4xjicBUD5/PTfX5xRehjqTaJCRA//4wbRr873+ul9GwYfDuu3DppS4pXH65a2PYu9eNaH7oIRvZbKqfp3MNGXNI377uPi0NUlJCGkooxMe73kZnnw3//KfrRPXqq+7KYO5clzTy8lzHqoQEmD7d7dukiVtq0xgvWSIw1aN5czjlFJcIxo0LdTQhFRcH/fq525NPwtKlMH48fPqpez0nx7UpgJulo0kTOPpod2va1P/jwm1LHCYQlghM9fH54IUX3Cis+PhQR1MjxMa6i6VHH3WJ4cABlyj+8hdo2NCNXSi8/fwzfPONe/zrr/6nbxJx8ydVJGkcfbRLHHGlvgWWLYPZs0M/E+uyZW56j759bUZYr1kiMNXH54OnnnKT+fzud6GOpkYpnOaiol98ubmwc2fJJFE6aezYAWvWuPudO8tOHI0aFSWGmBj48EPIy2vNrFnuyuS449zzIu6+vFtF9qnIfuvXw113ub8zIcGVzRlneFDwBrBEYKpTnz7uPi3NEoEfqakV/+UbFwfNmrlbReTlFSWOspLGjh2wdm1hxy4hN9d1iQ31xLHZ2S45nnKKm9n8hBPcrfDx8ce73lomcJYITPVp2hROO80lggkTQh1NVImNdcVf+E9QlmXLXBVVTk4+iYkxLFnikpOqa8jOzy/5uKxbRfYpb78vv3Qzvx486GIfNMiNwdiwARYvLjkeQwRatjw8QRQ+btzY7WPKZonAVC+fD2bMcJXhCQmhjsaUUlhFNWPGFkaOPOHQFYqI+0KurobolBRo185/VZmq6467aRN89527L3y8aJF7rbj69f0niBNOcNVe1lxlicBUN5/PdZVZvhzOPDPU0Rg/UlMhJ2crqaknhDwOf1VlIq4TWvPmbqrw0vbuhS1bDk8Sq1fDW2+5XlmFYmNdMijraqJhw+A2nufnu6q33Nyi++KPy3tu1SrYvNlNYRLsxnNLBKZ69enj/ienp1siMJ6oU8dVf/mrAsvPhx9/LJkgCh/Pn+/aSYqrVw+yskC1NTNmuHaKWrUC/zKvanuLiOthVlhlFyyeJgIR6Q88BsQC01T14VKv3w5cD+QCO4CRqvq9lzGZEGvSBDp2dO0EEyeGOhoTZWJi3NRXycnQu/fhr2dmul/dhQnilVcKx3cI+fmuzaJVK9dYHxfnriiK3/t7rrzXKrr/66/D7NkukR044H5HhUUiEJFYYCpwNpABfCYiC1R1TbHdvgBSVHWfiNwE/B9wuVcxmRrC54Onn3bX6Nbdw9Qg9eq53ykdO7rtM84o2Xj+3HOhGdPQrJkbiV7YtFY4UD9YvJxr6HRgo6puUtUDwBxgYPEdVDVNVQun3voESPYwHlNT+HyuT+Ann4Q6EmPKVdh4PnLklqBXxwQSxwMPBL9aCLytGmoB/FBsOwPoUc7+1wFv+3tBREYBowCaNWtGenp6QAFlZWUFfGwkClV5xMXE0DMmhu9nzmRLqDupF2OfjyJWFiUNHJhFTs5WQl0kriGfoMdRIxqLReQqIAXo4+91VX0GeAYgJSVF+wZ4XZSenk6gx0aikJZHly602ryZVjXo38M+H0WsLEqK9PLwsmpoG9Cy2HZywXMliMhZwF+AAaqaU/p1E6F8Plc1ZCu1GBNyXiaCz4A2ItJaRBKAocCC4juISBfgX7gk8LOHsZiaxudzLV8ffxzqSIyJep4lAlXNBUYDi4G1wFxVXS0i94vIgILdJgF1gVdEZJWILCjjdCbS9Orl+sZF6PKVxoQTT9sIVHURsKjUc3cXe3yWl+9varB69dw8ApYIjAk5W6rShI7P50br3Huvrc9oTAhZIjCh06yZG3v/wANu1I4lA2NCwhKBCZ3du9198XHzxphqZ4nAhM4555Scitrf5C/GGM9ZIjChk5rqrgL693dVRPPnhzoiY6KSJQITWqmpbjWRm2+GRx5xaxUYY6pVjZhiwkQ5EXjsMcjIgFtvhRYt3NqExphqYVcEpmaIjYV//xtOPx2uuMJ6EBlTjSwRmJqjdm148023ashFF8G334Y6ImOigiUCU7McfTS8/barLjrvPPjZpqAyxmuWCEzNc9JJbpXxn35yK3Xv3RvqiIyJaJYITM3Uowe89BKsWAHDhrmVv40xnrBEYGqugQPh8cddu8Gtt0INWs3MmEhi3UdNzXbzzbB1K/zf/8Fxx8H48aGOyJiIY4nA1HwPPQQ//AATJkDLlnDllaGOyJiIYonA1HwxMTBzpms8vvZaaN4cfv/7UEdlTMSwNgITHhITYd48aNvWjTr++utQR2RMxLBEYMJHw4ZuXqK6deH8892UFMaYKrNEYMLLccfBwoVuLYMLLiha08AYEzBLBCb8dO4Mr70Ga9bApZe6RW2MMQHzNBGISH8RWS8iG0XksH5/ItJbRFaKSK6IDPYyFhNhzj4bpk2DJUvg+uttjIExVeBZryERiQWmAmcDGcBnIrJAVdcU220rMAK406s4TAS75ho3xuDuu12V0d/+FuqIjAlLXnYfPR3YqKqbAERkDjAQOJQIVHVLwWv5HsZhItnEiS4ZPPigSwajRoU6ImPCjpeJoAXwQ7HtDKBHICcSkVHAKIBmzZqRHuAi51lZWQEfG4kipTxk6FDaf/01jW+6ia937uTX1NSAzhMp5REMVhYlRXp5hMWAMlV9BngGICUlRfv27RvQedLT0wn02EgUUeXRowf06UPHv/0N3n8fUlIqfYqIKo8qsrIoKdLLw8vG4m1Ay2LbyQXPGRN8deu6bqVNm7pupZs2hToiY8KGl4ngM6CNiLQWkQRgKLDAw/cz0e6YY9yiNgcPukVtdu4MdUTGhAXPEoGq5gKjgcXAWmCuqq4WkftFZACAiHQXkQxgCPAvEVntVTwmSpxyCixYAN9/DwMGwP79oY7ImBrP0zYCVV0ELCr13N3FHn+GqzIyJnjOPBNefBEuuwyGD4eXX4bY2FBHZUyNZSOLTWQaPBj+8Q83AvlOG6ZiTHnCoteQMQEZO9aNMZgyxY0xGDs21BEZUyNZIjCR7R//cIva3HEHJCfDkCGhjsiYGscSgYlsMTHwwgvwv/+59oLmzV0bgjHmEGsjMJGvVi144w1o1cr1JFq3LtQRGVOjWCIw0aFJEzfGID4e+vd3VwjGGMASgYkmrVu70cc7drjRx1lZoY7ImBrBEoGJLikpMHcurFrlxhnk5oY6ImNCzhKBiT4XXABPPeWqim66yRa1MVHPeg2Z6DRqVNE6Bscf79Y1MCZKWSIw0euBB9wYg7/+FXJyOO5//4PERAhwPQNjwpUlAhO9RODZZ2HNGvjb32gNMGMGdO/uGpYbNoQGDdx94c3fdq1a7lzGhClLBCa6JSTA+efDihWIKuTnQ0YG/Por7NrlbgcPln+O+PjyE8WRtuvVc4lk2TJIT4e+fe2qxFQrSwTG9O8PkyaRn5NDTGIivPJK0RexKmRnFyWFXbtg9+4jb2/bVvR4377y3z8mBurUcd1ZVd12r15w0knQqNHht8aNix43bGgzq5oqs0RgTGoqLFnClhkzOGHkyJK/xkVc1U+tWm56ikAcOFCULMpKIu++C5984vbPz4e1a+Hbb+G331wiKk/9+v4TxpFuDRtCnJ+vgGXLOG72bGsvKRQF5WGJwBiA1FS25uRwghf/0RMS4Oij3a0s558P/fq5pJGQAPPnF33pZGe7hODv9uuvhz+3bl3R4yMlkXr1SiaH/Hz46CNa5+XBrFkwcqRb7KcwGdauffjj0s8lJbmrmqoKZlVZXp5bpGjfvvLvSz+3cSO89BKtc3Ndefzxj9Chgyu3wlv9+iW3ExPDrs3IEoExNUHBVYnfL76kJHc1EsgVSXlJxF9S2bAB8vIQcIPtnnkmsL8nMdF/0qhoMtm2zc0cm5vrqr5uucUtRVqZL/Hi9wcOBPZ3xMaWLI/HHz/yMXFxhycHfwmjIs/VqVOUVD1sQ7JEYExNkZoa/KqHyiaRZcugX7+i9pK334ZOncr+wg3k8Y4d/p8v68s6Px8mTy7aTkgoO5k0aXJ4cinrviL7LF9esjzmz4dTT4XMTNizx92Xvvl7ftcu11W5+HP5+Uf+9xBxySAx0SVqEfd4yZKgflYsERhjipTVXtKwoffvXVh9s38/LF0KV17pemzFx7uV5nr3dl/O1dk4Xl77UVWour+zoslk6VLYudMdd+CAuzKwRGCM8YyX7SXliY2FunXd7ZJL4L33akZ3Wi/KQ8RdddSu7aq8jqTgSu1QG1LfvsGLBY8TgYj0Bx4DYoFpqvpwqdcTgeeBbsBO4HJV3eJlTMaYMOFFVVm4Kq8NKQg8SwQiEgtMBc4GMoDPRGSBqq4pttt1wG+qepKIDAX+DlzuVUzGGBO2PEyMXs4+ejqwUVU3qeoBYA4wsNQ+A4HnCh6/CvQTCbN+V8YYE+a8rBpqAfxQbDsD6FHWPqqaKyK7gSbAL8V3EpFRwCiAZs2akZ6eHlBAWVlZAR8biaw8SrLyKGJlUVKkl0dYNBar6jPAMwApKSnaN8CGkvT0dAI9NhJZeZRk5VHEyqKkSC8PL6uGtgEti20nFzzndx8RiQMa4BqNjTHGVBMvE8FnQBsRaS0iCcBQYEGpfRYA1xQ8Hgy8p2rLRRljTHXyrGqooM5/NLAY1310hqquFpH7gc9VdQEwHXhBRDYCv+KShTHGmGok4fYDXER2AN8HePhRlGqIjnJWHiVZeRSxsigpEsrjeFX1O/Nh2CWCqhCRz1U1JdRx1BRWHiVZeRSxsigp0svDyzYCY4wxYcASgTHGRLloSwQBTq4esaw8SrLyKGJlUVJEl0dUtREYY4w5XLRdERhjjCnFEoExxkS5qEkEItJfRNaLyEYRGR/qeEJFRFqKSJqIrBGR1SIyJtQx1QQiEisiX4jIW6GOJdREpKGIvCoi60RkrYhE7aIAIjK24P/JNyLykogkhTomL0RFIii2NsJ5QDtgmIi0C21UIZML3KGq7YDfATdHcVkUNwZYG+ogaojHgP+o6ilAJ6K0XESkBXArkKKq7XEzJETk7AdRkQio2NoIUUFVf1LVlQWPM3H/yVuENqrQEpFk4AJgWqhjCTURaQD0xk3/gqoeUNVdoY0qpOKAWgWTYtYGfgxxPJ6IlkTgb22EqP7yAxCRVkAX4NPQRhJyU4A/AfmhDqQGaA3sAGYWVJVNE5E6oQ4qFFR1G/AIsBX4Cditqu+ENipvREsiMKWISF3gNeA2Vd0T6nhCRUQuBH5W1RWhjqWGiAO6Ak+pahdgLxCVbWoi0ghXc9AaOBaoIyJXhTYqb0RLIqjI2ghRQ0TicUlgtqq+Hup4QqwnMEBEtuCqDH8vIi+GNqSQygAyVLXwKvFVXGKIRmcBm1V1h6oeBF4HzghxTJ6IlkRQkbURokLBmtDTgbWqOjnU8YSaqk5Q1WRVbYX7XLynqhH5q68iVPV/wA8icnLBU/2ANSEMKZS2Ar8TkdoF/2/6EaEN52GxVGVVlbU2QojDCpWewHDgaxFZVfDcXaq6KIQxmZrlFmB2wY+mTcC1IY4nJFT1UxF5FViJ6233BRE61YRNMWGMMVEuWqqGjDHGlMESgTHGRDlLBMYYE+UsERhjTJSzRGCMMVHOEoEx1UhE+toMp6amsURgjDFRzhKBMX6IyFUislxEVonIvwrWK8gSkUcL5qdfIiJHF+zbWUQ+EZGvRGRewRw1iMhJIvKuiHwpIitF5MSC09ctNt//7IJRq8aEjCUCY0oRkVOBy4GeqtoZyAOuBOoAn6vqacD7wD0FhzwP/FlVOwJfF3t+NjBVVTvh5qj5qeD5LsBtuLUxTsCN9jYmZKJiigljKqkf0A34rODHei3gZ9w01S8X7PMi8HrB/P0NVfX9guefA14RkXpAC1WdB6Cq2QAF51uuqhkF26uAVsCH3v9ZxvhnicCYwwnwnKpOKPGkyF9L7Rfo/Cw5xR7nYf8PTYhZ1ZAxh1sCDBaRpgAi0lhEjsf9fxlcsM8VwIequhv4TUR6FTw/HHi/YPW3DBG5uOAciSJSu1r/CmMqyH6JGFOKqq4RkYnAOyISAxwEbsYt0nJ6wWs/49oRAK4Bni74oi8+W+dw4F8icn/BOYZU459hTIXZ7KPGVJCIZKlq3VDHYUywWdWQMcZEObsiMMaYKGdXBMYYE+UsERhjTJSzRGCMMVHOEoExxkQ5SwTGGBPl/j94myRGUlfF5AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0:00:07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "QLKucB64W8Uv",
        "outputId": "a216e26e-653d-4a85-d942-53a5817923fd"
      },
      "source": [
        "start = time.time()\n",
        "# 라이브러리\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='normal',activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "hist = model.fit(X3_train, rorl_train, epochs=10, batch_size=64)\n",
        "\n",
        "    \n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(lg_x_test, lg_y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.7041 - accuracy: 0.5043\n",
            "Epoch 2/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.6791 - accuracy: 0.5597\n",
            "Epoch 3/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.5016 - accuracy: 0.7469\n",
            "Epoch 4/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.3347 - accuracy: 0.8133\n",
            "Epoch 5/10\n",
            "260/260 [==============================] - 0s 2ms/step - loss: 0.2816 - accuracy: 0.8396\n",
            "Epoch 6/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.2705 - accuracy: 0.8350\n",
            "Epoch 7/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.2594 - accuracy: 0.8477\n",
            "Epoch 8/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.2482 - accuracy: 0.8438\n",
            "Epoch 9/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.2241 - accuracy: 0.8589\n",
            "Epoch 10/10\n",
            "260/260 [==============================] - 1s 2ms/step - loss: 0.2053 - accuracy: 0.8600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "130/130 [==============================] - 0s 975us/step - loss: 0.1189 - accuracy: 0.9793\n",
            "loss_and_metrics : [0.11891062557697296, 0.979292094707489]\n",
            "0:00:06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgCfvXjjW8Uz",
        "outputId": "3548957a-624c-42a9-b8c7-5a0b7a7fb33a"
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn import metrics   \n",
        "\n",
        "#분류->object 회귀\n",
        "start = time.time()\n",
        "\n",
        "# 부스팅 타입은 default값인 gbdt로, 학습률은 0.01로 지정 하였음\n",
        "LGB = lgb.LGBMClassifier(objective=\"regression\", boosting_type='gbdt', learning_rate = 0.01)\n",
        "\n",
        "param_list = {\"n_estimators\": list(range(10, 300, 10)),\n",
        "              \"max_depth\": list(range(4, 21, 4)),\n",
        "              \"max_features\": list(range(3, 13, 2)),\n",
        "              \"min_samples_split\": list(range(3, 13, 2))}\n",
        "\n",
        "# 하이퍼파라미터 최적화\n",
        "LGB_random_search = RandomizedSearchCV(estimator = LGB,\n",
        "                                        param_distributions = param_list,\n",
        "                                        n_iter = 10,      # 10번반복하는 lightgbm 구현 : 성능개선 시도\n",
        "                                        cv = 3,           # cross-validation 3번 반복\n",
        "                                        n_jobs = 10,\n",
        "                                        random_state=42)\n",
        "\n",
        "LGB_random_search.fit(X3_train, rorl_train)\n",
        "y_pred = LGB_random_search.predict(lg_x_test)\n",
        "\n",
        "#성능평가\n",
        "print('정확도 :', metrics.accuracy_score(lg_y_test, y_pred))\n",
        "\n",
        "print( LGB_random_search.best_params_ )\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정확도 : 0.9963881531423068\n",
            "{'n_estimators': 290, 'min_samples_split': 9, 'max_features': 7, 'max_depth': 8}\n",
            "0:01:11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bONUZW4bGeO",
        "outputId": "fe5db48e-fc16-4091-9663-c0a67bcaf482"
      },
      "source": [
        "# accuracy 확인\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(confusion_matrix(lg_y_test, y_pred))\n",
        "print(classification_report(lg_y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2018    8]\n",
            " [   7 2120]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2026\n",
            "           1       1.00      1.00      1.00      2127\n",
            "\n",
            "    accuracy                           1.00      4153\n",
            "   macro avg       1.00      1.00      1.00      4153\n",
            "weighted avg       1.00      1.00      1.00      4153\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyUVXbJ_W8U3"
      },
      "source": [
        "RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q34JkQDGW8U4"
      },
      "source": [
        "# 데이터 변환\n",
        "# LTSM : 입력을 (샘플 수, 타입스텝, 특성 수)로 형태 변환\n",
        "x_train = np.array(X3_train).reshape((X3_train.shape[0], X3_train.shape[1], 1))\n",
        "y_train = np.array(rorl_train).reshape((rorl_train.shape[0], 1))\n",
        "x_test = np.array(lg_x_test).reshape((lg_x_test.shape[0], lg_x_test.shape[1],1))\n",
        "y_test = np.array(lg_y_test).reshape((lg_y_test.shape[0], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "89-fQ4VcW8U8",
        "outputId": "13e4d628-ef08-4fe7-c949-f52cbe49774c"
      },
      "source": [
        "start = time.time()\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Activation, Dropout, LSTM\n",
        "\n",
        "# RNN 모델 구성\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(65, input_shape = (65,1), return_sequences = True ,activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(SimpleRNN(130, return_sequences = True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(65, return_sequences = True, activation='tanh'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "hist = model.fit(x_train, y_train, epochs=10, batch_size=64 )\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "260/260 [==============================] - 31s 118ms/step - loss: 0.5056 - accuracy: 0.6811\n",
            "Epoch 2/10\n",
            "260/260 [==============================] - 31s 118ms/step - loss: 0.3101 - accuracy: 0.7991\n",
            "Epoch 3/10\n",
            "260/260 [==============================] - 30s 116ms/step - loss: 0.2967 - accuracy: 0.8090\n",
            "Epoch 4/10\n",
            "260/260 [==============================] - 30s 116ms/step - loss: 0.2832 - accuracy: 0.8183\n",
            "Epoch 5/10\n",
            "260/260 [==============================] - 30s 115ms/step - loss: 0.2835 - accuracy: 0.8205\n",
            "Epoch 6/10\n",
            "260/260 [==============================] - 35s 133ms/step - loss: 0.2741 - accuracy: 0.8255\n",
            "Epoch 7/10\n",
            "260/260 [==============================] - 31s 118ms/step - loss: 0.2751 - accuracy: 0.8273\n",
            "Epoch 8/10\n",
            "260/260 [==============================] - 30s 116ms/step - loss: 0.2719 - accuracy: 0.8293\n",
            "Epoch 9/10\n",
            "260/260 [==============================] - 30s 116ms/step - loss: 0.2671 - accuracy: 0.8322\n",
            "Epoch 10/10\n",
            "260/260 [==============================] - 30s 116ms/step - loss: 0.2655 - accuracy: 0.8327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "130/130 [==============================] - 3s 20ms/step - loss: 0.2530 - accuracy: 0.8373\n",
            "loss_and_metrics : [0.2530091106891632, 0.8373374342918396]\n",
            "0:05:15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ3wdsr5MqtY",
        "outputId": "70de5db2-1dbf-42e7-d765-e37b7684ccc1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_5 (SimpleRNN)     (None, 65, 65)            4355      \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 65, 65)            0         \n",
            "_________________________________________________________________\n",
            "simple_rnn_6 (SimpleRNN)     (None, 65, 130)           25480     \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 65, 130)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 65, 65)            50960     \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 65, 32)            2112      \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 65, 32)            0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 65, 1)             33        \n",
            "=================================================================\n",
            "Total params: 82,940\n",
            "Trainable params: 82,940\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIa_xSylb3IE"
      },
      "source": [
        "# 데이터셋 ro,rl 심각도 1단계만 없애기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KzfBGhIb3IT",
        "outputId": "4b9ad998-7924-4725-b712-7dcad4c1507c"
      },
      "source": [
        "# 데이터 로드\n",
        "\n",
        "\n",
        "# 정상데이터 (normal1,normal2)\n",
        "start = time.time()\n",
        "n = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/1_Benchmark Tests/normal.xls\",sheet_name=\"Complete Data Set\")\n",
        "n1 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/1_Benchmark Tests/normal1.xls\",sheet_name=\"Complete Data Set\")\n",
        "n2 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/1_Benchmark Tests/normal2.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"normal\", clock(start) )\n",
        "\n",
        "# 고장데이터\n",
        "# condenser fouling\n",
        "start = time.time()\n",
        "cf12 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/2_Condenser fouling/cf12.xls\",sheet_name=\"Complete Data Set\")\n",
        "cf20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/2_Condenser fouling/cf20.xls\",sheet_name=\"Complete Data Set\")\n",
        "cf30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/2_Condenser fouling/cf30.xls\",sheet_name=\"Complete Data Set\")\n",
        "cf45 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/2_Condenser fouling/cf45.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"cf\", clock(start) )\n",
        "\n",
        "# Refrigerant overcharge\n",
        "start = time.time()\n",
        "#ro10 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro10.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro20.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro30.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro40 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"ro\",  clock(start) )\n",
        "\n",
        "#Excess oil\n",
        "start = time.time()\n",
        "eo14 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/4_Excess oil/eo14.xls\",sheet_name=\"Complete Data Set\")\n",
        "eo32 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/4_Excess oil/eo32.xls\",sheet_name=\"Complete Data Set\")\n",
        "eo50 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/4_Excess oil/eo50.xls\",sheet_name=\"Complete Data Set\")\n",
        "eo68 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/4_Excess oil/eo68--unsteady test1.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"eo\", clock(start) )\n",
        "\n",
        "# Reduced condenser water flow\n",
        "start = time.time()\n",
        "fwc10 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/5_Reduced condenser water flow/fwc10.xls\",sheet_name=\"Complete Data Set\")\n",
        "fwc20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/5_Reduced condenser water flow/fwc20.xls\",sheet_name=\"Complete Data Set\")\n",
        "fwc30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/5_Reduced condenser water flow/fwc30.xls\",sheet_name=\"Complete Data Set\")\n",
        "fwc40 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/5_Reduced condenser water flow/fwc40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"fwc\",  clock(start) )\n",
        "\n",
        "# Non condensables in refrigerant\n",
        "start = time.time()\n",
        "nc1 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/6_Non-condensables in refrigerant/nc1.xls\",sheet_name=\"Complete Data Set\")\n",
        "nc2 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/6_Non-condensables in refrigerant/nc2.xls\",sheet_name=\"Complete Data Set\")\n",
        "nc3 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/6_Non-condensables in refrigerant/nc3.xls\",sheet_name=\"Complete Data Set\")\n",
        "nc5 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/6_Non-condensables in refrigerant/nc5.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"nc\",  clock(start) )\n",
        "\n",
        "# 7_reduced evaporator water flow\n",
        "start = time.time()\n",
        "fwe10 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/7_Reduced evaporator water flow/fwe10.xls\",sheet_name=\"Complete Data Set\")\n",
        "fwe20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/7_Reduced evaporator water flow/fwe20.xls\",sheet_name=\"Complete Data Set\")\n",
        "fwe30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/7_Reduced evaporator water flow/fwe30.xls\",sheet_name=\"Complete Data Set\")\n",
        "fwe40 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/7_Reduced evaporator water flow/fwe40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"fwe\",  clock(start) )\n",
        "\n",
        "# 8_refrigerant leak\n",
        "start = time.time()\n",
        "#rl10 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl10.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl20.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl30.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl40 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"rl\",  clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normal 0:00:02\n",
            "cf 0:00:03\n",
            "ro 0:00:02\n",
            "eo 0:00:03\n",
            "fwc 0:00:03\n",
            "nc 0:00:03\n",
            "fwe 0:00:03\n",
            "rl 0:00:02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDOqVsoIb3Id"
      },
      "source": [
        "normal = pd.concat([n,n1,n2],axis=0)\n",
        "cf = pd.concat([cf12,cf20,cf30,cf45],axis=0)\n",
        "ro = pd.concat([ro20,ro30,ro40], axis=0)\n",
        "eo= pd.concat([eo14,eo32,eo50,eo68], axis=0)\n",
        "fwc= pd.concat([fwc10,fwc20,fwc30,fwc40], axis=0)\n",
        "nc= pd.concat([nc1,nc2,nc3,nc5], axis=0)\n",
        "fwe= pd.concat([fwe10,fwe20,fwe30,fwe40], axis=0)\n",
        "rl= pd.concat([rl20,rl30,rl40], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFFzCMoBb3Ii"
      },
      "source": [
        "normal['rorl'] = 0\n",
        "normal['detect'] = 0\n",
        "normal['target'] = 1\n",
        "\n",
        "cf['rorl'] = 0\n",
        "cf['detect'] = 1\n",
        "cf['target'] = 2\n",
        "\n",
        "ro['rorl'] = 3\n",
        "ro['detect'] = 1\n",
        "ro['target'] = 3\n",
        "\n",
        "eo['rorl'] = 0\n",
        "eo['detect'] = 1\n",
        "eo['target'] = 4\n",
        "\n",
        "fwc['rorl'] = 0\n",
        "fwc['detect'] = 1\n",
        "fwc['target'] = 5\n",
        "\n",
        "nc['rorl'] = 0\n",
        "nc['detect'] = 1\n",
        "nc['target'] = 6\n",
        "\n",
        "fwe['rorl'] = 0\n",
        "fwe['detect'] = 1\n",
        "fwe['target'] = 7\n",
        "\n",
        "rl['rorl'] = 8\n",
        "rl['detect'] = 1\n",
        "rl['target'] = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyfRMQiob3In",
        "outputId": "88ddfaa4-0a7b-4543-eda6-d27de556d1ae"
      },
      "source": [
        "start = time.time()\n",
        "# 라이브러리\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='normal',activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "model.fit(X3_train, rorl_train, epochs=10, batch_size=64)\n",
        "\n",
        "# 모델 예측 (예측 Test 파일 넣으세용) - Accuracy 구하는 용도\n",
        "#pred = model.predict(lg_x_test)\n",
        "#print('정확도 :', metrics.accuracy_score(lg_y_test, pred))\n",
        "\n",
        "##### 모델 평가(여기에 Test 파일 넣으세용) #####\n",
        "loss_and_metrics = model.evaluate(lg_x_test, lg_y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.7094 - accuracy: 0.4998\n",
            "Epoch 2/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.6983 - accuracy: 0.5001\n",
            "Epoch 3/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.6973 - accuracy: 0.5019\n",
            "Epoch 4/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.6909 - accuracy: 0.5294\n",
            "Epoch 5/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.5967 - accuracy: 0.6780\n",
            "Epoch 6/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.4968 - accuracy: 0.7736\n",
            "Epoch 7/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.4360 - accuracy: 0.8135\n",
            "Epoch 8/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.4159 - accuracy: 0.8269\n",
            "Epoch 9/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.4121 - accuracy: 0.8264\n",
            "Epoch 10/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.4058 - accuracy: 0.8276\n",
            "195/195 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.9335\n",
            "loss_and_metrics : [0.26223108172416687, 0.93353670835495]\n",
            "0:00:08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "30CMPTpmb3Is",
        "outputId": "17196bc1-ea5b-45b6-f11b-afac7c797d77"
      },
      "source": [
        "start = time.time()\n",
        "# 라이브러리\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='normal',activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='softmax'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='linear'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='linear'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "history = model.fit(X3_train, rorl_train, epochs=10, batch_size=64, validation_data=(lg_x_test, lg_y_test))\n",
        "\n",
        "# 모델 예측 (예측 Test 파일 넣으세용) - Accuracy 구하는 용도\n",
        "#pred = model.predict(lg_x_test)\n",
        "#print('정확도 :', metrics.accuracy_score(lg_y_test, pred))\n",
        "\n",
        "##### 모델 평가(여기에 Test 파일 넣으세용) #####\n",
        "\n",
        "print('\\nAccuracy: {:.4f}'.format(model.evaluate(lg_x_test, lg_y_test)[1]))\n",
        "\n",
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "\n",
        "x_len = numpy.arange(len(y_loss))\n",
        "plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n",
        "plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "390/390 [==============================] - 1s 3ms/step - loss: 0.6979 - accuracy: 0.5031 - val_loss: 0.6931 - val_accuracy: 0.4999\n",
            "Epoch 2/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.6939 - accuracy: 0.5058 - val_loss: 0.6937 - val_accuracy: 0.4999\n",
            "Epoch 3/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.6573 - accuracy: 0.5735 - val_loss: 0.5764 - val_accuracy: 0.8552\n",
            "Epoch 4/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.4993 - accuracy: 0.7643 - val_loss: 0.4211 - val_accuracy: 0.8550\n",
            "Epoch 5/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.4061 - accuracy: 0.8137 - val_loss: 0.3619 - val_accuracy: 0.8318\n",
            "Epoch 6/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.3574 - accuracy: 0.8373 - val_loss: 0.3708 - val_accuracy: 0.8441\n",
            "Epoch 7/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.2954 - accuracy: 0.8804 - val_loss: 0.1674 - val_accuracy: 0.9557\n",
            "Epoch 8/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.2643 - accuracy: 0.8968 - val_loss: 0.1940 - val_accuracy: 0.9253\n",
            "Epoch 9/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.2630 - accuracy: 0.8940 - val_loss: 0.1698 - val_accuracy: 0.9419\n",
            "Epoch 10/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.2699 - accuracy: 0.8873 - val_loss: 0.2429 - val_accuracy: 0.9104\n",
            "195/195 [==============================] - 0s 990us/step - loss: 0.2429 - accuracy: 0.9104\n",
            "\n",
            "Accuracy: 0.9104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0:00:10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "JjG0Lq6fb3Iw",
        "outputId": "34c368b8-9ee6-4821-acf2-50a18cae71cb"
      },
      "source": [
        "start = time.time()\n",
        "# 라이브러리\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='normal',activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "hist = model.fit(X3_train, rorl_train, epochs=10, batch_size=64)\n",
        "\n",
        "    \n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(lg_x_test, lg_y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.7042 - accuracy: 0.5059\n",
            "Epoch 2/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.6940 - accuracy: 0.5107\n",
            "Epoch 3/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.6918 - accuracy: 0.5272\n",
            "Epoch 4/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.5989 - accuracy: 0.6693\n",
            "Epoch 5/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.4533 - accuracy: 0.7651\n",
            "Epoch 6/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.3412 - accuracy: 0.8092\n",
            "Epoch 7/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.3188 - accuracy: 0.8182\n",
            "Epoch 8/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.2975 - accuracy: 0.8228\n",
            "Epoch 9/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.2755 - accuracy: 0.8347\n",
            "Epoch 10/10\n",
            "390/390 [==============================] - 1s 2ms/step - loss: 0.2606 - accuracy: 0.8437\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "195/195 [==============================] - 0s 957us/step - loss: 0.0897 - accuracy: 0.9639\n",
            "loss_and_metrics : [0.08967918157577515, 0.9638786315917969]\n",
            "0:00:09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5REf-b4Bb3I3",
        "outputId": "b7b88df9-a77f-4efe-84df-6f6168fb1fda"
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn import metrics   \n",
        "\n",
        "#분류->object 회귀\n",
        "start = time.time()\n",
        "\n",
        "# 부스팅 타입은 default값인 gbdt로, 학습률은 0.01로 지정 하였음\n",
        "LGB = lgb.LGBMClassifier(objective=\"regression\", boosting_type='gbdt', learning_rate = 0.01)\n",
        "\n",
        "param_list = {\"n_estimators\": list(range(10, 300, 10)),\n",
        "              \"max_depth\": list(range(4, 21, 4)),\n",
        "              \"max_features\": list(range(3, 13, 2)),\n",
        "              \"min_samples_split\": list(range(3, 13, 2))}\n",
        "\n",
        "# 하이퍼파라미터 최적화\n",
        "LGB_random_search = RandomizedSearchCV(estimator = LGB,\n",
        "                                        param_distributions = param_list,\n",
        "                                        n_iter = 10,      # 10번반복하는 lightgbm 구현 : 성능개선 시도\n",
        "                                        cv = 3,           # cross-validation 3번 반복\n",
        "                                        n_jobs = 10,\n",
        "                                        random_state=42)\n",
        "\n",
        "LGB_random_search.fit(X3_train, rorl_train)\n",
        "y_pred = LGB_random_search.predict(lg_x_test)\n",
        "\n",
        "#성능평가\n",
        "print('정확도 :', metrics.accuracy_score(lg_y_test, y_pred))\n",
        "\n",
        "print( LGB_random_search.best_params_ )\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정확도 : 0.9950232782148017\n",
            "{'n_estimators': 290, 'min_samples_split': 9, 'max_features': 7, 'max_depth': 8}\n",
            "0:01:29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvXW6-MKb3I7",
        "outputId": "760e689e-4771-4a2e-97e1-0295fe33fff8"
      },
      "source": [
        "# accuracy 확인\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(confusion_matrix(lg_y_test, y_pred))\n",
        "print(classification_report(lg_y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3096   19]\n",
            " [  12 3102]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00      3115\n",
            "           1       0.99      1.00      1.00      3114\n",
            "\n",
            "    accuracy                           1.00      6229\n",
            "   macro avg       1.00      1.00      1.00      6229\n",
            "weighted avg       1.00      1.00      1.00      6229\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEsZLvw5b3JA"
      },
      "source": [
        "RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBs-oDODb3JB"
      },
      "source": [
        "# 데이터 변환\n",
        "# LTSM : 입력을 (샘플 수, 타입스텝, 특성 수)로 형태 변환\n",
        "x_train = np.array(X3_train).reshape((X3_train.shape[0], X3_train.shape[1], 1))\n",
        "y_train = np.array(rorl_train).reshape((rorl_train.shape[0], 1))\n",
        "x_test = np.array(lg_x_test).reshape((lg_x_test.shape[0], lg_x_test.shape[1],1))\n",
        "y_test = np.array(lg_y_test).reshape((lg_y_test.shape[0], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "20Japzd-b3JF",
        "outputId": "408b3f8e-525b-4bd5-e717-57a02af54c74"
      },
      "source": [
        "start = time.time()\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Activation, Dropout, LSTM\n",
        "\n",
        "# RNN 모델 구성\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape = (65,1), return_sequences = True ,activation='tanh'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(512, input_shape = (65,1), return_sequences = True ,activation='tanh'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, input_shape = (65,1), return_sequences = True ,activation='tanh'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(128, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(64, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "hist = model.fit(x_train, y_train, epochs=100, batch_size=512 )\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "49/49 [==============================] - 7s 136ms/step - loss: 0.6799 - accuracy: 0.5666\n",
            "Epoch 2/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.5946 - accuracy: 0.6391\n",
            "Epoch 3/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.4801 - accuracy: 0.7039\n",
            "Epoch 4/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.4573 - accuracy: 0.7227\n",
            "Epoch 5/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.4173 - accuracy: 0.7395\n",
            "Epoch 6/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.3974 - accuracy: 0.7500\n",
            "Epoch 7/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.3842 - accuracy: 0.7582\n",
            "Epoch 8/100\n",
            "49/49 [==============================] - 7s 139ms/step - loss: 0.3773 - accuracy: 0.7622\n",
            "Epoch 9/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.3690 - accuracy: 0.7686\n",
            "Epoch 10/100\n",
            "49/49 [==============================] - 7s 139ms/step - loss: 0.3624 - accuracy: 0.7743\n",
            "Epoch 11/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.3559 - accuracy: 0.7790\n",
            "Epoch 12/100\n",
            "49/49 [==============================] - 7s 137ms/step - loss: 0.3580 - accuracy: 0.7769\n",
            "Epoch 13/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.3476 - accuracy: 0.7839\n",
            "Epoch 14/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.3364 - accuracy: 0.7906\n",
            "Epoch 15/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.3394 - accuracy: 0.7906\n",
            "Epoch 16/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.3211 - accuracy: 0.8007\n",
            "Epoch 17/100\n",
            "49/49 [==============================] - 7s 138ms/step - loss: 0.3162 - accuracy: 0.8043\n",
            "Epoch 18/100\n",
            " 1/49 [..............................] - ETA: 3s - loss: 0.3027 - accuracy: 0.8122"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-8bdb4244c38b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m##### 모델 학습(여기에 Train 파일 넣으세용) #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# 5. 학습과정 살펴보기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3feI99SSb3JJ",
        "outputId": "7c2f1aaa-18c8-4bab-f754-eefc9ebda091"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_20 (LSTM)               (None, 65, 128)           66560     \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 65, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, 65, 256)           394240    \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 65, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_22 (LSTM)               (None, 65, 512)           1574912   \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 65, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_23 (LSTM)               (None, 65, 256)           787456    \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 65, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_24 (LSTM)               (None, 65, 128)           197120    \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 65, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_25 (LSTM)               (None, 65, 64)            49408     \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 65, 64)            0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 65, 32)            2080      \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 65, 32)            0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 65, 16)            528       \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 65, 1)             17        \n",
            "=================================================================\n",
            "Total params: 3,072,321\n",
            "Trainable params: 3,072,321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPBkgXCNKGpy"
      },
      "source": [
        "# Train 데이터는 2,3,4단계만, Test 데이터는 1~4단계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtIs_eiPKDub",
        "outputId": "d652a9d9-0aa4-45bf-a860-1a7708298574"
      },
      "source": [
        "# 데이터 로드\n",
        "# Refrigerant overcharge\n",
        "start = time.time()\n",
        "ro10 = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/FDD Data(1043-RP)/3_Refrigerant overcharge/ro10.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro20 = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/FDD Data(1043-RP)/3_Refrigerant overcharge/ro20.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro30 = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/FDD Data(1043-RP)/3_Refrigerant overcharge/ro30.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro40 = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/FDD Data(1043-RP)/3_Refrigerant overcharge/ro40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"ro\",  clock(start) )\n",
        "\n",
        "# Refrigerant leak\n",
        "start = time.time()\n",
        "rl10 = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/FDD Data(1043-RP)/8_Refrigerant leak/rl10.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl20 = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/FDD Data(1043-RP)/8_Refrigerant leak/rl20.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl30 = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/FDD Data(1043-RP)/8_Refrigerant leak/rl30.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl40 = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/FDD Data(1043-RP)/8_Refrigerant leak/rl40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"rl\",  clock(start) )\n",
        "\n",
        "# 심각도 수준 적기\n",
        "ro10['severity'] = 1\n",
        "ro20['severity'] = 2\n",
        "ro30['severity'] = 3\n",
        "ro40['severity'] = 4\n",
        "\n",
        "rl10['severity'] = 1\n",
        "rl20['severity'] = 2\n",
        "rl30['severity'] = 3\n",
        "rl40['severity'] = 4\n",
        "\n",
        "#\n",
        "ro = pd.concat([ro10,ro20,ro30,ro40], axis=0)\n",
        "rl= pd.concat([rl10,rl20,rl30,rl40], axis=0)\n",
        "\n",
        "#\n",
        "ro['rorl'] = 0\n",
        "rl['rorl'] = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ro 0:00:12\n",
            "rl 0:00:11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "vPOufUI3S_ll",
        "outputId": "dcab803f-00a5-46d5-c74f-2c215179deec"
      },
      "source": [
        "data1 = pd.concat([ro, rl], axis=0)\n",
        "data1 = data1.drop(['Time'],axis=1)\n",
        "data1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TWE_set</th>\n",
              "      <th>TEI</th>\n",
              "      <th>TWEI</th>\n",
              "      <th>TEO</th>\n",
              "      <th>TWEO</th>\n",
              "      <th>TCI</th>\n",
              "      <th>TWCI</th>\n",
              "      <th>TCO</th>\n",
              "      <th>TWCO</th>\n",
              "      <th>TSI</th>\n",
              "      <th>TSO</th>\n",
              "      <th>TBI</th>\n",
              "      <th>TBO</th>\n",
              "      <th>Cond Tons</th>\n",
              "      <th>Cooling Tons</th>\n",
              "      <th>Shared Cond Tons</th>\n",
              "      <th>Cond Energy Balance</th>\n",
              "      <th>Evap Tons</th>\n",
              "      <th>Shared Evap Tons</th>\n",
              "      <th>Building Tons</th>\n",
              "      <th>Evap Energy Balance</th>\n",
              "      <th>kW</th>\n",
              "      <th>COP</th>\n",
              "      <th>kW/Ton</th>\n",
              "      <th>FWC</th>\n",
              "      <th>FWE</th>\n",
              "      <th>TEA</th>\n",
              "      <th>TCA</th>\n",
              "      <th>TRE</th>\n",
              "      <th>PRE</th>\n",
              "      <th>TRC</th>\n",
              "      <th>PRC</th>\n",
              "      <th>TRC_sub</th>\n",
              "      <th>T_suc</th>\n",
              "      <th>Tsh_suc</th>\n",
              "      <th>TR_dis</th>\n",
              "      <th>Tsh_dis</th>\n",
              "      <th>P_lift</th>\n",
              "      <th>Amps</th>\n",
              "      <th>RLA%</th>\n",
              "      <th>Heat Balance (kW)</th>\n",
              "      <th>Heat Balance%</th>\n",
              "      <th>Tolerance%</th>\n",
              "      <th>Unit Status</th>\n",
              "      <th>Active Fault</th>\n",
              "      <th>TO_sump</th>\n",
              "      <th>TO_feed</th>\n",
              "      <th>PO_feed</th>\n",
              "      <th>PO_net</th>\n",
              "      <th>TWCD</th>\n",
              "      <th>TWED</th>\n",
              "      <th>VSS</th>\n",
              "      <th>VSL</th>\n",
              "      <th>VH</th>\n",
              "      <th>VM</th>\n",
              "      <th>VC</th>\n",
              "      <th>VE</th>\n",
              "      <th>VW</th>\n",
              "      <th>TWI</th>\n",
              "      <th>TWO</th>\n",
              "      <th>THI</th>\n",
              "      <th>THO</th>\n",
              "      <th>FWW</th>\n",
              "      <th>FWH</th>\n",
              "      <th>FWB</th>\n",
              "      <th>severity</th>\n",
              "      <th>rorl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>70.67</td>\n",
              "      <td>70.6</td>\n",
              "      <td>71.76</td>\n",
              "      <td>71.7</td>\n",
              "      <td>71.73</td>\n",
              "      <td>71.5</td>\n",
              "      <td>72.59</td>\n",
              "      <td>72.5</td>\n",
              "      <td>72.24</td>\n",
              "      <td>73.22</td>\n",
              "      <td>70.89</td>\n",
              "      <td>71.37</td>\n",
              "      <td>1.383000e-46</td>\n",
              "      <td>5.720000e-47</td>\n",
              "      <td>1.590000e-46</td>\n",
              "      <td>2.400000e-46</td>\n",
              "      <td>1.407000e-46</td>\n",
              "      <td>-1.122000e-46</td>\n",
              "      <td>6.212000e-47</td>\n",
              "      <td>9.067000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>4.949000e-46</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.6</td>\n",
              "      <td>73.5</td>\n",
              "      <td>71.2</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>84.7</td>\n",
              "      <td>13.1</td>\n",
              "      <td>75.2</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>114.7</td>\n",
              "      <td>90.7</td>\n",
              "      <td>69.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>66.70</td>\n",
              "      <td>71.87</td>\n",
              "      <td>70.65</td>\n",
              "      <td>71.54</td>\n",
              "      <td>2.655000e-46</td>\n",
              "      <td>1.491000e-45</td>\n",
              "      <td>1.175000e-45</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>70.67</td>\n",
              "      <td>70.8</td>\n",
              "      <td>71.76</td>\n",
              "      <td>71.7</td>\n",
              "      <td>71.73</td>\n",
              "      <td>71.7</td>\n",
              "      <td>72.55</td>\n",
              "      <td>72.3</td>\n",
              "      <td>72.28</td>\n",
              "      <td>73.27</td>\n",
              "      <td>70.89</td>\n",
              "      <td>71.37</td>\n",
              "      <td>1.317000e-46</td>\n",
              "      <td>4.399000e-47</td>\n",
              "      <td>1.590000e-46</td>\n",
              "      <td>2.466000e-46</td>\n",
              "      <td>1.407000e-46</td>\n",
              "      <td>-1.122000e-46</td>\n",
              "      <td>6.212000e-47</td>\n",
              "      <td>9.067000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>4.949000e-46</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.6</td>\n",
              "      <td>73.5</td>\n",
              "      <td>71.2</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>84.7</td>\n",
              "      <td>13.1</td>\n",
              "      <td>75.2</td>\n",
              "      <td>3.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>114.5</td>\n",
              "      <td>90.8</td>\n",
              "      <td>69.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>66.70</td>\n",
              "      <td>71.87</td>\n",
              "      <td>70.65</td>\n",
              "      <td>71.58</td>\n",
              "      <td>2.042000e-46</td>\n",
              "      <td>1.491000e-45</td>\n",
              "      <td>1.175000e-45</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50</td>\n",
              "      <td>70.67</td>\n",
              "      <td>70.9</td>\n",
              "      <td>71.76</td>\n",
              "      <td>71.8</td>\n",
              "      <td>71.73</td>\n",
              "      <td>72.0</td>\n",
              "      <td>72.59</td>\n",
              "      <td>72.0</td>\n",
              "      <td>72.28</td>\n",
              "      <td>73.27</td>\n",
              "      <td>70.89</td>\n",
              "      <td>71.37</td>\n",
              "      <td>8.520000e+00</td>\n",
              "      <td>3.118000e+00</td>\n",
              "      <td>9.796000e+00</td>\n",
              "      <td>1.520000e+01</td>\n",
              "      <td>6.133000e+00</td>\n",
              "      <td>-4.889000e+00</td>\n",
              "      <td>2.707000e+00</td>\n",
              "      <td>3.951000e+00</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>2.156000e+01</td>\n",
              "      <td>2.742000e-46</td>\n",
              "      <td>2.383000e+02</td>\n",
              "      <td>1.350000e+02</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.1</td>\n",
              "      <td>72.5</td>\n",
              "      <td>71.9</td>\n",
              "      <td>74.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>84.7</td>\n",
              "      <td>13.6</td>\n",
              "      <td>75.4</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-8.392</td>\n",
              "      <td>-28.01</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>114.5</td>\n",
              "      <td>90.8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>66.70</td>\n",
              "      <td>71.87</td>\n",
              "      <td>70.69</td>\n",
              "      <td>71.54</td>\n",
              "      <td>1.447000e+01</td>\n",
              "      <td>6.496000e+01</td>\n",
              "      <td>1.210000e+02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50</td>\n",
              "      <td>70.75</td>\n",
              "      <td>71.5</td>\n",
              "      <td>71.76</td>\n",
              "      <td>71.4</td>\n",
              "      <td>71.82</td>\n",
              "      <td>71.7</td>\n",
              "      <td>72.51</td>\n",
              "      <td>72.0</td>\n",
              "      <td>72.24</td>\n",
              "      <td>73.06</td>\n",
              "      <td>70.97</td>\n",
              "      <td>71.37</td>\n",
              "      <td>7.644000e+00</td>\n",
              "      <td>3.006000e+00</td>\n",
              "      <td>9.060000e+00</td>\n",
              "      <td>1.370000e+01</td>\n",
              "      <td>8.448000e+00</td>\n",
              "      <td>-6.594000e+00</td>\n",
              "      <td>3.345000e+00</td>\n",
              "      <td>5.198000e+00</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>2.970000e+01</td>\n",
              "      <td>1.991000e-46</td>\n",
              "      <td>2.643000e+02</td>\n",
              "      <td>2.010000e+02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.4</td>\n",
              "      <td>73.3</td>\n",
              "      <td>71.6</td>\n",
              "      <td>73.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>84.8</td>\n",
              "      <td>13.4</td>\n",
              "      <td>75.4</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.824</td>\n",
              "      <td>10.51</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>113.8</td>\n",
              "      <td>90.8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>66.70</td>\n",
              "      <td>71.83</td>\n",
              "      <td>71.59</td>\n",
              "      <td>71.41</td>\n",
              "      <td>1.406000e+01</td>\n",
              "      <td>8.028000e+01</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>70.88</td>\n",
              "      <td>71.5</td>\n",
              "      <td>71.68</td>\n",
              "      <td>71.5</td>\n",
              "      <td>71.86</td>\n",
              "      <td>71.8</td>\n",
              "      <td>72.43</td>\n",
              "      <td>71.8</td>\n",
              "      <td>72.20</td>\n",
              "      <td>72.73</td>\n",
              "      <td>71.02</td>\n",
              "      <td>71.41</td>\n",
              "      <td>6.385000e+00</td>\n",
              "      <td>2.593000e+00</td>\n",
              "      <td>5.991000e+00</td>\n",
              "      <td>9.783000e+00</td>\n",
              "      <td>7.115000e+00</td>\n",
              "      <td>-5.881000e+00</td>\n",
              "      <td>3.534000e+00</td>\n",
              "      <td>4.768000e+00</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>2.502000e+01</td>\n",
              "      <td>2.363000e-46</td>\n",
              "      <td>2.682000e+02</td>\n",
              "      <td>2.123000e+02</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.1</td>\n",
              "      <td>72.8</td>\n",
              "      <td>71.6</td>\n",
              "      <td>73.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>84.9</td>\n",
              "      <td>13.8</td>\n",
              "      <td>75.2</td>\n",
              "      <td>3.8</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.566</td>\n",
              "      <td>11.43</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>113.8</td>\n",
              "      <td>90.8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>66.46</td>\n",
              "      <td>71.75</td>\n",
              "      <td>71.47</td>\n",
              "      <td>71.37</td>\n",
              "      <td>1.176000e+01</td>\n",
              "      <td>8.481000e+01</td>\n",
              "      <td>1.271000e+02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5186</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.1</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.40</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.85</td>\n",
              "      <td>57.00</td>\n",
              "      <td>5.248000e-47</td>\n",
              "      <td>2.419000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>4.042000e-48</td>\n",
              "      <td>1.984000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.4</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>57.3</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.7</td>\n",
              "      <td>17.8</td>\n",
              "      <td>97.3</td>\n",
              "      <td>39.8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>106.3</td>\n",
              "      <td>106.5</td>\n",
              "      <td>49.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.60</td>\n",
              "      <td>57.18</td>\n",
              "      <td>57.20</td>\n",
              "      <td>5.806000e-46</td>\n",
              "      <td>4.762000e-46</td>\n",
              "      <td>3.771000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5187</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>56.9</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.8</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.36</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.29</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>56.96</td>\n",
              "      <td>4.588000e-47</td>\n",
              "      <td>1.100000e-47</td>\n",
              "      <td>4.557000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>9.272000e-48</td>\n",
              "      <td>1.671000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>57.3</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.6</td>\n",
              "      <td>17.8</td>\n",
              "      <td>97.3</td>\n",
              "      <td>40.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>106.3</td>\n",
              "      <td>106.7</td>\n",
              "      <td>49.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.60</td>\n",
              "      <td>57.18</td>\n",
              "      <td>57.25</td>\n",
              "      <td>2.641000e-46</td>\n",
              "      <td>2.225000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5188</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.1</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.2</td>\n",
              "      <td>57.40</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>57.00</td>\n",
              "      <td>5.248000e-47</td>\n",
              "      <td>2.419000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>1.456000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.7</td>\n",
              "      <td>56.9</td>\n",
              "      <td>53.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.3</td>\n",
              "      <td>17.5</td>\n",
              "      <td>97.0</td>\n",
              "      <td>39.9</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>106.1</td>\n",
              "      <td>106.5</td>\n",
              "      <td>49.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.64</td>\n",
              "      <td>57.18</td>\n",
              "      <td>57.29</td>\n",
              "      <td>5.806000e-46</td>\n",
              "      <td>3.494000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5189</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.1</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.04</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.40</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>57.00</td>\n",
              "      <td>5.907000e-47</td>\n",
              "      <td>2.419000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-4.109000e-48</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>1.456000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>56.9</td>\n",
              "      <td>53.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.3</td>\n",
              "      <td>17.4</td>\n",
              "      <td>96.8</td>\n",
              "      <td>40.1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>105.9</td>\n",
              "      <td>106.5</td>\n",
              "      <td>49.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.60</td>\n",
              "      <td>57.14</td>\n",
              "      <td>57.29</td>\n",
              "      <td>5.806000e-46</td>\n",
              "      <td>3.494000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5190</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.2</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.8</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.36</td>\n",
              "      <td>57.2</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>57.00</td>\n",
              "      <td>4.588000e-47</td>\n",
              "      <td>1.759000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>1.456000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>57.3</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.1</td>\n",
              "      <td>17.5</td>\n",
              "      <td>96.8</td>\n",
              "      <td>39.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>105.9</td>\n",
              "      <td>106.3</td>\n",
              "      <td>49.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.64</td>\n",
              "      <td>57.10</td>\n",
              "      <td>57.33</td>\n",
              "      <td>4.222000e-46</td>\n",
              "      <td>3.494000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41528 rows × 67 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      TWE_set    TEI  TWEI    TEO  ...           FWH           FWB  severity  rorl\n",
              "0          50  70.67  70.6  71.76  ...  1.491000e-45  1.175000e-45         1     0\n",
              "1          50  70.67  70.8  71.76  ...  1.491000e-45  1.175000e-45         1     0\n",
              "2          50  70.67  70.9  71.76  ...  6.496000e+01  1.210000e+02         1     0\n",
              "3          50  70.75  71.5  71.76  ...  8.028000e+01  1.060000e+02         1     0\n",
              "4          50  70.88  71.5  71.68  ...  8.481000e+01  1.271000e+02         1     0\n",
              "...       ...    ...   ...    ...  ...           ...           ...       ...   ...\n",
              "5186       40  56.83  57.1  56.82  ...  4.762000e-46  3.771000e-45         4     1\n",
              "5187       40  56.83  56.9  56.82  ...  2.225000e-46  3.644000e-45         4     1\n",
              "5188       40  56.83  57.1  56.82  ...  3.494000e-46  3.644000e-45         4     1\n",
              "5189       40  56.83  57.1  56.82  ...  3.494000e-46  3.644000e-45         4     1\n",
              "5190       40  56.83  57.2  56.82  ...  3.494000e-46  3.644000e-45         4     1\n",
              "\n",
              "[41528 rows x 67 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ2JhmuxinTS"
      },
      "source": [
        "# 표준화\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(data1.iloc[:,:-2])\n",
        "X_scaled = scaler.transform(data1.iloc[:,:-2])\n",
        "X_scaled = pd.DataFrame(X_scaled).reset_index(drop=True)\n",
        "\n",
        "y_rorl_sev = pd.DataFrame(data1.iloc[:,-2:]).reset_index(drop=True)\n",
        "\n",
        "X = pd.concat([X_scaled, y_rorl_sev], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "nggjPKMhjUxR",
        "outputId": "e21dffd2-a8c8-45ea-be70-263505f4e546"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>severity</th>\n",
              "      <th>rorl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.335243</td>\n",
              "      <td>3.402779</td>\n",
              "      <td>3.327928</td>\n",
              "      <td>5.326046</td>\n",
              "      <td>5.215082</td>\n",
              "      <td>-0.420948</td>\n",
              "      <td>-0.422333</td>\n",
              "      <td>-1.051454</td>\n",
              "      <td>-1.046437</td>\n",
              "      <td>-0.900070</td>\n",
              "      <td>-0.004790</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>3.510340</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.765152</td>\n",
              "      <td>-1.673922</td>\n",
              "      <td>7.784881</td>\n",
              "      <td>8.806172</td>\n",
              "      <td>-1.544866</td>\n",
              "      <td>-1.482462</td>\n",
              "      <td>-1.664642</td>\n",
              "      <td>6.882298</td>\n",
              "      <td>4.098866</td>\n",
              "      <td>-3.769125</td>\n",
              "      <td>-2.652622</td>\n",
              "      <td>-3.686021</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-11.203416</td>\n",
              "      <td>20.560203</td>\n",
              "      <td>0.077702</td>\n",
              "      <td>-5.840967</td>\n",
              "      <td>-6.262078</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.123650</td>\n",
              "      <td>-2.428707</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.953836</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-2.405616</td>\n",
              "      <td>4.134077</td>\n",
              "      <td>0.257199</td>\n",
              "      <td>2.079823</td>\n",
              "      <td>2.838880</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.335243</td>\n",
              "      <td>3.402779</td>\n",
              "      <td>3.364809</td>\n",
              "      <td>5.326046</td>\n",
              "      <td>5.215082</td>\n",
              "      <td>-0.420948</td>\n",
              "      <td>-0.396225</td>\n",
              "      <td>-1.056382</td>\n",
              "      <td>-1.070751</td>\n",
              "      <td>-0.894954</td>\n",
              "      <td>-0.004789</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>3.510340</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.765152</td>\n",
              "      <td>-1.673922</td>\n",
              "      <td>7.784881</td>\n",
              "      <td>8.806172</td>\n",
              "      <td>-1.544866</td>\n",
              "      <td>-1.482462</td>\n",
              "      <td>-1.664642</td>\n",
              "      <td>6.882298</td>\n",
              "      <td>4.098866</td>\n",
              "      <td>-3.769125</td>\n",
              "      <td>-2.669300</td>\n",
              "      <td>-3.686021</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-11.203416</td>\n",
              "      <td>20.560203</td>\n",
              "      <td>0.023122</td>\n",
              "      <td>-5.810746</td>\n",
              "      <td>-6.262078</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.282125</td>\n",
              "      <td>-2.428707</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.953836</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-0.651139</td>\n",
              "      <td>4.134077</td>\n",
              "      <td>0.257199</td>\n",
              "      <td>2.079823</td>\n",
              "      <td>2.845402</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.335243</td>\n",
              "      <td>3.402779</td>\n",
              "      <td>3.383250</td>\n",
              "      <td>5.326046</td>\n",
              "      <td>5.235392</td>\n",
              "      <td>-0.420948</td>\n",
              "      <td>-0.357063</td>\n",
              "      <td>-1.051454</td>\n",
              "      <td>-1.107222</td>\n",
              "      <td>-0.894954</td>\n",
              "      <td>-0.004789</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>3.510340</td>\n",
              "      <td>-2.308409</td>\n",
              "      <td>-1.950887</td>\n",
              "      <td>-0.005095</td>\n",
              "      <td>-0.004748</td>\n",
              "      <td>-2.180182</td>\n",
              "      <td>-0.007872</td>\n",
              "      <td>0.004794</td>\n",
              "      <td>0.626725</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>2.783456</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-1.960805</td>\n",
              "      <td>-6.673784</td>\n",
              "      <td>-2.447019</td>\n",
              "      <td>-1.673922</td>\n",
              "      <td>7.662305</td>\n",
              "      <td>8.576831</td>\n",
              "      <td>-1.467809</td>\n",
              "      <td>-1.419856</td>\n",
              "      <td>-1.641132</td>\n",
              "      <td>6.882298</td>\n",
              "      <td>4.342110</td>\n",
              "      <td>-3.752682</td>\n",
              "      <td>-2.694316</td>\n",
              "      <td>-3.614197</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004909</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>0.023122</td>\n",
              "      <td>-5.810746</td>\n",
              "      <td>-6.359710</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.519838</td>\n",
              "      <td>-2.428707</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.953836</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-0.651139</td>\n",
              "      <td>4.134077</td>\n",
              "      <td>0.257199</td>\n",
              "      <td>2.085309</td>\n",
              "      <td>2.838880</td>\n",
              "      <td>-0.751171</td>\n",
              "      <td>-0.004918</td>\n",
              "      <td>0.004511</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.335243</td>\n",
              "      <td>3.417711</td>\n",
              "      <td>3.493896</td>\n",
              "      <td>5.326046</td>\n",
              "      <td>5.154151</td>\n",
              "      <td>-0.408945</td>\n",
              "      <td>-0.396225</td>\n",
              "      <td>-1.061310</td>\n",
              "      <td>-1.107222</td>\n",
              "      <td>-0.900070</td>\n",
              "      <td>-0.004792</td>\n",
              "      <td>0.003633</td>\n",
              "      <td>3.510340</td>\n",
              "      <td>-2.341469</td>\n",
              "      <td>-1.963326</td>\n",
              "      <td>-0.005096</td>\n",
              "      <td>-0.004750</td>\n",
              "      <td>-2.081504</td>\n",
              "      <td>-0.007958</td>\n",
              "      <td>0.004826</td>\n",
              "      <td>0.952783</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>4.065771</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.228081</td>\n",
              "      <td>-1.140284</td>\n",
              "      <td>-2.810600</td>\n",
              "      <td>-1.673922</td>\n",
              "      <td>7.735851</td>\n",
              "      <td>8.760304</td>\n",
              "      <td>-1.500834</td>\n",
              "      <td>-1.451159</td>\n",
              "      <td>-1.688153</td>\n",
              "      <td>6.899425</td>\n",
              "      <td>4.244813</td>\n",
              "      <td>-3.752682</td>\n",
              "      <td>-2.685977</td>\n",
              "      <td>-3.662080</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-0.167907</td>\n",
              "      <td>-5.810746</td>\n",
              "      <td>-6.359710</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.440600</td>\n",
              "      <td>-2.391489</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.953836</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-0.651139</td>\n",
              "      <td>4.134077</td>\n",
              "      <td>0.251245</td>\n",
              "      <td>2.208740</td>\n",
              "      <td>2.817681</td>\n",
              "      <td>-0.757975</td>\n",
              "      <td>-0.004885</td>\n",
              "      <td>0.004252</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.335243</td>\n",
              "      <td>3.441974</td>\n",
              "      <td>3.493896</td>\n",
              "      <td>5.309470</td>\n",
              "      <td>5.174461</td>\n",
              "      <td>-0.403610</td>\n",
              "      <td>-0.383171</td>\n",
              "      <td>-1.071167</td>\n",
              "      <td>-1.131536</td>\n",
              "      <td>-0.905187</td>\n",
              "      <td>-0.004796</td>\n",
              "      <td>0.003655</td>\n",
              "      <td>3.517826</td>\n",
              "      <td>-2.388982</td>\n",
              "      <td>-2.009195</td>\n",
              "      <td>-0.005100</td>\n",
              "      <td>-0.004755</td>\n",
              "      <td>-2.138324</td>\n",
              "      <td>-0.007922</td>\n",
              "      <td>0.004835</td>\n",
              "      <td>0.840349</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>3.328519</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>0.031828</td>\n",
              "      <td>-0.192882</td>\n",
              "      <td>-2.674257</td>\n",
              "      <td>-1.673922</td>\n",
              "      <td>7.662305</td>\n",
              "      <td>8.645634</td>\n",
              "      <td>-1.500834</td>\n",
              "      <td>-1.451159</td>\n",
              "      <td>-1.664642</td>\n",
              "      <td>6.916553</td>\n",
              "      <td>4.439408</td>\n",
              "      <td>-3.769125</td>\n",
              "      <td>-2.669300</td>\n",
              "      <td>-3.656095</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-0.167907</td>\n",
              "      <td>-5.810746</td>\n",
              "      <td>-6.359710</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.519838</td>\n",
              "      <td>-2.391489</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.953836</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-0.651139</td>\n",
              "      <td>4.016591</td>\n",
              "      <td>0.239336</td>\n",
              "      <td>2.192282</td>\n",
              "      <td>2.811159</td>\n",
              "      <td>-0.796150</td>\n",
              "      <td>-0.004876</td>\n",
              "      <td>0.004617</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41523</th>\n",
              "      <td>-1.074138</td>\n",
              "      <td>0.819658</td>\n",
              "      <td>0.838407</td>\n",
              "      <td>2.230424</td>\n",
              "      <td>2.209143</td>\n",
              "      <td>-2.374790</td>\n",
              "      <td>-2.302082</td>\n",
              "      <td>-2.922942</td>\n",
              "      <td>-2.942924</td>\n",
              "      <td>-2.817463</td>\n",
              "      <td>-0.005007</td>\n",
              "      <td>-0.002744</td>\n",
              "      <td>0.821058</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.765152</td>\n",
              "      <td>-1.512094</td>\n",
              "      <td>4.156631</td>\n",
              "      <td>4.219358</td>\n",
              "      <td>-3.074998</td>\n",
              "      <td>-2.665702</td>\n",
              "      <td>-1.688153</td>\n",
              "      <td>5.169589</td>\n",
              "      <td>6.385363</td>\n",
              "      <td>-1.952184</td>\n",
              "      <td>0.332614</td>\n",
              "      <td>-3.680036</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-2.214653</td>\n",
              "      <td>-1.066068</td>\n",
              "      <td>-8.146378</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.519838</td>\n",
              "      <td>-2.317053</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.663842</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-2.405616</td>\n",
              "      <td>-1.113654</td>\n",
              "      <td>-2.015927</td>\n",
              "      <td>0.232477</td>\n",
              "      <td>0.500567</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41524</th>\n",
              "      <td>-1.074138</td>\n",
              "      <td>0.819658</td>\n",
              "      <td>0.801526</td>\n",
              "      <td>2.230424</td>\n",
              "      <td>2.188833</td>\n",
              "      <td>-2.374790</td>\n",
              "      <td>-2.302082</td>\n",
              "      <td>-2.927870</td>\n",
              "      <td>-2.918610</td>\n",
              "      <td>-2.812347</td>\n",
              "      <td>-0.005007</td>\n",
              "      <td>-0.002726</td>\n",
              "      <td>0.813572</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.765152</td>\n",
              "      <td>-1.593008</td>\n",
              "      <td>4.156631</td>\n",
              "      <td>4.219358</td>\n",
              "      <td>-3.074998</td>\n",
              "      <td>-2.665702</td>\n",
              "      <td>-1.688153</td>\n",
              "      <td>5.152462</td>\n",
              "      <td>6.385363</td>\n",
              "      <td>-1.952184</td>\n",
              "      <td>0.357630</td>\n",
              "      <td>-3.680036</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-2.214653</td>\n",
              "      <td>-1.005626</td>\n",
              "      <td>-8.195194</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.440600</td>\n",
              "      <td>-2.428707</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.663842</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-2.405616</td>\n",
              "      <td>-1.113654</td>\n",
              "      <td>-2.015927</td>\n",
              "      <td>0.232477</td>\n",
              "      <td>0.508720</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41525</th>\n",
              "      <td>-1.074138</td>\n",
              "      <td>0.819658</td>\n",
              "      <td>0.838407</td>\n",
              "      <td>2.230424</td>\n",
              "      <td>2.209143</td>\n",
              "      <td>-2.374790</td>\n",
              "      <td>-2.289029</td>\n",
              "      <td>-2.922942</td>\n",
              "      <td>-2.918610</td>\n",
              "      <td>-2.817463</td>\n",
              "      <td>-0.005007</td>\n",
              "      <td>-0.002726</td>\n",
              "      <td>0.821058</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.810600</td>\n",
              "      <td>-1.593008</td>\n",
              "      <td>4.156631</td>\n",
              "      <td>4.265226</td>\n",
              "      <td>-3.119031</td>\n",
              "      <td>-2.697005</td>\n",
              "      <td>-1.688153</td>\n",
              "      <td>5.101081</td>\n",
              "      <td>6.239417</td>\n",
              "      <td>-1.976848</td>\n",
              "      <td>0.340952</td>\n",
              "      <td>-3.638139</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-2.269233</td>\n",
              "      <td>-1.066068</td>\n",
              "      <td>-8.146378</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.519838</td>\n",
              "      <td>-2.354271</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.663842</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-2.405616</td>\n",
              "      <td>-1.113654</td>\n",
              "      <td>-2.009973</td>\n",
              "      <td>0.232477</td>\n",
              "      <td>0.515243</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41526</th>\n",
              "      <td>-1.074138</td>\n",
              "      <td>0.819658</td>\n",
              "      <td>0.838407</td>\n",
              "      <td>2.230424</td>\n",
              "      <td>2.209143</td>\n",
              "      <td>-2.380125</td>\n",
              "      <td>-2.302082</td>\n",
              "      <td>-2.922942</td>\n",
              "      <td>-2.918610</td>\n",
              "      <td>-2.817463</td>\n",
              "      <td>-0.005007</td>\n",
              "      <td>-0.002726</td>\n",
              "      <td>0.821058</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.765152</td>\n",
              "      <td>-1.673922</td>\n",
              "      <td>4.156631</td>\n",
              "      <td>4.219358</td>\n",
              "      <td>-3.119031</td>\n",
              "      <td>-2.697005</td>\n",
              "      <td>-1.688153</td>\n",
              "      <td>5.101081</td>\n",
              "      <td>6.190768</td>\n",
              "      <td>-1.993291</td>\n",
              "      <td>0.357630</td>\n",
              "      <td>-3.668065</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-2.323812</td>\n",
              "      <td>-1.066068</td>\n",
              "      <td>-8.195194</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.519838</td>\n",
              "      <td>-2.317053</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.663842</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-2.405616</td>\n",
              "      <td>-1.113654</td>\n",
              "      <td>-2.015927</td>\n",
              "      <td>0.226991</td>\n",
              "      <td>0.515243</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41527</th>\n",
              "      <td>-1.074138</td>\n",
              "      <td>0.819658</td>\n",
              "      <td>0.856848</td>\n",
              "      <td>2.230424</td>\n",
              "      <td>2.188833</td>\n",
              "      <td>-2.374790</td>\n",
              "      <td>-2.302082</td>\n",
              "      <td>-2.927870</td>\n",
              "      <td>-2.906453</td>\n",
              "      <td>-2.817463</td>\n",
              "      <td>-0.005007</td>\n",
              "      <td>-0.002726</td>\n",
              "      <td>0.821058</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.765152</td>\n",
              "      <td>-1.633465</td>\n",
              "      <td>4.156631</td>\n",
              "      <td>4.219358</td>\n",
              "      <td>-3.074998</td>\n",
              "      <td>-2.665702</td>\n",
              "      <td>-1.688153</td>\n",
              "      <td>5.066827</td>\n",
              "      <td>6.239417</td>\n",
              "      <td>-1.993291</td>\n",
              "      <td>0.290921</td>\n",
              "      <td>-3.686021</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-2.323812</td>\n",
              "      <td>-1.126509</td>\n",
              "      <td>-8.146378</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.519838</td>\n",
              "      <td>-2.317053</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.663842</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-2.405616</td>\n",
              "      <td>-1.113654</td>\n",
              "      <td>-2.009973</td>\n",
              "      <td>0.221505</td>\n",
              "      <td>0.521765</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41528 rows × 67 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1         2  ...        64  severity  rorl\n",
              "0      1.335243  3.402779  3.327928  ...  0.002418         1     0\n",
              "1      1.335243  3.402779  3.364809  ...  0.002418         1     0\n",
              "2      1.335243  3.402779  3.383250  ...  0.004511         1     0\n",
              "3      1.335243  3.417711  3.493896  ...  0.004252         1     0\n",
              "4      1.335243  3.441974  3.493896  ...  0.004617         1     0\n",
              "...         ...       ...       ...  ...       ...       ...   ...\n",
              "41523 -1.074138  0.819658  0.838407  ...  0.002418         4     1\n",
              "41524 -1.074138  0.819658  0.801526  ...  0.002418         4     1\n",
              "41525 -1.074138  0.819658  0.838407  ...  0.002418         4     1\n",
              "41526 -1.074138  0.819658  0.838407  ...  0.002418         4     1\n",
              "41527 -1.074138  0.819658  0.856848  ...  0.002418         4     1\n",
              "\n",
              "[41528 rows x 67 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkRHHKqljY3V"
      },
      "source": [
        "X2 = X.iloc[:,:-1]\n",
        "y_rorl = X.iloc[:,-2:]\n",
        "y_sev = data1.iloc[:,-2]\n",
        "\n",
        "\n",
        "# train, test 분리\n",
        "#rorl\n",
        "X3_train, X3_test, rorl_train, rorl_test = ms.train_test_split(X2, y_rorl, \n",
        "                                                      test_size = 0.25, random_state = 100, stratify = y_sev )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZUqB2OcellV"
      },
      "source": [
        "# train 셋 (2,3,4만)\n",
        "X3_train = X3_train.loc[X3_train[\"severity\"] != 1].iloc[:,:-1]\n",
        "rorl_train = rorl_train.loc[rorl_train[\"severity\"] != 1].iloc[:,-1]\n",
        "\n",
        "X3_test = X3_test.iloc[:,:-1]\n",
        "rorl_test = rorl_test.iloc[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akr8zZZrlvPq"
      },
      "source": [
        "모델이당"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo56pE45loEg",
        "outputId": "1e710d16-8b1c-4ff6-ea7e-0a9887b84f19"
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn import metrics   \n",
        "\n",
        "#분류->object 회귀\n",
        "start = time.time()\n",
        "\n",
        "# 부스팅 타입은 default값인 gbdt로, 학습률은 0.01로 지정 하였음\n",
        "LGB = lgb.LGBMClassifier(objective=\"regression\", boosting_type='gbdt') #learning_rate = 0.01\n",
        "\n",
        "param_list = {\"n_estimators\": list(range(10, 300, 10)),\n",
        "              \"max_depth\": list(range(4, 21, 4)),\n",
        "              \"max_features\": list(range(3, 13, 2)),\n",
        "              \"min_samples_split\": list(range(3, 13, 2)),\n",
        "              \"learning_rate\": [0.0001,0.001,0.01,0.1,1]}\n",
        "\n",
        "# 하이퍼파라미터 최적화\n",
        "LGB_random_search = RandomizedSearchCV(estimator = LGB,\n",
        "                                        param_distributions = param_list,\n",
        "                                        n_iter = 10,      # 10번반복하는 lightgbm 구현 : 성능개선 시도\n",
        "                                        cv = 3,           # cross-validation 3번 반복\n",
        "                                        n_jobs = -1,\n",
        "                                        random_state=42)\n",
        "\n",
        "LGB_random_search.fit(X3_train, rorl_train)\n",
        "y_pred = LGB_random_search.predict(X3_test)\n",
        "\n",
        "#성능평가\n",
        "print('정확도 :', metrics.accuracy_score(rorl_test, y_pred))\n",
        "print(confusion_matrix(rorl_test, y_pred))\n",
        "print(classification_report(rorl_test, y_pred))\n",
        "\n",
        "print( LGB_random_search.best_params_ )\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정확도 : 0.9339240994028125\n",
            "[[5006  169]\n",
            " [ 517 4690]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94      5175\n",
            "           1       0.97      0.90      0.93      5207\n",
            "\n",
            "    accuracy                           0.93     10382\n",
            "   macro avg       0.94      0.93      0.93     10382\n",
            "weighted avg       0.94      0.93      0.93     10382\n",
            "\n",
            "{'n_estimators': 170, 'min_samples_split': 7, 'max_features': 7, 'max_depth': 8, 'learning_rate': 0.1}\n",
            "0:00:43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF9kunh3Tycd"
      },
      "source": [
        "DNN 이당"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZfjQFu8fKrW"
      },
      "source": [
        "# 원핫 인코딩 (One_Hot)\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(rorl_train)\n",
        "one_hot_test_labels = to_categorical(rorl_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwkjD5mmxy5u"
      },
      "source": [
        "\n",
        "\n",
        "*   https://keras.io/ko/visualization/\n",
        "*   https://tykimos.github.io/2019/05/10/KerasTuner/\n",
        "\n",
        "\n",
        "*   Learning rate: https://jjeongil.tistory.com/823\n",
        "*   https://keras.io/ko/optimizers/#adam\n",
        "*   https://keras.io/api/optimizers/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PV-TrfTwemKt",
        "outputId": "01ce2b3d-0c17-4754-9874-61bbe13dd818"
      },
      "source": [
        "start = time.time()\n",
        "# 라이브러리\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='RandomUniform',activation='tanh')) # uniform 93, RandomUniform 94, lecun_uniform 93, RandomNormal 93\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))   # relu\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dense(2, activation='sigmoid')) # sigmoid\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "# model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer= opt, loss=\"binary_crossentropy\", metrics=['accuracy'])   # Learning rate 추가 \n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "hist = model.fit(X3_train, one_hot_train_labels, epochs=500, batch_size=64)\n",
        "\n",
        "    \n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(X3_test, one_hot_test_labels, batch_size=64)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.1411 - accuracy: 0.9505\n",
            "Epoch 2/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0651 - accuracy: 0.9812\n",
            "Epoch 3/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0568 - accuracy: 0.9819\n",
            "Epoch 4/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0513 - accuracy: 0.9827\n",
            "Epoch 5/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0507 - accuracy: 0.9827\n",
            "Epoch 6/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0507 - accuracy: 0.9816\n",
            "Epoch 7/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0465 - accuracy: 0.9823\n",
            "Epoch 8/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0468 - accuracy: 0.9828\n",
            "Epoch 9/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0452 - accuracy: 0.9829\n",
            "Epoch 10/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0444 - accuracy: 0.9829\n",
            "Epoch 11/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0419 - accuracy: 0.9847\n",
            "Epoch 12/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0448 - accuracy: 0.9831\n",
            "Epoch 13/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0398 - accuracy: 0.9842\n",
            "Epoch 14/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0417 - accuracy: 0.9837\n",
            "Epoch 15/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0424 - accuracy: 0.9838\n",
            "Epoch 16/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0407 - accuracy: 0.9847\n",
            "Epoch 17/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0408 - accuracy: 0.9839\n",
            "Epoch 18/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0370 - accuracy: 0.9855\n",
            "Epoch 19/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0413 - accuracy: 0.9848\n",
            "Epoch 20/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0398 - accuracy: 0.9843\n",
            "Epoch 21/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0386 - accuracy: 0.9858\n",
            "Epoch 22/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0386 - accuracy: 0.9836\n",
            "Epoch 23/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0372 - accuracy: 0.9856\n",
            "Epoch 24/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0361 - accuracy: 0.9849\n",
            "Epoch 25/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0375 - accuracy: 0.9857\n",
            "Epoch 26/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0366 - accuracy: 0.9856\n",
            "Epoch 27/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0333 - accuracy: 0.9863\n",
            "Epoch 28/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0372 - accuracy: 0.9851\n",
            "Epoch 29/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0348 - accuracy: 0.9870\n",
            "Epoch 30/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0366 - accuracy: 0.9846\n",
            "Epoch 31/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0357 - accuracy: 0.9857\n",
            "Epoch 32/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0351 - accuracy: 0.9859\n",
            "Epoch 33/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0366 - accuracy: 0.9849\n",
            "Epoch 34/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0347 - accuracy: 0.9860\n",
            "Epoch 35/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0334 - accuracy: 0.9865\n",
            "Epoch 36/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0355 - accuracy: 0.9858\n",
            "Epoch 37/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0345 - accuracy: 0.9862\n",
            "Epoch 38/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0345 - accuracy: 0.9859\n",
            "Epoch 39/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0321 - accuracy: 0.9865\n",
            "Epoch 40/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0315 - accuracy: 0.9867\n",
            "Epoch 41/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0325 - accuracy: 0.9866\n",
            "Epoch 42/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0321 - accuracy: 0.9864\n",
            "Epoch 43/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0337 - accuracy: 0.9864\n",
            "Epoch 44/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0312 - accuracy: 0.9867\n",
            "Epoch 45/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0327 - accuracy: 0.9862\n",
            "Epoch 46/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0305 - accuracy: 0.9871\n",
            "Epoch 47/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0323 - accuracy: 0.9859\n",
            "Epoch 48/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0335 - accuracy: 0.9857\n",
            "Epoch 49/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0324 - accuracy: 0.9860\n",
            "Epoch 50/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0322 - accuracy: 0.9868\n",
            "Epoch 51/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0303 - accuracy: 0.9869\n",
            "Epoch 52/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0306 - accuracy: 0.9859\n",
            "Epoch 53/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0306 - accuracy: 0.9875\n",
            "Epoch 54/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0319 - accuracy: 0.9859\n",
            "Epoch 55/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0312 - accuracy: 0.9860\n",
            "Epoch 56/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0310 - accuracy: 0.9863\n",
            "Epoch 57/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0304 - accuracy: 0.9865\n",
            "Epoch 58/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0290 - accuracy: 0.9874\n",
            "Epoch 59/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0296 - accuracy: 0.9872\n",
            "Epoch 60/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0319 - accuracy: 0.9860\n",
            "Epoch 61/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0286 - accuracy: 0.9866\n",
            "Epoch 62/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0285 - accuracy: 0.9870\n",
            "Epoch 63/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0283 - accuracy: 0.9870\n",
            "Epoch 64/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0279 - accuracy: 0.9877\n",
            "Epoch 65/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0282 - accuracy: 0.9878\n",
            "Epoch 66/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0296 - accuracy: 0.9864\n",
            "Epoch 67/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0250 - accuracy: 0.9889\n",
            "Epoch 68/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0301 - accuracy: 0.9857\n",
            "Epoch 69/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0297 - accuracy: 0.9857\n",
            "Epoch 70/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0296 - accuracy: 0.9861\n",
            "Epoch 71/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0270 - accuracy: 0.9882\n",
            "Epoch 72/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0310 - accuracy: 0.9851\n",
            "Epoch 73/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0280 - accuracy: 0.9872\n",
            "Epoch 74/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0265 - accuracy: 0.9881\n",
            "Epoch 75/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0278 - accuracy: 0.9878\n",
            "Epoch 76/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0257 - accuracy: 0.9878\n",
            "Epoch 77/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0262 - accuracy: 0.9876\n",
            "Epoch 78/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0272 - accuracy: 0.9878\n",
            "Epoch 79/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0255 - accuracy: 0.9878\n",
            "Epoch 80/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0278 - accuracy: 0.9881\n",
            "Epoch 81/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0250 - accuracy: 0.9883\n",
            "Epoch 82/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0266 - accuracy: 0.9881\n",
            "Epoch 83/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0257 - accuracy: 0.9889\n",
            "Epoch 84/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0244 - accuracy: 0.9885\n",
            "Epoch 85/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0256 - accuracy: 0.9879\n",
            "Epoch 86/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0257 - accuracy: 0.9880\n",
            "Epoch 87/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0250 - accuracy: 0.9889\n",
            "Epoch 88/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0247 - accuracy: 0.9884\n",
            "Epoch 89/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0251 - accuracy: 0.9881\n",
            "Epoch 90/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0254 - accuracy: 0.9881\n",
            "Epoch 91/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0263 - accuracy: 0.9874\n",
            "Epoch 92/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0249 - accuracy: 0.9870\n",
            "Epoch 93/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0273 - accuracy: 0.9881\n",
            "Epoch 94/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0226 - accuracy: 0.9899\n",
            "Epoch 95/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0248 - accuracy: 0.9885\n",
            "Epoch 96/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0256 - accuracy: 0.9883\n",
            "Epoch 97/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0262 - accuracy: 0.9875\n",
            "Epoch 98/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0222 - accuracy: 0.9894\n",
            "Epoch 99/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0240 - accuracy: 0.9890\n",
            "Epoch 100/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0255 - accuracy: 0.9875\n",
            "Epoch 101/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9892\n",
            "Epoch 102/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0268 - accuracy: 0.9881\n",
            "Epoch 103/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0244 - accuracy: 0.9884\n",
            "Epoch 104/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0241 - accuracy: 0.9887\n",
            "Epoch 105/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0248 - accuracy: 0.9891\n",
            "Epoch 106/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0234 - accuracy: 0.9898\n",
            "Epoch 107/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0249 - accuracy: 0.9889\n",
            "Epoch 108/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0235 - accuracy: 0.9890\n",
            "Epoch 109/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0251 - accuracy: 0.9880\n",
            "Epoch 110/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0251 - accuracy: 0.9888\n",
            "Epoch 111/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9891\n",
            "Epoch 112/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0253 - accuracy: 0.9888\n",
            "Epoch 113/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0242 - accuracy: 0.9890\n",
            "Epoch 114/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0257 - accuracy: 0.9887\n",
            "Epoch 115/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0224 - accuracy: 0.9896\n",
            "Epoch 116/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0267 - accuracy: 0.9881\n",
            "Epoch 117/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9888\n",
            "Epoch 118/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0265 - accuracy: 0.9893\n",
            "Epoch 119/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0231 - accuracy: 0.9891\n",
            "Epoch 120/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0207 - accuracy: 0.9902\n",
            "Epoch 121/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9892\n",
            "Epoch 122/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0251 - accuracy: 0.9879\n",
            "Epoch 123/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0241 - accuracy: 0.9881\n",
            "Epoch 124/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0241 - accuracy: 0.9888\n",
            "Epoch 125/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0229 - accuracy: 0.9899\n",
            "Epoch 126/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0232 - accuracy: 0.9895\n",
            "Epoch 127/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0231 - accuracy: 0.9896\n",
            "Epoch 128/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0228 - accuracy: 0.9902\n",
            "Epoch 129/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0249 - accuracy: 0.9890\n",
            "Epoch 130/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0263 - accuracy: 0.9885\n",
            "Epoch 131/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0255 - accuracy: 0.9889\n",
            "Epoch 132/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9906\n",
            "Epoch 133/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9898\n",
            "Epoch 134/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9897\n",
            "Epoch 135/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0222 - accuracy: 0.9895\n",
            "Epoch 136/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0213 - accuracy: 0.9897\n",
            "Epoch 137/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0230 - accuracy: 0.9898\n",
            "Epoch 138/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0219 - accuracy: 0.9905\n",
            "Epoch 139/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0235 - accuracy: 0.9904\n",
            "Epoch 140/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0218 - accuracy: 0.9895\n",
            "Epoch 141/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9901\n",
            "Epoch 142/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0240 - accuracy: 0.9886\n",
            "Epoch 143/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0207 - accuracy: 0.9902\n",
            "Epoch 144/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0246 - accuracy: 0.9887\n",
            "Epoch 145/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0247 - accuracy: 0.9893\n",
            "Epoch 146/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0227 - accuracy: 0.9908\n",
            "Epoch 147/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0231 - accuracy: 0.9903\n",
            "Epoch 148/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9894\n",
            "Epoch 149/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0231 - accuracy: 0.9894\n",
            "Epoch 150/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0220 - accuracy: 0.9907\n",
            "Epoch 151/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0236 - accuracy: 0.9896\n",
            "Epoch 152/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9903\n",
            "Epoch 153/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0212 - accuracy: 0.9903\n",
            "Epoch 154/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0224 - accuracy: 0.9897\n",
            "Epoch 155/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0227 - accuracy: 0.9898\n",
            "Epoch 156/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0221 - accuracy: 0.9900\n",
            "Epoch 157/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 0.9903\n",
            "Epoch 158/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9910\n",
            "Epoch 159/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0213 - accuracy: 0.9900\n",
            "Epoch 160/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0247 - accuracy: 0.9893\n",
            "Epoch 161/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0225 - accuracy: 0.9899\n",
            "Epoch 162/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0208 - accuracy: 0.9898\n",
            "Epoch 163/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9896\n",
            "Epoch 164/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0207 - accuracy: 0.9905\n",
            "Epoch 165/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9902\n",
            "Epoch 166/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0234 - accuracy: 0.9893\n",
            "Epoch 167/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0224 - accuracy: 0.9898\n",
            "Epoch 168/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9902\n",
            "Epoch 169/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0221 - accuracy: 0.9897\n",
            "Epoch 170/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 0.9908\n",
            "Epoch 171/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0219 - accuracy: 0.9890\n",
            "Epoch 172/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0207 - accuracy: 0.9898\n",
            "Epoch 173/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9906\n",
            "Epoch 174/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9902\n",
            "Epoch 175/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0204 - accuracy: 0.9906\n",
            "Epoch 176/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0221 - accuracy: 0.9897\n",
            "Epoch 177/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9909\n",
            "Epoch 178/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0226 - accuracy: 0.9902\n",
            "Epoch 179/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9896\n",
            "Epoch 180/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0250 - accuracy: 0.9895\n",
            "Epoch 181/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0216 - accuracy: 0.9903\n",
            "Epoch 182/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9906\n",
            "Epoch 183/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0206 - accuracy: 0.9908\n",
            "Epoch 184/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0225 - accuracy: 0.9889\n",
            "Epoch 185/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0261 - accuracy: 0.9891\n",
            "Epoch 186/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0224 - accuracy: 0.9901\n",
            "Epoch 187/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0219 - accuracy: 0.9899\n",
            "Epoch 188/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9900\n",
            "Epoch 189/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0208 - accuracy: 0.9903\n",
            "Epoch 190/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0227 - accuracy: 0.9896\n",
            "Epoch 191/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9910\n",
            "Epoch 192/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0252 - accuracy: 0.9881\n",
            "Epoch 193/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0197 - accuracy: 0.9904\n",
            "Epoch 194/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0208 - accuracy: 0.9907\n",
            "Epoch 195/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0208 - accuracy: 0.9897\n",
            "Epoch 196/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0200 - accuracy: 0.9911\n",
            "Epoch 197/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0259 - accuracy: 0.9891\n",
            "Epoch 198/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0183 - accuracy: 0.9912\n",
            "Epoch 199/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9906\n",
            "Epoch 200/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0226 - accuracy: 0.9898\n",
            "Epoch 201/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0213 - accuracy: 0.9905\n",
            "Epoch 202/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9919\n",
            "Epoch 203/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9914\n",
            "Epoch 204/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0196 - accuracy: 0.9910\n",
            "Epoch 205/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0214 - accuracy: 0.9901\n",
            "Epoch 206/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0197 - accuracy: 0.9914\n",
            "Epoch 207/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0222 - accuracy: 0.9904\n",
            "Epoch 208/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9911\n",
            "Epoch 209/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0222 - accuracy: 0.9906\n",
            "Epoch 210/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0211 - accuracy: 0.9900\n",
            "Epoch 211/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0212 - accuracy: 0.9902\n",
            "Epoch 212/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0204 - accuracy: 0.9901\n",
            "Epoch 213/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9914\n",
            "Epoch 214/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9897\n",
            "Epoch 215/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0220 - accuracy: 0.9902\n",
            "Epoch 216/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0186 - accuracy: 0.9911\n",
            "Epoch 217/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0196 - accuracy: 0.9910\n",
            "Epoch 218/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9913\n",
            "Epoch 219/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0221 - accuracy: 0.9899\n",
            "Epoch 220/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0224 - accuracy: 0.9897\n",
            "Epoch 221/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9903\n",
            "Epoch 222/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9905\n",
            "Epoch 223/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0222 - accuracy: 0.9904\n",
            "Epoch 224/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9913\n",
            "Epoch 225/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 0.9906\n",
            "Epoch 226/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9910\n",
            "Epoch 227/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0204 - accuracy: 0.9907\n",
            "Epoch 228/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0208 - accuracy: 0.9888\n",
            "Epoch 229/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9901\n",
            "Epoch 230/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9909\n",
            "Epoch 231/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9908\n",
            "Epoch 232/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0212 - accuracy: 0.9900\n",
            "Epoch 233/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0222 - accuracy: 0.9902\n",
            "Epoch 234/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9901\n",
            "Epoch 235/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0212 - accuracy: 0.9899\n",
            "Epoch 236/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0188 - accuracy: 0.9912\n",
            "Epoch 237/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9915\n",
            "Epoch 238/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 0.9904\n",
            "Epoch 239/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0198 - accuracy: 0.9910\n",
            "Epoch 240/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9904\n",
            "Epoch 241/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9893\n",
            "Epoch 242/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9895\n",
            "Epoch 243/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9904\n",
            "Epoch 244/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9910\n",
            "Epoch 245/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9914\n",
            "Epoch 246/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9915\n",
            "Epoch 247/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9912\n",
            "Epoch 248/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 0.9908\n",
            "Epoch 249/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0219 - accuracy: 0.9904\n",
            "Epoch 250/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9904\n",
            "Epoch 251/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9912\n",
            "Epoch 252/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9906\n",
            "Epoch 253/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0188 - accuracy: 0.9912\n",
            "Epoch 254/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0225 - accuracy: 0.9907\n",
            "Epoch 255/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0178 - accuracy: 0.9919\n",
            "Epoch 256/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0204 - accuracy: 0.9914\n",
            "Epoch 257/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9913\n",
            "Epoch 258/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9910\n",
            "Epoch 259/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0224 - accuracy: 0.9904\n",
            "Epoch 260/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 0.9912\n",
            "Epoch 261/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0196 - accuracy: 0.9915\n",
            "Epoch 262/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9916\n",
            "Epoch 263/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9913\n",
            "Epoch 264/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0251 - accuracy: 0.9892\n",
            "Epoch 265/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9911\n",
            "Epoch 266/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0221 - accuracy: 0.9902\n",
            "Epoch 267/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9918\n",
            "Epoch 268/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0229 - accuracy: 0.9895\n",
            "Epoch 269/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0201 - accuracy: 0.9913\n",
            "Epoch 270/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0219 - accuracy: 0.9908\n",
            "Epoch 271/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9920\n",
            "Epoch 272/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9914\n",
            "Epoch 273/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0219 - accuracy: 0.9895\n",
            "Epoch 274/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9911\n",
            "Epoch 275/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9913\n",
            "Epoch 276/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9891\n",
            "Epoch 277/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 0.9904\n",
            "Epoch 278/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0216 - accuracy: 0.9904\n",
            "Epoch 279/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9911\n",
            "Epoch 280/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0204 - accuracy: 0.9905\n",
            "Epoch 281/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0196 - accuracy: 0.9914\n",
            "Epoch 282/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9919\n",
            "Epoch 283/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9907\n",
            "Epoch 284/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0173 - accuracy: 0.9927\n",
            "Epoch 285/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0182 - accuracy: 0.9917\n",
            "Epoch 286/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9909\n",
            "Epoch 287/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0185 - accuracy: 0.9918\n",
            "Epoch 288/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0204 - accuracy: 0.9908\n",
            "Epoch 289/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9912\n",
            "Epoch 290/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9916\n",
            "Epoch 291/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0184 - accuracy: 0.9914\n",
            "Epoch 292/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9914\n",
            "Epoch 293/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0173 - accuracy: 0.9916\n",
            "Epoch 294/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0213 - accuracy: 0.9906\n",
            "Epoch 295/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0206 - accuracy: 0.9899\n",
            "Epoch 296/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0200 - accuracy: 0.9908\n",
            "Epoch 297/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 0.9909\n",
            "Epoch 298/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9902\n",
            "Epoch 299/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9902\n",
            "Epoch 300/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0213 - accuracy: 0.9908\n",
            "Epoch 301/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9909\n",
            "Epoch 302/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0212 - accuracy: 0.9908\n",
            "Epoch 303/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0198 - accuracy: 0.9910\n",
            "Epoch 304/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0192 - accuracy: 0.9907\n",
            "Epoch 305/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9912\n",
            "Epoch 306/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0192 - accuracy: 0.9910\n",
            "Epoch 307/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9923\n",
            "Epoch 308/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9916\n",
            "Epoch 309/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9910\n",
            "Epoch 310/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0207 - accuracy: 0.9914\n",
            "Epoch 311/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9913\n",
            "Epoch 312/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9917\n",
            "Epoch 313/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9908\n",
            "Epoch 314/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0189 - accuracy: 0.9913\n",
            "Epoch 315/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9916\n",
            "Epoch 316/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0178 - accuracy: 0.9921\n",
            "Epoch 317/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0201 - accuracy: 0.9909\n",
            "Epoch 318/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0189 - accuracy: 0.9913\n",
            "Epoch 319/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0170 - accuracy: 0.9918\n",
            "Epoch 320/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9915\n",
            "Epoch 321/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0189 - accuracy: 0.9911\n",
            "Epoch 322/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0192 - accuracy: 0.9914\n",
            "Epoch 323/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9920\n",
            "Epoch 324/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0208 - accuracy: 0.9912\n",
            "Epoch 325/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9919\n",
            "Epoch 326/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0220 - accuracy: 0.9890\n",
            "Epoch 327/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9907\n",
            "Epoch 328/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0168 - accuracy: 0.9922\n",
            "Epoch 329/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0201 - accuracy: 0.9909\n",
            "Epoch 330/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9921\n",
            "Epoch 331/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9912\n",
            "Epoch 332/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0173 - accuracy: 0.9923\n",
            "Epoch 333/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9909\n",
            "Epoch 334/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9914\n",
            "Epoch 335/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0183 - accuracy: 0.9912\n",
            "Epoch 336/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9915\n",
            "Epoch 337/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0183 - accuracy: 0.9912\n",
            "Epoch 338/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9913\n",
            "Epoch 339/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9915\n",
            "Epoch 340/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9913\n",
            "Epoch 341/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9910\n",
            "Epoch 342/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0212 - accuracy: 0.9905\n",
            "Epoch 343/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0182 - accuracy: 0.9918\n",
            "Epoch 344/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0188 - accuracy: 0.9916\n",
            "Epoch 345/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0182 - accuracy: 0.9917\n",
            "Epoch 346/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0171 - accuracy: 0.9916\n",
            "Epoch 347/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0185 - accuracy: 0.9920\n",
            "Epoch 348/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0173 - accuracy: 0.9917\n",
            "Epoch 349/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0178 - accuracy: 0.9915\n",
            "Epoch 350/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0186 - accuracy: 0.9914\n",
            "Epoch 351/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9916\n",
            "Epoch 352/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9919\n",
            "Epoch 353/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0213 - accuracy: 0.9909\n",
            "Epoch 354/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0173 - accuracy: 0.9919\n",
            "Epoch 355/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0184 - accuracy: 0.9919\n",
            "Epoch 356/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0169 - accuracy: 0.9924\n",
            "Epoch 357/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9905\n",
            "Epoch 358/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9909\n",
            "Epoch 359/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9919\n",
            "Epoch 360/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0169 - accuracy: 0.9917\n",
            "Epoch 361/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0226 - accuracy: 0.9909\n",
            "Epoch 362/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9908\n",
            "Epoch 363/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9920\n",
            "Epoch 364/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0178 - accuracy: 0.9921\n",
            "Epoch 365/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9920\n",
            "Epoch 366/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0186 - accuracy: 0.9917\n",
            "Epoch 367/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0168 - accuracy: 0.9922\n",
            "Epoch 368/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0179 - accuracy: 0.9920\n",
            "Epoch 369/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0178 - accuracy: 0.9915\n",
            "Epoch 370/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0182 - accuracy: 0.9919\n",
            "Epoch 371/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0196 - accuracy: 0.9912\n",
            "Epoch 372/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0199 - accuracy: 0.9914\n",
            "Epoch 373/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0193 - accuracy: 0.9912\n",
            "Epoch 374/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0192 - accuracy: 0.9911\n",
            "Epoch 375/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0191 - accuracy: 0.9919\n",
            "Epoch 376/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0184 - accuracy: 0.9920\n",
            "Epoch 377/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0182 - accuracy: 0.9917\n",
            "Epoch 378/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0201 - accuracy: 0.9912\n",
            "Epoch 379/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0196 - accuracy: 0.9914\n",
            "Epoch 380/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0196 - accuracy: 0.9907\n",
            "Epoch 381/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0169 - accuracy: 0.9915\n",
            "Epoch 382/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9914\n",
            "Epoch 383/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9912\n",
            "Epoch 384/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9914\n",
            "Epoch 385/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0205 - accuracy: 0.9902\n",
            "Epoch 386/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0173 - accuracy: 0.9919\n",
            "Epoch 387/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9914\n",
            "Epoch 388/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0159 - accuracy: 0.9928\n",
            "Epoch 389/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9911\n",
            "Epoch 390/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9919\n",
            "Epoch 391/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9916\n",
            "Epoch 392/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0218 - accuracy: 0.9905\n",
            "Epoch 393/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0183 - accuracy: 0.9922\n",
            "Epoch 394/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0184 - accuracy: 0.9917\n",
            "Epoch 395/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0186 - accuracy: 0.9915\n",
            "Epoch 396/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0182 - accuracy: 0.9921\n",
            "Epoch 397/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0186 - accuracy: 0.9916\n",
            "Epoch 398/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9919\n",
            "Epoch 399/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9914\n",
            "Epoch 400/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9914\n",
            "Epoch 401/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0186 - accuracy: 0.9921\n",
            "Epoch 402/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9924\n",
            "Epoch 403/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0161 - accuracy: 0.9924\n",
            "Epoch 404/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0168 - accuracy: 0.9926\n",
            "Epoch 405/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0188 - accuracy: 0.9915\n",
            "Epoch 406/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0165 - accuracy: 0.9923\n",
            "Epoch 407/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9916\n",
            "Epoch 408/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0169 - accuracy: 0.9922\n",
            "Epoch 409/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9921\n",
            "Epoch 410/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0167 - accuracy: 0.9927\n",
            "Epoch 411/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0172 - accuracy: 0.9921\n",
            "Epoch 412/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0157 - accuracy: 0.9927\n",
            "Epoch 413/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0166 - accuracy: 0.9927\n",
            "Epoch 414/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9922\n",
            "Epoch 415/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0206 - accuracy: 0.9913\n",
            "Epoch 416/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9909\n",
            "Epoch 417/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9918\n",
            "Epoch 418/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0167 - accuracy: 0.9923\n",
            "Epoch 419/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0172 - accuracy: 0.9929\n",
            "Epoch 420/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9927\n",
            "Epoch 421/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0192 - accuracy: 0.9920\n",
            "Epoch 422/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9921\n",
            "Epoch 423/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0208 - accuracy: 0.9913\n",
            "Epoch 424/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0168 - accuracy: 0.9925\n",
            "Epoch 425/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0194 - accuracy: 0.9920\n",
            "Epoch 426/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0172 - accuracy: 0.9925\n",
            "Epoch 427/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0184 - accuracy: 0.9921\n",
            "Epoch 428/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0201 - accuracy: 0.9918\n",
            "Epoch 429/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9925\n",
            "Epoch 430/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9925\n",
            "Epoch 431/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0201 - accuracy: 0.9914\n",
            "Epoch 432/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9917\n",
            "Epoch 433/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0184 - accuracy: 0.9913\n",
            "Epoch 434/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0161 - accuracy: 0.9926\n",
            "Epoch 435/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0170 - accuracy: 0.9921\n",
            "Epoch 436/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9922\n",
            "Epoch 437/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9919\n",
            "Epoch 438/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0171 - accuracy: 0.9921\n",
            "Epoch 439/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0177 - accuracy: 0.9920\n",
            "Epoch 440/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9920\n",
            "Epoch 441/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0198 - accuracy: 0.9908\n",
            "Epoch 442/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0191 - accuracy: 0.9916\n",
            "Epoch 443/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9920\n",
            "Epoch 444/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0173 - accuracy: 0.9924\n",
            "Epoch 445/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9924\n",
            "Epoch 446/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0161 - accuracy: 0.9926\n",
            "Epoch 447/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0185 - accuracy: 0.9920\n",
            "Epoch 448/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9906\n",
            "Epoch 449/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9909\n",
            "Epoch 450/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0188 - accuracy: 0.9918\n",
            "Epoch 451/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0184 - accuracy: 0.9927\n",
            "Epoch 452/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0166 - accuracy: 0.9923\n",
            "Epoch 453/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0173 - accuracy: 0.9918\n",
            "Epoch 454/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0161 - accuracy: 0.9923\n",
            "Epoch 455/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0177 - accuracy: 0.9921\n",
            "Epoch 456/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0162 - accuracy: 0.9923\n",
            "Epoch 457/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0172 - accuracy: 0.9926\n",
            "Epoch 458/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0185 - accuracy: 0.9925\n",
            "Epoch 459/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0194 - accuracy: 0.9922\n",
            "Epoch 460/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0171 - accuracy: 0.9929\n",
            "Epoch 461/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0208 - accuracy: 0.9915\n",
            "Epoch 462/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0204 - accuracy: 0.9917\n",
            "Epoch 463/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 0.9910\n",
            "Epoch 464/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0171 - accuracy: 0.9924\n",
            "Epoch 465/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0186 - accuracy: 0.9925\n",
            "Epoch 466/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0179 - accuracy: 0.9924\n",
            "Epoch 467/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0173 - accuracy: 0.9924\n",
            "Epoch 468/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0189 - accuracy: 0.9921\n",
            "Epoch 469/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9920\n",
            "Epoch 470/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0165 - accuracy: 0.9920\n",
            "Epoch 471/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9923\n",
            "Epoch 472/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9921\n",
            "Epoch 473/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0178 - accuracy: 0.9924\n",
            "Epoch 474/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0171 - accuracy: 0.9926\n",
            "Epoch 475/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9925\n",
            "Epoch 476/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9920\n",
            "Epoch 477/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0158 - accuracy: 0.9927\n",
            "Epoch 478/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0216 - accuracy: 0.9917\n",
            "Epoch 479/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9926\n",
            "Epoch 480/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9913\n",
            "Epoch 481/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9923\n",
            "Epoch 482/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0184 - accuracy: 0.9926\n",
            "Epoch 483/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0157 - accuracy: 0.9931\n",
            "Epoch 484/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9921\n",
            "Epoch 485/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9915\n",
            "Epoch 486/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0182 - accuracy: 0.9915\n",
            "Epoch 487/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0196 - accuracy: 0.9912\n",
            "Epoch 488/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 0.9918\n",
            "Epoch 489/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9923\n",
            "Epoch 490/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9923\n",
            "Epoch 491/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0154 - accuracy: 0.9927\n",
            "Epoch 492/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 0.9921\n",
            "Epoch 493/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0159 - accuracy: 0.9926\n",
            "Epoch 494/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0159 - accuracy: 0.9931\n",
            "Epoch 495/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0171 - accuracy: 0.9923\n",
            "Epoch 496/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0158 - accuracy: 0.9936\n",
            "Epoch 497/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0173 - accuracy: 0.9929\n",
            "Epoch 498/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0180 - accuracy: 0.9916\n",
            "Epoch 499/500\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0181 - accuracy: 0.9910\n",
            "Epoch 500/500\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0168 - accuracy: 0.9923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "163/163 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.9360\n",
            "loss_and_metrics : [0.27927184104919434, 0.9360431432723999]\n",
            "0:09:53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z7mZSj0yaFY7",
        "outputId": "a13fc836-1972-45ba-8b90-08d81bd1249e"
      },
      "source": [
        "# 만들어봥\n",
        "\n",
        "start = time.time()\n",
        "# 라이브러리\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='normal',activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(128, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "hist = model.fit(X3_train, rorl_train, epochs=20, batch_size=64)\n",
        "\n",
        "    \n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(X3_test, rorl_test, batch_size=64)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.1783 - accuracy: 0.9277\n",
            "Epoch 2/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0778 - accuracy: 0.9762\n",
            "Epoch 3/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0615 - accuracy: 0.9798\n",
            "Epoch 4/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0579 - accuracy: 0.9794\n",
            "Epoch 5/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0561 - accuracy: 0.9803\n",
            "Epoch 6/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0534 - accuracy: 0.9813\n",
            "Epoch 7/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0451 - accuracy: 0.9844\n",
            "Epoch 8/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0432 - accuracy: 0.9838\n",
            "Epoch 9/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0435 - accuracy: 0.9834\n",
            "Epoch 10/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0393 - accuracy: 0.9856\n",
            "Epoch 11/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0388 - accuracy: 0.9854\n",
            "Epoch 12/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0417 - accuracy: 0.9840\n",
            "Epoch 13/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0395 - accuracy: 0.9854\n",
            "Epoch 14/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0380 - accuracy: 0.9856\n",
            "Epoch 15/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0381 - accuracy: 0.9853\n",
            "Epoch 16/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0347 - accuracy: 0.9869\n",
            "Epoch 17/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0354 - accuracy: 0.9862\n",
            "Epoch 18/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0316 - accuracy: 0.9878\n",
            "Epoch 19/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0339 - accuracy: 0.9866\n",
            "Epoch 20/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0332 - accuracy: 0.9876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "163/163 [==============================] - 0s 2ms/step - loss: 0.4104 - accuracy: 0.9190\n",
            "loss_and_metrics : [0.4104137122631073, 0.9189944267272949]\n",
            "0:00:21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pzkos3fKT038",
        "outputId": "6df33221-6eef-4f36-8261-09da4b1b3511"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_55 (Dense)             (None, 16)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_67 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dropout_68 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_69 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dropout_70 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_71 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_72 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_73 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 22,913\n",
            "Trainable params: 22,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxbIx-tgYYSy"
      },
      "source": [
        "## 이 모델들은 그냥 ㅎㅎ :)\n",
        "\n",
        "\n",
        "*   LSTM\n",
        "*   RNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHdgSsnYSUhW"
      },
      "source": [
        "LSTM 이당"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj58oQ0CnWOO"
      },
      "source": [
        "# 데이터 변환\n",
        "# LTSM : 입력을 (샘플 수, 타입스텝, 특성 수)로 형태 변환\n",
        "x_train = np.array(X3_train).reshape((X3_train.shape[0], X3_train.shape[1], 1))\n",
        "y_train = np.array(rorl_train).reshape((rorl_train.shape[0], 1))\n",
        "x_test = np.array(X3_test).reshape((X3_test.shape[0], X3_test.shape[1],1))\n",
        "y_test = np.array(rorl_test).reshape((rorl_test.shape[0], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gPDwrEat0Ec",
        "outputId": "c38c08dc-37cd-4043-a669-1d9f03470cf2"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23360, 65, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klmT_-AHt2RD",
        "outputId": "22fb4f63-fef4-4c14-b167-e734bd153c2a"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10382, 65, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "suyHFBwVnWOc",
        "outputId": "7abccc52-dc16-4b95-8649-ecc093c68e27"
      },
      "source": [
        "start = time.time()\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Activation, Dropout, LSTM\n",
        "\n",
        "# RNN 모델 구성\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape = (65,1), return_sequences = True ,activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(64, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(128, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(256, return_sequences = True ,activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(128, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(64, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)  # learning rate는 0.001\n",
        "model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "hist = model.fit(x_train, y_train, epochs=30, batch_size=512 )\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.6811 - accuracy: 0.5661\n",
            "Epoch 2/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.6271 - accuracy: 0.6074\n",
            "Epoch 3/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.5390 - accuracy: 0.6661\n",
            "Epoch 4/30\n",
            "46/46 [==============================] - 9s 193ms/step - loss: 0.5159 - accuracy: 0.6851\n",
            "Epoch 5/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.4646 - accuracy: 0.7147\n",
            "Epoch 6/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.4560 - accuracy: 0.7183\n",
            "Epoch 7/30\n",
            "46/46 [==============================] - 9s 191ms/step - loss: 0.4280 - accuracy: 0.7322\n",
            "Epoch 8/30\n",
            "46/46 [==============================] - 9s 193ms/step - loss: 0.4188 - accuracy: 0.7377\n",
            "Epoch 9/30\n",
            "46/46 [==============================] - 9s 191ms/step - loss: 0.4112 - accuracy: 0.7408\n",
            "Epoch 10/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.4049 - accuracy: 0.7448\n",
            "Epoch 11/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.4152 - accuracy: 0.7394\n",
            "Epoch 12/30\n",
            "46/46 [==============================] - 9s 191ms/step - loss: 0.3962 - accuracy: 0.7490\n",
            "Epoch 13/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.4019 - accuracy: 0.7458\n",
            "Epoch 14/30\n",
            "46/46 [==============================] - 9s 191ms/step - loss: 0.3875 - accuracy: 0.7541\n",
            "Epoch 15/30\n",
            "46/46 [==============================] - 9s 191ms/step - loss: 0.3796 - accuracy: 0.7586\n",
            "Epoch 16/30\n",
            "46/46 [==============================] - 9s 191ms/step - loss: 0.3815 - accuracy: 0.7583\n",
            "Epoch 17/30\n",
            "46/46 [==============================] - 9s 193ms/step - loss: 0.3774 - accuracy: 0.7610\n",
            "Epoch 18/30\n",
            "46/46 [==============================] - 9s 191ms/step - loss: 0.3799 - accuracy: 0.7592\n",
            "Epoch 19/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.3751 - accuracy: 0.7627\n",
            "Epoch 20/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.3690 - accuracy: 0.7663\n",
            "Epoch 21/30\n",
            "46/46 [==============================] - 9s 193ms/step - loss: 0.3661 - accuracy: 0.7677\n",
            "Epoch 22/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.3588 - accuracy: 0.7730\n",
            "Epoch 23/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.3570 - accuracy: 0.7736\n",
            "Epoch 24/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.3561 - accuracy: 0.7752\n",
            "Epoch 25/30\n",
            "46/46 [==============================] - 9s 191ms/step - loss: 0.3545 - accuracy: 0.7760\n",
            "Epoch 26/30\n",
            "46/46 [==============================] - 9s 191ms/step - loss: 0.3478 - accuracy: 0.7794\n",
            "Epoch 27/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.3445 - accuracy: 0.7820\n",
            "Epoch 28/30\n",
            "46/46 [==============================] - 9s 191ms/step - loss: 0.3526 - accuracy: 0.7795\n",
            "Epoch 29/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.3419 - accuracy: 0.7849\n",
            "Epoch 30/30\n",
            "46/46 [==============================] - 9s 192ms/step - loss: 0.3381 - accuracy: 0.7866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAEWCAYAAAAtuzN2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwU9Z3/8ddn7huG4VBA5BAVURhEkPxQMYcbj6zxSDyiuTaRzSa68ZGN0RxujMbVqEkMiVExITHH6hpNNhqNJhqJyUYSEFAOUUSQGQSBYRjmnumZz++P6hmaYZiL7unqnvfz8ahHV1VXdX9qWutNVX3rW+buiIiIhEVGsgsQERGJpWASEZFQUTCJiEioKJhERCRUFEwiIhIqCiYREQkVBZOIiISKgknkEMxsi5k1mlmtme01s7+Z2WfMLCP6/k/NzM1sbsw6x5iZx0wvNbMmMzsqZt77zGzLoG6MSApRMIn07J/dvRg4GrgduB74ccz7e4Bv9vIZ9cCNiSlPJP0omET6wN1r3P1x4FLg42Z2YvStB4EZZragh9UXAZeb2ZRE1ymSDhRMIv3g7v8AKoHTo7MagP8Cbu1htW3AA8A3EludSHpQMIn039vAiJjp+4EJZnZOD+vcBvyzmU1PaGUiaUDBJNJ/4wiuLQHg7s3ALdGhW+6+C/gBcHPCqxNJcQomkX4wszkEwfTXLm/9BBgOXNTD6ncC7wZmJ6Y6kfSgYBLpAzMrMbMPAA8Dv3D3NbHvu3sE+DpBq71uufte4NvAlxJZq0iqUzCJ9OwJM6sFKoCvAt8BPnmIZR8Ctvfyed8D2uJXnkj6MT0oUEREwkRHTCIiEioJCyYzW2JmO81s7SHeNzNbZGZvmNkrZnZyomoREZGehWmfncgjpp8CZ/fw/jnA1OiwELg3gbWIiEjPfkpI9tkJCyZ3f4GYez268UHgZx5YBgw3syMTVY+IiBxamPbZWYn40D4aR9DSqUNldN5BrZrMbCFBQgPMLigoSHx1IiJppKGhwYGVMbMWu/vifnxEn/fZhyuZwdRn0T/eYoDCwkKvr69PckUiIqnFzBrd/ZRk19EXyWyVtw04KmZ6fHSeiIiEz6Dts5MZTI8DH4u29JgH1Lh73A8JRUQkLgZtn52wU3lm9hBwJjDSzCoJumvJBnD3+4CngHOBNwgeHXCou+lFRCTBwrTPTrmeH7q7xtTa2kplZSVNTU1Jqir15eXlMX78eLKzs5NdiogkgJk1uHthsuvoi5Ro/NCbyspKiouLmThxImaW7HJSjrtTVVVFZWUlkyZNSnY5IjLEpUWXRE1NTZSVlSmUBsjMKCsr0xGniIRCWgQToFA6TPr7iUhYpE0wiYhIelAwxcHevXv54Q9/OKB1zz33XPbu3dvn5W+66SbuuuuuAX2XiEgqUDDFQU/BFIlEelz3qaeeYvjw4YkoS0QkJSmY4uCGG25g06ZNlJeXc91117F06VJOP/10zj//fE444QQALrjgAmbPns306dNZvHh/91QTJ05k9+7dbNmyhWnTpnHVVVcxffp0/umf/onGxsYev3f16tXMmzePGTNmcOGFF1JdXQ3AokWLOOGEE5gxYwaXXXYZAH/+858pLy+nvLycWbNmUVtbm6C/hojI4UmL5uKxNm68lrq61XH9zKKicqZOvfuQ799+++2sXbuW1auD7126dCkrV65k7dq1nc2vlyxZwogRI2hsbGTOnDlcfPHFlJWVdal9Iw899BAPPPAAl1xyCY899hhXXnnlIb/3Yx/7GN///vdZsGAB//mf/8k3vvEN7r77bm6//XY2b95Mbm5u52nCu+66i3vuuYf58+dTV1dHXl7e4f5ZREQSQkdMCTJ37twD7glatGgRM2fOZN68eVRUVLBx48aD1pk0aRLl5eUAzJ49my1bthzy82tqati7dy8LFiwA4OMf/zgvvPACADNmzOCKK67gF7/4BVlZwb895s+fzxe+8AUWLVrE3r17O+eLiIRN2u2dejqyGUyFhftvsF66dCnPPvssL774IgUFBZx55pnd3jOUm5vbOZ6ZmdnrqbxDefLJJ3nhhRd44oknuPXWW1mzZg033HAD5513Hk899RTz58/nmWee4fjjjx/Q54uIJJKOmOKguLi4x2s2NTU1lJaWUlBQwIYNG1i2bNlhf+ewYcMoLS3lL3/5CwA///nPWbBgAe3t7VRUVPDud7+bb33rW9TU1FBXV8emTZs46aSTuP7665kzZw4bNmw47BpERBIh7Y6YkqGsrIz58+dz4okncs4553Deeecd8P7ZZ5/Nfffdx7Rp0zjuuOOYN29eXL73wQcf5DOf+QwNDQ1MnjyZn/zkJ7S1tXHllVdSU1ODu/Pv//7vDB8+nBtvvJHnn3+ejIwMpk+fzjnnnBOXGkRE4i0tOnF99dVXmTZtWpIqSh/6O4qkr1TqxFWn8kREJFQUTCIiEippE0ypdkoybPT3E5GwSItgysvLo6qqSjvXAep4HpNuuhWRMEiLVnnjx4+nsrKSXbt2JbuUlNXxBFsRkWRLi1Z5IiLSM7XKExERGSAFk4iIhIqCSUREQkXBJCIioaJgEhGRUFEwiYhIqCiYREQkVBRMIiISKgomEREJFQWTiIiEioJJRERCRcEkIiKhomASEZFQUTCJiEioKJhERCRUFEwiIhIqCQ0mMzvbzF4zszfM7IZu3p9gZs+b2Soze8XMzk1kPSIi0r0w7a8T9gRbM8sEXgfOAiqB5cDl7r4+ZpnFwCp3v9fMTgCecveJPX2unmArItJ/PT3BNlH764FK5BHTXOANd3/T3VuAh4EPdlnGgZLo+DDg7QTWIyIi3QvV/jorUR8MjAMqYqYrgVO7LHMT8AczuwYoBN7X3QeZ2UJgIUBOTk7cCxURGQKyzGxFzPRid18cHY/b/joekt344XLgp+4+HjgX+LmZHVSTuy9291Pc/ZSsrERmqYhIOLkHw2GIdOxHo8Pi3lc5QJ/21/GQyL38NuComOnx0XmxPgWcDeDuL5pZHjAS2JnAukREBl1bG2zfDpWVUFERvFZWwq5d0NAAjY29D/feCwsXJqS8UO2vExlMy4GpZjaJYAMvAz7SZZmtwHuBn5rZNCAP2JXAmkRkiGpvh5oa2LNn/1BdHbzW1EBtLezbFwyx47HTkQgUFUFxcc9DYSHs3r0/gCoqglBqazuwpvx8GDMGCgqC8fz8YP3Ro/dPxw7l5Qn784Rqf52wVnkA0eaEdwOZwBJ3v9XMbgZWuPvj0ZYdDwBFBBfWvuTuf+jpM9UqTyQ1tLZCU1OwM44dIpGDpyORYNmmpuDIoGO8u6GlJVi+tTUYDjXe2HhgAFVX93wqLCsLSkr2D8XFB49nZUFdXRBUPQ3t7UGQHHVUMIwff/Dr+PFQWgpmg/N79NQqL/p+3PfXA641kcGUCAomkeRraoJt2/afjoo9NdUxvPNOYr47KysYsrODoWO862teHowY0ftQWgrDhkFubnxCwh2am+P3efHSWzCFiVoSiAwRjY1QVXXgsHt38LpnT3Cdo6kp2Kn29FpTE6zXVWnp/iOBk0+GceOCU1qZmfuHrKzup7OygiOMvLxgiB2PnZeTAxnJbrLVC7OgXhk4BZPIIGlvD04DxV67qKnZP97cfOD1hNjrDrHTeXlQX3/gtZKOoSNkYqc7hsbGQ9dWWBgMeXnBv/Q7wiA3N/je0tL984qL9wdQ7FCYEv8Wl1SgYBKJ0d4e7MR37AguVu/Y0f2wd2/wL2Oz4F/w3b12DA0N+y+gJ/rMeVZWcHqqrCx4nTABZs0KpnsacnMTW5dIfyiYZEhqaIBXX4W1a/cP69bB228f3HIKgqOGI4+EI46AE06A4cOD+R33lrS3d//qHqxbUhJcx4i9uN51Xk7O/ov/HUNsM+LY8aKi7q+XFBWF67qGyEAomCQltLYGwfHSS7ByJbz8cjC/tHT/0HEhu+vQ3h6sGxtCmzbtP3rJzQ3CZsECmDgxaL57xBEHDkVFSdt0kSFHrfJkQNrbg9NTu3YFw+7d+8e7zquqCq5LxDaTjR3ven2ipSUIj5de2h9Er7wSXIOB4LPKy4PTVh3NgKurg3p6kpEBxx4LJ5544DBlSvBZIukslVrlKZik8470rVuDZr67dwfXUKqrD/1aUxOEU3cKCmDUqGAYOTK4hrFv3/4mxT216MrKCo5uWlqC+cOGBS28Zs8OhpNPhmOO6b5lViSyv77YwT04IjruOLWWkqFLwZRACqb+a2mB9euD4KmoOPh127bur6vk5QWBMXx496+lpfsDqCOERo0KgqknjY3BtZzYe186xpubYebM/UE0ebKumYjEg4IpgRRMfbNtG/z+9/DUU/DHPwbNlDvk5ARHJxMm7L8zPXZ81KggfHR0IZI+UimYdGY9TbS2wosvBkH0+98H12QgCJorroD3vAcmTQoCaNSo8N+kKCJDl4IpBbkHN1i+8w688EIQRn/4Q3AdJysLTj8d7rgDzj03uLaiU2EikkoUTCGze3dwxNPRmq1r1zEd4x2NAyDo+uWSS4Igeu97g3tiRERSla4xhUR7OyxZAtdfH3QlA0EfYrF353e0cIsdP+UUOOkkHRWJSM90jUn65eWX4d/+LbhGdMYZcOedwf02w4YpcERk6NEl8CSqrYUvfCFoFr1xIzz4ICxdCnPnBq3iFEoiMhTpiCkJ3OGxx+Daa4Nm3QsXwm23BV3qiIgMdTpiGmSbNgWNFD784eA60Ysvwv33K5RERDoomAZJczPcckvQN9tf/wrf/S6sWAHz5iW7MhGRcBkyp/LcnebmreTlHT2o37tjB/z0p7B4MWzeHBwpffe7QRNvERE52JA5Ytq69TaWL59JXd2ahH9XJAJPPgkXXhh0/fPlLwc9MDz9NDzyiEJJRKQnQ+Y+pqamraxc+S7AOPnkF8nLOyrutW3ZEtyLtGRJ0Khh9Gj4xCfgU58Kmn+LiCRLKt3HNGSCCaCubg2rVp1Gbu54Zs36C9nZh9/ioLkZHn8cHngAnn02mHf22fDpT8MHPhB0mCoikmwKpgQ63J4fqquX8sor76ekZC4zZvyBzMz8AX/WE0/Av/xL0EXQhAnBkdEnPhGMi4iEiYIpgeLRJdHOnY+wfv1ljBx5AdOn/wqzzH5/xurVMH9+8PC5226D970v6EJIRCSMFEwJFK++8iorv8cbb1zL2LGfZerUH2D96GZh506YMyfo3275cjjiiMMuR0QkoVIpmIZMc/Guxo//PM3N26iouJPc3LEcffRX+7ReSwtcfHEQTn/9q0JJRCTehmwwAUyefDvNzW+zefPXyMkZy5FHfrLH5d3hc58LAunhh4M+7kREJL6GdDCZZXD88Utobd3Ja69dRU7OGMrKzj3k8j/4AfzoR/DVr8Kllw5ioSIiQ8iQvcYUKxKpZfXqBTQ0vEZ5+fOUlMw9aJlnnw2agX/gA/DrX+vR5CKSWlLpGpOCKaq5eQerVv0/2tpqmTXrbxQUTO187403gkdRjB0bdLpaXBz3rxcRSahUCib9uz8qN/cIZsx4GoBXXnk/LS3vALBvH5x/fvBspMcfVyiJiCSagilGQcGxnHTS72hp2cHrr3+Wtjb4yEfg9dfh0Udh8uRkVygikv4UTF2UlJzKhAk3sHv3r/niFyt58klYtAje/e5kVyYiMjQM6VZ5h3LUUf/BT37yDnffPZ5//Vfns5/VM85FRAZLQo+YzOxsM3vNzN4wsxsOscwlZrbezNaZ2X8nsp6+WrmykG9963vMnLmUG2/8bbLLERFJuDDtrxPWKs+CDuheB84CKoHlwOXuvj5mmanAI8B73L3azEa7+86ePjdRrfI6vPQSnHce5Oc79913GsOH72bOnLVkZGQn7DtFRBKtp1Z5idpfD1Qij5jmAm+4+5vu3gI8DHywyzJXAfe4ezVAojayr377WzjjDMjNhaeeMmbP/jKNja+zffsDySxLRCTRQrW/TmQwjQMqYqYro/NiHQsca2b/Z2bLzOzs7j7IzBaa2QozWxGJROJeqDt85zvBE2enT4e//x2mTYOysvMYNmwBW7bcRCRSG/fvFREZRFkd+9HosDDmvbjtr+Mh2a3ysoCpwJnA5cADZja860LuvtjdT3H3U7Ky4tteIxKBz34W/uM/4KKLYOnS/R2zmhlTptxJa+suKirujOv3iogMskjHfjQ6LO7n+n3aX8dDIoNpGxD7/PLx0XmxKoHH3b3V3TcTnOOcyiDZty/oYui+++D66+GRR6Cg4MBlSkrmMHr0ZVRUfJvm5rcHqzQRkcEUqv11IoNpOTDVzCaZWQ5wGfB4l2X+lyB9MbORBIeKbyawpk5vvRU86O+554LHot9++6H7v5s06VbcW9my5euDUZqIyGAL1f46YcHk7hHgauAZ4FXgEXdfZ2Y3m9n50cWeAarMbD3wPHCdu1clqqYOy5fDqadCRQX8/vfw6U/3vHx+/mTGjfsc27cvob5+XaLLExEZVGHbXw+5Tlwfeww++lEYMwaefBJOOKFv67W2VrFs2RSGDz+dk056YsDfLyKSDIPdiauZneTuawaybrIbPwwad7jzTvjQh2DmzKDlXV9DCSA7u4yjj/4KVVW/o7p6acLqFBFJEz80s3+Y2WfNbFh/VhwywXTbbfClL8Ell8Cf/gSjR/f/M8aNu4bc3KN4883rcG+Pf5EiImnC3U8HriBoVPGSmf23mZ3Vl3WHTDBdcQXceis89BDk5w/sMzIz85k06ZvU1q5g585H4lugiEiacfeNwNeA64EFwCIz22BmF/W03pC7xnS43NtZseJk2tr2MXfuq2Rk5CatFhGRvkrCNaYZwCeB84A/Aj9295VmNhZ40d2PPtS6Q+aIKV7MMpgy5U6amjazbdsPk12OiEhYfR9YCcx098+5+0oAd3+b4CjqkHTENEAvv/x+amuXc+qpm8jOLk12OSIiPdKj1YeAKVPuIBLZy9attye7FBGR0DGzqWb2aPQxGW92DH1ZV8E0QEVFMxkz5mNUVNzJunWXUVf3SrJLEhEJk58A9wIR4N3Az4Bf9GVFnco7DJFILVu3/hfbtt1DW1stZWX/zIQJX2HYsHnJLk1E5ABJaPzwkrvPNrM17n5S7Lze1tUR02HIyipm8uTbmDfvLSZOvJmamv9j1ap3sXr1e6mu/hOpFvoiInHUbGYZwEYzu9rMLgSK+rJin46YzOzzBIdltcCPgFnADe7+h4HXPDBhOmLqKhKpY/v2xVRU3EVLy3ZKSuYxYcJXKCv7AGaW7PJEZAhLwhHTHIJ+94YDtwAlwJ3uvqzXdfsYTC+7+0wzez/wr8CNwM/d/eTDqnwAwhxMHdramtix46dUVHyLpqYtFBbO4Oijv8KoUR8ieIKxiMjgGsxgij6q/Vvu/sWBrN/XU3kd/9w/lyCQ1sXMky4yM/MYN+4zzJ37Oscf/yDuLaxffxnr1l1Ke3v8n8ArIhIm7t4GnDbQ9ft6xPQTgsfsTgJmApnA0r5cxIq3VDhi6sq9nYqKb/Pmm19izJiPc/zxSwhOvYqIDI4knMq7lyA3fgV07rTd/de9rdvX55R/CigH3nT3BjMbQdDVhPSBWQYTJlxHe3sjW7Z8naysEo455nu67iQi6SwPqALeEzPPgbgF07uA1e5eb2ZXAicD3+tvlUPd0UffSCRSQ2Xld8jKGsakSbckuyQRkYRw9wEfvPQ1mO4FZprZTOA/CFrm/Yygt1jpIzNjypS7aGvbx1tvfZPMzGFMmDCga4MiIqEWvQR00LUid/+X3tbtazBF3N3N7IPAD9z9x2b2qX7WKQThdOyx9xGJ1PLmm9eRlVXC2LELk12WiEi8/S5mPA+4EHi7Lyv2NZhqzezLwEeB06M3TWX3q0TpZJbJtGk/o62tltdf/wyZmcWMGXN5sssSEYkbd38sdtrMHgL+2pd1+9o07FKgGfgXd98BjAfu7E+RcqCMjBymT3+UYcPOYMOGj7F79xPJLklEJJGmAn16dnif+8ozszHAnOjkP9x958BqOzyp2Fy8J5HIPl5++X3U1b3CjBlPUVr6nt5XEhHppyQ0F6/lwGtMO4Avdz2S6nbdPt7HdAnBEdJSghtrTweuc/dHB1Lw4Ui3YAJoba1i1aoFNDVtobz8OUpKTk12SSKSZlLpeUx97pIIOKvjKMnMRgHPuvvMBNd3kHQMJoDm5u2sWnUakUg15eVLKSqakeySRCSNJOGI6ULgT+5eE50eDpzp7v/b67p9DKbObsuj0xnAy7HzBku6BhNAY+NmVq06nUhkD0VF5RQWnkRR0QwKC0+isPAkPSlXRAYsCcG02t3Lu8xb5e6zelu3r63ynjazZ4CHotOXAk/1r0zpTX7+JMrLl7Jt2yLq6l5h165fsX374s73c3PHU1g4o0tgTVf3RiISRt3tmPqUOf1p/HAxMD86+Rd3/03faouvdD5i6srdaW7eRn39GurrX6GuLnhtaNiAeysARUUnM3XqIoYNm9/Lp4nIUJaEI6YlwF7gnuiszwEj3P0Tva6bag+zG0rBdCjt7S00NLzGvn1/Y8uWW2hp2cbo0ZczefId5OWNT3Z5IhJCSQimQoJHJL2PoHXeH4Fb3b3XHXiPwdRNc7/OtwB395IBVXwYFEwHikTq2Lr1dioq7sIskwkTvsxRR32RzMy8ZJcmIiGSdq3ywkTB1L3Gxs1s2vRFdu/+NXl5E5ky5duMHHmhejAXESApR0x/BD7s7nuj06XAw+7+/t7W1VXzNJGfP4kTT3yMmTOfJSOjkHXrLo7euLs22aWJyNA0siOUANy9mj72/KBgSjOlpe/llFNWc8wx36eubhUrVpSzceM1tLbuSXZpIjK0tJvZhI4JM5tI95eGDqJTeWmspWU3W7b8J2+/fT9m2RQVnURR0ayYYQaZmQXJLlNEBkESTuWdDSwG/sz+HoMWuvszva6rYEp/dXWvsGPHz6irW0Vd3SoikeroOxkUFBwXE1TlFBfPIju7LKn1ikj8JaPxg5mNBhYCq4B8YKe7v9DregqmoSW4N2ortbWrOoOqrm4Vzc2Vncvk5BxBQcHxBwz5+ceRlzchoTfzNjfvoK2tloKCqQn7DpGhKglHTJ8GPk/wNIrVwDzgRXfvtadqBZMAwWm/IKRW09DwKg0NG2hoeJVIpPPaJRkZ+eTnHxsTWMeSnz+FvLxJZGeP6lcLwLa2emprV7Jv39+prf0H+/b9nebmrQAUFc1m7NirGD36crKyBv2OBJG0lIRgWkPwRIpl7l5uZscD/+XuF/W6biKDKXqO8XtAJvAjd7/9EMtdDDwKzHH3FT19poJp8Lg7ra27oiH1WvQ1GJqaNhN7HTMjo5D8/Enk5U0mP38yeXkd45PIzZ1AU9Nb1Nb+nX37/s6+ff+gvn4t0AZAXt5EiotPpaTkVMwy2L59CfX1r5CRUcDo0Zdy5JFXUVIyT03fRQ5Db8EU7/21mS139zlmtho41d2bzWydu0/vtdZEBZOZZQKvA2cBlcBy4HJ3X99luWLgSSAHuFrBlBra2ppoatpEY+NmmprepKlpM42Nb9LU9CaNjZtpb+/+N8rKGk5x8VxKSk6Nvs4lJ+fAFqTuTm3tcrZvf4B33nmI9vZ6CgqmM3bsVYwZ81Gys0f0Wl97ezPNzZU0NW3FvY1hw07TTccypPUUTInYX5vZb4BPAtcC7wGqgWx3P7fXWhMYTO8Cbuq4mSr6aHbc/bYuy91N0FXFdcAXFUypr+NIa39YbSE3dzwlJXPJz5/ar+tUkUgtO3f+D9u3P0Bt7T8wy2XUqIs48shPkZVVSlPTVpqb34q+bu18bWnZccDnZGQUUFr6PsrKPkBZ2Xnk5o6N92aLhFovwZSQ/XXMeguAYcDT7t7S2/J97V18IMYBFTHTlcABT8Azs5OBo9z9STO77lAfZGYLCVp2kJOTk4BSJZ7MjJyc0eTkjD7shx5mZRUzduynGTv209TVvRI9ivoFO3c+dMByGRn55OZOIC9vAoWF55GXN6Fzur29mT17nmL37ieoqnocCDq/DULqnykuPlk9tMtQkGVmsUGy2N07Hl8Qt/11d9z9z/0qtD8Lx1P0mU7fAT7R27LRP95iCI6YEluZhFVR0QymTv0+kyffwZ49vwcyOgMoO7usx2tQZWXncMwxi2hoWE9V1e+oqvodb731Td5662Zyco5gxIjzKCv7ACUl88jJGaPrWZKOIu5+ykBW7M/+Oh4SGUzbgKNipsdH53UoBk4ElkZ3AkcAj5vZ+X09PJShKTMzn1Gjem3YcxAzo7BwOoWF05kw4XpaWnazZ8/TVFX9jl27HmXHjh8DkJVVSkHBNAoLT6Cg4ITO19zc8QosSVeh2l8n8hpTFsHFtPcSbOBy4CPuvu4Qyy9F15gkSdrbW9m370Xq6l6moWE99fXraWh4ldbWXZ3LZGYWUVAwrTOk2tubaG9voK2tgfb2+uhrQ5fXRjIzi8nJGU129ugeXzMzi4hEamlr2z8cPL2PtrZ6srJKyMkZS27ukeTkHElOzthejxplaOvlGlNC9tcDlbAjJnePmNnVwDMEzQ+XuPs6M7sZWOHujyfqu0X6KyMjm+HDz2D48DMOmN/SsouGhlejQRWEVXX1H2lp2U5GRj6ZmQVkZBREXwvJzCwgK6uUzMxxZGQUkJGRR1tbLa2tO2lo2EBNzQu0tlbRxy7DumWW3fmgyK7zc3KOICfnSHJzx0Zfx5GXNzHafH8iOTlH6HqaHCRs+2vdYCsyAO4+4KOT9vYIkUgVLS07aW3d2fna1lZHZmZx55CV1TFeEjNeTEZGNm1tDbS07KC5+W1aWrZ3DrHTzc3biUSqDvhusxzy8o7uDKrYITt7JFlZw8nKGkZGhhoZpRs9jymBFEwifdfW1kBT01aamrbQ1LQ5+rqlczr2VGWsjIw8MjOHdQZVMATj2dmjoj2AHEdBwXF9uq9Mkk/BlEAKJpH4aWurp6npLZqattDauoe2thoikb1EIjUxw97o/GBobd2Je6TzM7KzR5Kff1xnUAXD8eTlTSYjIzuJWyexFEwJpGASSa729ghNTZtpaHiNxsbXot1VBUNr6zsxS2aSlVVMRkZ+55CZmd/NdAEZGbmYZWOWTQAebm0AAA5iSURBVEZGdud499N50SO6/Oh496+ZmUVkZCTtjpjQUTAlkIJJJLxaW/d2hlVj40YikRra2xtpa2ukvX3/EEw3xIw34d4aHSLdNu7ov0zy8iaSnz+F/PxjokMwnpc3Oa5dVLk7LS07qK9fR0PDOhoaNpKXdzQlJe+iuHg2mZn5cfuugVIwJZCCSST9uTvubTFh1Up7eyvuLbS3N0eb6jce8NoRcB3h19paRWPjJhob36Cx8Q3a2moO+I7c3PHRkJpCTs5osrJGkJ1dGn0dQVZWaee8jIwCzCwaQO/Q0LCO+vr9Q0PD+pjnnEFmZjFtbbUA0Yd0llNS8i5KSt7FsGHvIjd3wqA37VcwJZCCSUT6y92JRPbEBNX+wGpqepOWll109HbfHbMcsrJKcW8lEtnTOT8rq5TCwukUFEynsPDEzhu4c3JG09LyDvv2LaOm5kX27XuR2trltLc3ApCTc2RnUBUVzYzezzaS7OyRZGTkJuRvoGBKIAWTiMSbu9PWVkcksofW1moikT1EItW0tu45YB5YtCeQjgA6os9HPu3trdTXv9IZVPv2vRh9fMyBMjOLOkOq6zBixLkUF88a0DYqmBJIwSQi6SI4LbiB1tYqWlt30dq6+5BDW1sdxx77AGPHfnpA35VKwaQmKyIiSZKTM4acnDF9WratrSnB1YSHgklEJAUMpQddqtMsEREJFQWTiIiEioJJRERCRcEkIiKhomASEZFQUTCJiEioKJhERCRUFEwiIhIqCiYREQkVBZOIiISKgklEREJFwSQiIqGiYBIRkVBRMImISKgomEREJFQUTCIiEioKJhERCRUFk4iIhIqCSUREQkXBJCIioaJgEhGRUFEwiYhIqCiYREQkVBRMIiISKgkNJjM728xeM7M3zOyGbt7/gpmtN7NXzOw5Mzs6kfWIiEj3wrS/TlgwmVkmcA9wDnACcLmZndBlsVXAKe4+A3gUuCNR9YiISPfCtr9O5BHTXOANd3/T3VuAh4EPxi7g7s+7e0N0chkwPoH1iIhI90K1v05kMI0DKmKmK6PzDuVTwO+7e8PMFprZCjNbEYlE4liiiMiQkdWxH40OC2Pei9v+Oh6yEvXB/WFmVwKnAAu6e9/dFwOLAQoLC30QSxMRSRcRdz/lcD+kt/11PCQymLYBR8VMj4/OO4CZvQ/4KrDA3ZsTWI+IiHQvVPvrRJ7KWw5MNbNJZpYDXAY8HruAmc0C7gfOd/edCaxFREQOLVT764QFk7tHgKuBZ4BXgUfcfZ2Z3Wxm50cXuxMoAn5lZqvN7PFDfJyIiCRI2PbX5p5al2wKCwu9vr4+2WWIiKQUM2tw98Jk19EX6vlBRERCRcEkIiKhomASEZFQUTCJiEioKJhERCRUFEwiIhIqCiYREQkVBZOIiISKgklEREJFwSQiIqGiYBIRkVBRMImISKgomEREJFQUTCIiEioKJhERCZVEPlp90LS2tlJZWUlTU1OyS0k5eXl5jB8/nuzs7GSXIiICpEkwVVZWUlxczMSJEzGzZJeTMtydqqoqKisrmTRpUrLLEREB0uRUXlNTE2VlZQqlfjIzysrKdKQpIqGSFsEEKJQGSH83EQmbtAkmERFJDwqmONi7dy8//OEPB7Tuueeey969e+NckYhI6lIwxUFPwRSJRHpc96mnnmL48OGJKEtEJCWlRau8WNdeC6tXx/czy8vh7rsP/f4NN9zApk2bKC8v56yzzuK8887jxhtvpLS0lA0bNvD6669zwQUXUFFRQVNTE5///OdZuHAhABMnTmTFihXU1dVxzjnncNppp/G3v/2NcePG8dvf/pb8/PwDvuuJJ57gm9/8Ji0tLZSVlfHLX/6SMWPGUFdXxzXXXMOKFSswM77+9a9z8cUX8/TTT/OVr3yFtrY2Ro4cyXPPPRffP46ISJylXTAlw+23387atWtZHU3EpUuXsnLlStauXdvZDHvJkiWMGDGCxsZG5syZw8UXX0xZWdkBn7Nx40YeeughHnjgAS655BIee+wxrrzyygOWOe2001i2bBlmxo9+9CPuuOMOvv3tb3PLLbcwbNgw1qxZA0B1dTW7du3iqquu4oUXXmDSpEns2bNnEP4aIiKHJ+2Cqacjm8E0d+7cA+4NWrRoEb/5zW8AqKioYOPGjQcF06RJkygvLwdg9uzZbNmy5aDPrays5NJLL2X79u20tLR0fsezzz7Lww8/3LlcaWkpTzzxBGeccUbnMiNGjIjrNoqIJIKuMSVIYWFh5/jSpUt59tlnefHFF3n55ZeZNWtWt/cO5ebmdo5nZmZ2e33qmmuu4eqrr2bNmjXcf//9ugdJRNKOgikOiouLqa2tPeT7NTU1lJaWUlBQwIYNG1i2bNmAv6umpoZx48YB8OCDD3bOP+uss7jnnns6p6urq5k3bx4vvPACmzdvBtCpPBFJCQqmOCgrK2P+/PmceOKJXHfddQe9f/bZZxOJRJg2bRo33HAD8+bNG/B33XTTTXz4wx9m9uzZjBw5snP+1772NaqrqznxxBOZOXMmzz//PKNGjWLx4sVcdNFFzJw5k0svvXTA3ysiMljM3ZNdQ78UFhZ6fX39AfNeffVVpk2blqSKUp/+fiLpz8wa3L2w9yWTT0dMIiISKgomEREJlbQJplQ7JRkW+ruJSNikRTDl5eVRVVWlnWw/dTyPKS8vL9mliIh0SosbbMePH09lZSW7du1Kdikpp+MJtiIiYZEWrfJERKRnapUXZWZnm9lrZvaGmd3Qzfu5ZvY/0ff/bmYTE1mPiIh0L0z764QFk5llAvcA5wAnAJeb2QldFvsUUO3uxwDfBb6VqHpERKR7YdtfJ/KIaS7whru/6e4twMPAB7ss80Ggo1+dR4H3mp71LSIy2EK1v05k44dxQEXMdCVw6qGWcfeImdUAZcDu2IXMbCGwMDrpZtY4wJqygJ6f3Jd60m2b0m17IP22Kd22B9Jvm7rbnnwzWxEzvdjdF0fH47a/joeUaJUX/eMt7nXBXpjZCnc/JQ4lhUa6bVO6bQ+k3zal2/ZA+m1Tqm9PIk/lbQOOipkeH53X7TJmlgUMA6oSWJOIiBwsVPvrRAbTcmCqmU0ysxzgMuDxLss8Dnw8Ov4h4E+eau3XRURSX6j21wk7lRc9B3k18AyQCSxx93VmdjOwwt0fB34M/NzM3gD2EPwxEumwTweGULptU7ptD6TfNqXb9kD6bVO/tids++uUu8FWRETSW1r0lSciIulDwSQiIqEyZIKpt+42Uo2ZbTGzNWa2usu9CSnDzJaY2U4zWxszb4SZ/dHMNkZfS5NZY38cYntuMrNt0d9ptZmdm8wa+8vMjjKz581svZmtM7PPR+en5O/Uw/ak7O9kZnlm9g8zezm6Td+Izp8U7TrojWhXQjnJrrWvhsQ1pmh3G68DZxHcOLYcuNzd1ye1sMNgZluAU9w97je3DRYzOwOoA37m7idG590B7HH326P/gCh19+uTWWdfHWJ7bgLq3P2uZNY2UGZ2JHCku680s2LgJeAC4BOk4O/Uw/ZcQor+TtHeFwrdvc7MsoG/Ap8HvgD82t0fNrP7gJfd/d5k1tpXQ+WIqS/dbcggc/cXCFr3xIrt9uRBgp1GSjjE9qQ0d9/u7iuj47XAqwQ9AKTk79TD9qQsD9RFJ7OjgwPvIeg6CFLoN4KhE0zddbeR0v8xEvyH9wczeynaZVO6GOPu26PjO4AxySwmTq42s1eip/pS4pRXd6K9Sc8C/k4a/E5dtgdS+Hcys0wzWw3sBP4IbAL2untHt0Qptc8bKsGUjk5z95MJegP+XPQ0UlqJ3ryX6uea7wWmAOXAduDbyS1nYMysCHgMuNbd98W+l4q/Uzfbk9K/k7u3uXs5QY8Nc4Hjk1zSYRkqwdSX7jZSirtvi77uBH5D8B9jOngneh2g43rAziTXc1jc/Z3oTqMdeIAU/J2i1y0eA37p7r+Ozk7Z36m77UmH3wnA3fcCzwPvAoZHuw6CFNvnDZVg6kt3GynDzAqjF24xs0Lgn4C1Pa+VMmK7Pfk48Nsk1nLYOnbeUReSYr9T9ML6j4FX3f07MW+l5O90qO1J5d/JzEaZ2fDoeD5BI69XCQLqQ9HFUuY3giHSKg8g2vzzbvZ3t3FrkksaMDObTHCUBEG3Uv+dittjZg8BZwIjgXeArwP/CzwCTADeAi5x95RoUHCI7TmT4PSQA1uAf425NhN6ZnYa8BdgDdAenf0VgusyKfc79bA9l5Oiv5OZzSBo3JBJcLDxiLvfHN1PPAyMAFYBV7p7c/Iq7bshE0wiIpIahsqpPBERSREKJhERCRUFk4iIhIqCSUREQkXBJCIioaJgEhlEZnammf0u2XWIhJmCSUREQkXBJNINM7sy+oyb1WZ2f7STzDoz+270mTfPmdmo6LLlZrYs2gHobzo6ADWzY8zs2ehzclaa2ZToxxeZ2aNmtsHMfhntjUBEohRMIl2Y2TTgUmB+tGPMNuAKoBBY4e7TgT8T9OwA8DPgenefQdCjQMf8XwL3uPtM4P8RdA4KQY/W1wInAJOB+QnfKJEUktX7IiJDznuB2cDy6MFMPkEnpe3A/0SX+QXwazMbBgx39z9H5z8I/Cral+E4d/8NgLs3AUQ/7x/uXhmdXg1MJHi4m4igYBLpjgEPuvuXD5hpdmOX5Qban1dsf2Vt6P9DkQPoVJ7IwZ4DPmRmowHMbISZHU3w/0tHb80fAf7q7jVAtZmdHp3/UeDP0aejVprZBdHPyDWzgkHdCpEUpX+piXTh7uvN7GsETwjOAFqBzwH1wNzoezsJrkNB8EiB+6LB8ybwyej8jwL3m9nN0c/48CBuhkjKUu/iIn1kZnXuXpTsOkTSnU7liYhIqOiISUREQkVHTCIiEioKJhERCRUFk4iIhIqCSUREQkXBJCIiofL/AREpQ3HK66RsAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "325/325 [==============================] - 15s 45ms/step - loss: 0.4060 - accuracy: 0.7676\n",
            "loss_and_metrics : [0.40600791573524475, 0.7676156759262085]\n",
            "0:04:55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is1S_VPjSX4Q"
      },
      "source": [
        "RNN 이당"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shZqhqeYShqv"
      },
      "source": [
        "# 데이터 변환\n",
        "# LTSM : 입력을 (샘플 수, 타입스텝, 특성 수)로 형태 변환\n",
        "x_train = np.array(X3_train).reshape((X3_train.shape[0], X3_train.shape[1], 1))\n",
        "y_train = np.array(rorl_train).reshape((rorl_train.shape[0], 1))\n",
        "x_test = np.array(X3_test).reshape((X3_test.shape[0], X3_test.shape[1],1))\n",
        "y_test = np.array(rorl_test).reshape((rorl_test.shape[0], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "0Q3-ID9CMO8Y",
        "outputId": "4688a7a1-370a-4bc8-ee31-990da4228e63"
      },
      "source": [
        "start = time.time()\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Activation, Dropout, LSTM\n",
        "\n",
        "# RNN 모델 구성\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(32,input_shape = (65,1), return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(SimpleRNN(128, return_sequences = True ,activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(SimpleRNN(256, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(SimpleRNN(128, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(SimpleRNN(64, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "hist = model.fit(x_train, y_train, epochs=10, batch_size=512 )\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "46/46 [==============================] - 9s 191ms/step - loss: 0.7134 - accuracy: 0.5553\n",
            "Epoch 2/10\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.5437 - accuracy: 0.6963\n",
            "Epoch 3/10\n",
            "46/46 [==============================] - 9s 193ms/step - loss: 0.4435 - accuracy: 0.7562\n",
            "Epoch 4/10\n",
            "46/46 [==============================] - 10s 213ms/step - loss: 0.4062 - accuracy: 0.7743\n",
            "Epoch 5/10\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.3860 - accuracy: 0.7848\n",
            "Epoch 6/10\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.3700 - accuracy: 0.7935\n",
            "Epoch 7/10\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.3534 - accuracy: 0.8024\n",
            "Epoch 8/10\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.3425 - accuracy: 0.8083\n",
            "Epoch 9/10\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.3316 - accuracy: 0.8160\n",
            "Epoch 10/10\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.3287 - accuracy: 0.8172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "325/325 [==============================] - 9s 26ms/step - loss: 0.3644 - accuracy: 0.8130\n",
            "loss_and_metrics : [0.364397794008255, 0.8130229115486145]\n",
            "0:01:48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_Kr8ZhjEnNN"
      },
      "source": [
        "# 3단계로 나누어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv7r_VZSN2BQ",
        "outputId": "868ad252-e231-4e2d-c027-4bccd9f89ed4"
      },
      "source": [
        "# 데이터 로드\n",
        "# Refrigerant overcharge\n",
        "start = time.time()\n",
        "ro10 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro10.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro20.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro30.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro40 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"ro\",  clock(start) )\n",
        "\n",
        "# Refrigerant leak\n",
        "start = time.time()\n",
        "rl10 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl10.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl20.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl30.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl40 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"rl\",  clock(start) )\n",
        "\n",
        "# 심각도 수준 적기\n",
        "ro10['severity'] = 1\n",
        "ro20['severity'] = 2\n",
        "ro30['severity'] = 3\n",
        "ro40['severity'] = 4\n",
        "\n",
        "rl10['severity'] = 1\n",
        "rl20['severity'] = 2\n",
        "rl30['severity'] = 3\n",
        "rl40['severity'] = 4\n",
        "\n",
        "#\n",
        "ro = pd.concat([ro20,ro30,ro40], axis=0)\n",
        "rl= pd.concat([rl20,rl30,rl40], axis=0)\n",
        "\n",
        "# 1은 ro, 2는 rl\n",
        "ro['rorl'] = 1  \n",
        "rl['rorl'] = 2  \n",
        "\n",
        "# 0은 심각도 1단계들만 묶어서\n",
        "ro10['rorl'] = 0  \n",
        "rl10['rorl'] = 0 \n",
        "\n",
        "#\n",
        "ro = pd.concat([ro10,ro], axis=0)\n",
        "rl= pd.concat([rl10,rl], axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ro 0:00:03\n",
            "rl 0:00:03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "7BGyJPCcN2Bo",
        "outputId": "3693876a-12a2-428d-806f-294c7818199c"
      },
      "source": [
        "data1 = pd.concat([ro, rl], axis=0)\n",
        "data1 = data1.drop(['Time'],axis=1)\n",
        "data1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TWE_set</th>\n",
              "      <th>TEI</th>\n",
              "      <th>TWEI</th>\n",
              "      <th>TEO</th>\n",
              "      <th>TWEO</th>\n",
              "      <th>TCI</th>\n",
              "      <th>TWCI</th>\n",
              "      <th>TCO</th>\n",
              "      <th>TWCO</th>\n",
              "      <th>TSI</th>\n",
              "      <th>TSO</th>\n",
              "      <th>TBI</th>\n",
              "      <th>TBO</th>\n",
              "      <th>Cond Tons</th>\n",
              "      <th>Cooling Tons</th>\n",
              "      <th>Shared Cond Tons</th>\n",
              "      <th>Cond Energy Balance</th>\n",
              "      <th>Evap Tons</th>\n",
              "      <th>Shared Evap Tons</th>\n",
              "      <th>Building Tons</th>\n",
              "      <th>Evap Energy Balance</th>\n",
              "      <th>kW</th>\n",
              "      <th>COP</th>\n",
              "      <th>kW/Ton</th>\n",
              "      <th>FWC</th>\n",
              "      <th>FWE</th>\n",
              "      <th>TEA</th>\n",
              "      <th>TCA</th>\n",
              "      <th>TRE</th>\n",
              "      <th>PRE</th>\n",
              "      <th>TRC</th>\n",
              "      <th>PRC</th>\n",
              "      <th>TRC_sub</th>\n",
              "      <th>T_suc</th>\n",
              "      <th>Tsh_suc</th>\n",
              "      <th>TR_dis</th>\n",
              "      <th>Tsh_dis</th>\n",
              "      <th>P_lift</th>\n",
              "      <th>Amps</th>\n",
              "      <th>RLA%</th>\n",
              "      <th>Heat Balance (kW)</th>\n",
              "      <th>Heat Balance%</th>\n",
              "      <th>Tolerance%</th>\n",
              "      <th>Unit Status</th>\n",
              "      <th>Active Fault</th>\n",
              "      <th>TO_sump</th>\n",
              "      <th>TO_feed</th>\n",
              "      <th>PO_feed</th>\n",
              "      <th>PO_net</th>\n",
              "      <th>TWCD</th>\n",
              "      <th>TWED</th>\n",
              "      <th>VSS</th>\n",
              "      <th>VSL</th>\n",
              "      <th>VH</th>\n",
              "      <th>VM</th>\n",
              "      <th>VC</th>\n",
              "      <th>VE</th>\n",
              "      <th>VW</th>\n",
              "      <th>TWI</th>\n",
              "      <th>TWO</th>\n",
              "      <th>THI</th>\n",
              "      <th>THO</th>\n",
              "      <th>FWW</th>\n",
              "      <th>FWH</th>\n",
              "      <th>FWB</th>\n",
              "      <th>severity</th>\n",
              "      <th>rorl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>70.67</td>\n",
              "      <td>70.6</td>\n",
              "      <td>71.76</td>\n",
              "      <td>71.7</td>\n",
              "      <td>71.73</td>\n",
              "      <td>71.5</td>\n",
              "      <td>72.59</td>\n",
              "      <td>72.5</td>\n",
              "      <td>72.24</td>\n",
              "      <td>73.22</td>\n",
              "      <td>70.89</td>\n",
              "      <td>71.37</td>\n",
              "      <td>1.383000e-46</td>\n",
              "      <td>5.720000e-47</td>\n",
              "      <td>1.590000e-46</td>\n",
              "      <td>2.400000e-46</td>\n",
              "      <td>1.407000e-46</td>\n",
              "      <td>-1.122000e-46</td>\n",
              "      <td>6.212000e-47</td>\n",
              "      <td>9.067000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>4.949000e-46</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.6</td>\n",
              "      <td>73.5</td>\n",
              "      <td>71.2</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>84.7</td>\n",
              "      <td>13.1</td>\n",
              "      <td>75.2</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>114.7</td>\n",
              "      <td>90.7</td>\n",
              "      <td>69.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>66.70</td>\n",
              "      <td>71.87</td>\n",
              "      <td>70.65</td>\n",
              "      <td>71.54</td>\n",
              "      <td>2.655000e-46</td>\n",
              "      <td>1.491000e-45</td>\n",
              "      <td>1.175000e-45</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>70.67</td>\n",
              "      <td>70.8</td>\n",
              "      <td>71.76</td>\n",
              "      <td>71.7</td>\n",
              "      <td>71.73</td>\n",
              "      <td>71.7</td>\n",
              "      <td>72.55</td>\n",
              "      <td>72.3</td>\n",
              "      <td>72.28</td>\n",
              "      <td>73.27</td>\n",
              "      <td>70.89</td>\n",
              "      <td>71.37</td>\n",
              "      <td>1.317000e-46</td>\n",
              "      <td>4.399000e-47</td>\n",
              "      <td>1.590000e-46</td>\n",
              "      <td>2.466000e-46</td>\n",
              "      <td>1.407000e-46</td>\n",
              "      <td>-1.122000e-46</td>\n",
              "      <td>6.212000e-47</td>\n",
              "      <td>9.067000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>4.949000e-46</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.6</td>\n",
              "      <td>73.5</td>\n",
              "      <td>71.2</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>84.7</td>\n",
              "      <td>13.1</td>\n",
              "      <td>75.2</td>\n",
              "      <td>3.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>114.5</td>\n",
              "      <td>90.8</td>\n",
              "      <td>69.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>66.70</td>\n",
              "      <td>71.87</td>\n",
              "      <td>70.65</td>\n",
              "      <td>71.58</td>\n",
              "      <td>2.042000e-46</td>\n",
              "      <td>1.491000e-45</td>\n",
              "      <td>1.175000e-45</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50</td>\n",
              "      <td>70.67</td>\n",
              "      <td>70.9</td>\n",
              "      <td>71.76</td>\n",
              "      <td>71.8</td>\n",
              "      <td>71.73</td>\n",
              "      <td>72.0</td>\n",
              "      <td>72.59</td>\n",
              "      <td>72.0</td>\n",
              "      <td>72.28</td>\n",
              "      <td>73.27</td>\n",
              "      <td>70.89</td>\n",
              "      <td>71.37</td>\n",
              "      <td>8.520000e+00</td>\n",
              "      <td>3.118000e+00</td>\n",
              "      <td>9.796000e+00</td>\n",
              "      <td>1.520000e+01</td>\n",
              "      <td>6.133000e+00</td>\n",
              "      <td>-4.889000e+00</td>\n",
              "      <td>2.707000e+00</td>\n",
              "      <td>3.951000e+00</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>2.156000e+01</td>\n",
              "      <td>2.742000e-46</td>\n",
              "      <td>2.383000e+02</td>\n",
              "      <td>1.350000e+02</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.1</td>\n",
              "      <td>72.5</td>\n",
              "      <td>71.9</td>\n",
              "      <td>74.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>84.7</td>\n",
              "      <td>13.6</td>\n",
              "      <td>75.4</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-8.392</td>\n",
              "      <td>-28.01</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>114.5</td>\n",
              "      <td>90.8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>66.70</td>\n",
              "      <td>71.87</td>\n",
              "      <td>70.69</td>\n",
              "      <td>71.54</td>\n",
              "      <td>1.447000e+01</td>\n",
              "      <td>6.496000e+01</td>\n",
              "      <td>1.210000e+02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50</td>\n",
              "      <td>70.75</td>\n",
              "      <td>71.5</td>\n",
              "      <td>71.76</td>\n",
              "      <td>71.4</td>\n",
              "      <td>71.82</td>\n",
              "      <td>71.7</td>\n",
              "      <td>72.51</td>\n",
              "      <td>72.0</td>\n",
              "      <td>72.24</td>\n",
              "      <td>73.06</td>\n",
              "      <td>70.97</td>\n",
              "      <td>71.37</td>\n",
              "      <td>7.644000e+00</td>\n",
              "      <td>3.006000e+00</td>\n",
              "      <td>9.060000e+00</td>\n",
              "      <td>1.370000e+01</td>\n",
              "      <td>8.448000e+00</td>\n",
              "      <td>-6.594000e+00</td>\n",
              "      <td>3.345000e+00</td>\n",
              "      <td>5.198000e+00</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>2.970000e+01</td>\n",
              "      <td>1.991000e-46</td>\n",
              "      <td>2.643000e+02</td>\n",
              "      <td>2.010000e+02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.4</td>\n",
              "      <td>73.3</td>\n",
              "      <td>71.6</td>\n",
              "      <td>73.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>84.8</td>\n",
              "      <td>13.4</td>\n",
              "      <td>75.4</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.824</td>\n",
              "      <td>10.51</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>113.8</td>\n",
              "      <td>90.8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>66.70</td>\n",
              "      <td>71.83</td>\n",
              "      <td>71.59</td>\n",
              "      <td>71.41</td>\n",
              "      <td>1.406000e+01</td>\n",
              "      <td>8.028000e+01</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>70.88</td>\n",
              "      <td>71.5</td>\n",
              "      <td>71.68</td>\n",
              "      <td>71.5</td>\n",
              "      <td>71.86</td>\n",
              "      <td>71.8</td>\n",
              "      <td>72.43</td>\n",
              "      <td>71.8</td>\n",
              "      <td>72.20</td>\n",
              "      <td>72.73</td>\n",
              "      <td>71.02</td>\n",
              "      <td>71.41</td>\n",
              "      <td>6.385000e+00</td>\n",
              "      <td>2.593000e+00</td>\n",
              "      <td>5.991000e+00</td>\n",
              "      <td>9.783000e+00</td>\n",
              "      <td>7.115000e+00</td>\n",
              "      <td>-5.881000e+00</td>\n",
              "      <td>3.534000e+00</td>\n",
              "      <td>4.768000e+00</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>2.502000e+01</td>\n",
              "      <td>2.363000e-46</td>\n",
              "      <td>2.682000e+02</td>\n",
              "      <td>2.123000e+02</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.1</td>\n",
              "      <td>72.8</td>\n",
              "      <td>71.6</td>\n",
              "      <td>73.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>84.9</td>\n",
              "      <td>13.8</td>\n",
              "      <td>75.2</td>\n",
              "      <td>3.8</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.566</td>\n",
              "      <td>11.43</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>113.8</td>\n",
              "      <td>90.8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>66.46</td>\n",
              "      <td>71.75</td>\n",
              "      <td>71.47</td>\n",
              "      <td>71.37</td>\n",
              "      <td>1.176000e+01</td>\n",
              "      <td>8.481000e+01</td>\n",
              "      <td>1.271000e+02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5186</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.1</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.40</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.85</td>\n",
              "      <td>57.00</td>\n",
              "      <td>5.248000e-47</td>\n",
              "      <td>2.419000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>4.042000e-48</td>\n",
              "      <td>1.984000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.4</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>57.3</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.7</td>\n",
              "      <td>17.8</td>\n",
              "      <td>97.3</td>\n",
              "      <td>39.8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>106.3</td>\n",
              "      <td>106.5</td>\n",
              "      <td>49.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.60</td>\n",
              "      <td>57.18</td>\n",
              "      <td>57.20</td>\n",
              "      <td>5.806000e-46</td>\n",
              "      <td>4.762000e-46</td>\n",
              "      <td>3.771000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5187</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>56.9</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.8</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.36</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.29</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>56.96</td>\n",
              "      <td>4.588000e-47</td>\n",
              "      <td>1.100000e-47</td>\n",
              "      <td>4.557000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>9.272000e-48</td>\n",
              "      <td>1.671000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>57.3</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.6</td>\n",
              "      <td>17.8</td>\n",
              "      <td>97.3</td>\n",
              "      <td>40.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>106.3</td>\n",
              "      <td>106.7</td>\n",
              "      <td>49.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.60</td>\n",
              "      <td>57.18</td>\n",
              "      <td>57.25</td>\n",
              "      <td>2.641000e-46</td>\n",
              "      <td>2.225000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5188</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.1</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.2</td>\n",
              "      <td>57.40</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>57.00</td>\n",
              "      <td>5.248000e-47</td>\n",
              "      <td>2.419000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>1.456000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.7</td>\n",
              "      <td>56.9</td>\n",
              "      <td>53.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.3</td>\n",
              "      <td>17.5</td>\n",
              "      <td>97.0</td>\n",
              "      <td>39.9</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>106.1</td>\n",
              "      <td>106.5</td>\n",
              "      <td>49.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.64</td>\n",
              "      <td>57.18</td>\n",
              "      <td>57.29</td>\n",
              "      <td>5.806000e-46</td>\n",
              "      <td>3.494000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5189</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.1</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.04</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.40</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>57.00</td>\n",
              "      <td>5.907000e-47</td>\n",
              "      <td>2.419000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-4.109000e-48</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>1.456000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>56.9</td>\n",
              "      <td>53.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.3</td>\n",
              "      <td>17.4</td>\n",
              "      <td>96.8</td>\n",
              "      <td>40.1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>105.9</td>\n",
              "      <td>106.5</td>\n",
              "      <td>49.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.60</td>\n",
              "      <td>57.14</td>\n",
              "      <td>57.29</td>\n",
              "      <td>5.806000e-46</td>\n",
              "      <td>3.494000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5190</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.2</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.8</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.36</td>\n",
              "      <td>57.2</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>57.00</td>\n",
              "      <td>4.588000e-47</td>\n",
              "      <td>1.759000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>1.456000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>57.3</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.1</td>\n",
              "      <td>17.5</td>\n",
              "      <td>96.8</td>\n",
              "      <td>39.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>105.9</td>\n",
              "      <td>106.3</td>\n",
              "      <td>49.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.64</td>\n",
              "      <td>57.10</td>\n",
              "      <td>57.33</td>\n",
              "      <td>4.222000e-46</td>\n",
              "      <td>3.494000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41528 rows × 67 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      TWE_set    TEI  TWEI    TEO  ...           FWH           FWB  severity  rorl\n",
              "0          50  70.67  70.6  71.76  ...  1.491000e-45  1.175000e-45         1     0\n",
              "1          50  70.67  70.8  71.76  ...  1.491000e-45  1.175000e-45         1     0\n",
              "2          50  70.67  70.9  71.76  ...  6.496000e+01  1.210000e+02         1     0\n",
              "3          50  70.75  71.5  71.76  ...  8.028000e+01  1.060000e+02         1     0\n",
              "4          50  70.88  71.5  71.68  ...  8.481000e+01  1.271000e+02         1     0\n",
              "...       ...    ...   ...    ...  ...           ...           ...       ...   ...\n",
              "5186       40  56.83  57.1  56.82  ...  4.762000e-46  3.771000e-45         4     2\n",
              "5187       40  56.83  56.9  56.82  ...  2.225000e-46  3.644000e-45         4     2\n",
              "5188       40  56.83  57.1  56.82  ...  3.494000e-46  3.644000e-45         4     2\n",
              "5189       40  56.83  57.1  56.82  ...  3.494000e-46  3.644000e-45         4     2\n",
              "5190       40  56.83  57.2  56.82  ...  3.494000e-46  3.644000e-45         4     2\n",
              "\n",
              "[41528 rows x 67 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 312
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVv7CzW8N2Bu"
      },
      "source": [
        "# 표준화\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(data1.iloc[:,:-2])\n",
        "X_scaled = scaler.transform(data1.iloc[:,:-2])\n",
        "X_scaled = pd.DataFrame(X_scaled).reset_index(drop=True)\n",
        "\n",
        "y_rorl_sev = pd.DataFrame(data1.iloc[:,-2:]).reset_index(drop=True)\n",
        "\n",
        "X = pd.concat([X_scaled, y_rorl_sev], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "2ANoAjPwN2B0",
        "outputId": "d7326728-b821-4a3f-c66b-b7514576a2cd"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>severity</th>\n",
              "      <th>rorl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.335243</td>\n",
              "      <td>3.402779</td>\n",
              "      <td>3.327928</td>\n",
              "      <td>5.326046</td>\n",
              "      <td>5.215082</td>\n",
              "      <td>-0.420948</td>\n",
              "      <td>-0.422333</td>\n",
              "      <td>-1.051454</td>\n",
              "      <td>-1.046437</td>\n",
              "      <td>-0.900070</td>\n",
              "      <td>-0.004790</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>3.510340</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.765152</td>\n",
              "      <td>-1.673922</td>\n",
              "      <td>7.784881</td>\n",
              "      <td>8.806172</td>\n",
              "      <td>-1.544866</td>\n",
              "      <td>-1.482462</td>\n",
              "      <td>-1.664642</td>\n",
              "      <td>6.882298</td>\n",
              "      <td>4.098866</td>\n",
              "      <td>-3.769125</td>\n",
              "      <td>-2.652622</td>\n",
              "      <td>-3.686021</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-11.203416</td>\n",
              "      <td>20.560203</td>\n",
              "      <td>0.077702</td>\n",
              "      <td>-5.840967</td>\n",
              "      <td>-6.262078</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.123650</td>\n",
              "      <td>-2.428707</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.953836</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-2.405616</td>\n",
              "      <td>4.134077</td>\n",
              "      <td>0.257199</td>\n",
              "      <td>2.079823</td>\n",
              "      <td>2.838880</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.335243</td>\n",
              "      <td>3.402779</td>\n",
              "      <td>3.364809</td>\n",
              "      <td>5.326046</td>\n",
              "      <td>5.215082</td>\n",
              "      <td>-0.420948</td>\n",
              "      <td>-0.396225</td>\n",
              "      <td>-1.056382</td>\n",
              "      <td>-1.070751</td>\n",
              "      <td>-0.894954</td>\n",
              "      <td>-0.004789</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>3.510340</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.765152</td>\n",
              "      <td>-1.673922</td>\n",
              "      <td>7.784881</td>\n",
              "      <td>8.806172</td>\n",
              "      <td>-1.544866</td>\n",
              "      <td>-1.482462</td>\n",
              "      <td>-1.664642</td>\n",
              "      <td>6.882298</td>\n",
              "      <td>4.098866</td>\n",
              "      <td>-3.769125</td>\n",
              "      <td>-2.669300</td>\n",
              "      <td>-3.686021</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-11.203416</td>\n",
              "      <td>20.560203</td>\n",
              "      <td>0.023122</td>\n",
              "      <td>-5.810746</td>\n",
              "      <td>-6.262078</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.282125</td>\n",
              "      <td>-2.428707</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.953836</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-0.651139</td>\n",
              "      <td>4.134077</td>\n",
              "      <td>0.257199</td>\n",
              "      <td>2.079823</td>\n",
              "      <td>2.845402</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.335243</td>\n",
              "      <td>3.402779</td>\n",
              "      <td>3.383250</td>\n",
              "      <td>5.326046</td>\n",
              "      <td>5.235392</td>\n",
              "      <td>-0.420948</td>\n",
              "      <td>-0.357063</td>\n",
              "      <td>-1.051454</td>\n",
              "      <td>-1.107222</td>\n",
              "      <td>-0.894954</td>\n",
              "      <td>-0.004789</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>3.510340</td>\n",
              "      <td>-2.308409</td>\n",
              "      <td>-1.950887</td>\n",
              "      <td>-0.005095</td>\n",
              "      <td>-0.004748</td>\n",
              "      <td>-2.180182</td>\n",
              "      <td>-0.007872</td>\n",
              "      <td>0.004794</td>\n",
              "      <td>0.626725</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>2.783456</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-1.960805</td>\n",
              "      <td>-6.673784</td>\n",
              "      <td>-2.447019</td>\n",
              "      <td>-1.673922</td>\n",
              "      <td>7.662305</td>\n",
              "      <td>8.576831</td>\n",
              "      <td>-1.467809</td>\n",
              "      <td>-1.419856</td>\n",
              "      <td>-1.641132</td>\n",
              "      <td>6.882298</td>\n",
              "      <td>4.342110</td>\n",
              "      <td>-3.752682</td>\n",
              "      <td>-2.694316</td>\n",
              "      <td>-3.614197</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004909</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>0.023122</td>\n",
              "      <td>-5.810746</td>\n",
              "      <td>-6.359710</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.519838</td>\n",
              "      <td>-2.428707</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.953836</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-0.651139</td>\n",
              "      <td>4.134077</td>\n",
              "      <td>0.257199</td>\n",
              "      <td>2.085309</td>\n",
              "      <td>2.838880</td>\n",
              "      <td>-0.751171</td>\n",
              "      <td>-0.004918</td>\n",
              "      <td>0.004511</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.335243</td>\n",
              "      <td>3.417711</td>\n",
              "      <td>3.493896</td>\n",
              "      <td>5.326046</td>\n",
              "      <td>5.154151</td>\n",
              "      <td>-0.408945</td>\n",
              "      <td>-0.396225</td>\n",
              "      <td>-1.061310</td>\n",
              "      <td>-1.107222</td>\n",
              "      <td>-0.900070</td>\n",
              "      <td>-0.004792</td>\n",
              "      <td>0.003633</td>\n",
              "      <td>3.510340</td>\n",
              "      <td>-2.341469</td>\n",
              "      <td>-1.963326</td>\n",
              "      <td>-0.005096</td>\n",
              "      <td>-0.004750</td>\n",
              "      <td>-2.081504</td>\n",
              "      <td>-0.007958</td>\n",
              "      <td>0.004826</td>\n",
              "      <td>0.952783</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>4.065771</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.228081</td>\n",
              "      <td>-1.140284</td>\n",
              "      <td>-2.810600</td>\n",
              "      <td>-1.673922</td>\n",
              "      <td>7.735851</td>\n",
              "      <td>8.760304</td>\n",
              "      <td>-1.500834</td>\n",
              "      <td>-1.451159</td>\n",
              "      <td>-1.688153</td>\n",
              "      <td>6.899425</td>\n",
              "      <td>4.244813</td>\n",
              "      <td>-3.752682</td>\n",
              "      <td>-2.685977</td>\n",
              "      <td>-3.662080</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-0.167907</td>\n",
              "      <td>-5.810746</td>\n",
              "      <td>-6.359710</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.440600</td>\n",
              "      <td>-2.391489</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.953836</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-0.651139</td>\n",
              "      <td>4.134077</td>\n",
              "      <td>0.251245</td>\n",
              "      <td>2.208740</td>\n",
              "      <td>2.817681</td>\n",
              "      <td>-0.757975</td>\n",
              "      <td>-0.004885</td>\n",
              "      <td>0.004252</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.335243</td>\n",
              "      <td>3.441974</td>\n",
              "      <td>3.493896</td>\n",
              "      <td>5.309470</td>\n",
              "      <td>5.174461</td>\n",
              "      <td>-0.403610</td>\n",
              "      <td>-0.383171</td>\n",
              "      <td>-1.071167</td>\n",
              "      <td>-1.131536</td>\n",
              "      <td>-0.905187</td>\n",
              "      <td>-0.004796</td>\n",
              "      <td>0.003655</td>\n",
              "      <td>3.517826</td>\n",
              "      <td>-2.388982</td>\n",
              "      <td>-2.009195</td>\n",
              "      <td>-0.005100</td>\n",
              "      <td>-0.004755</td>\n",
              "      <td>-2.138324</td>\n",
              "      <td>-0.007922</td>\n",
              "      <td>0.004835</td>\n",
              "      <td>0.840349</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>3.328519</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>0.031828</td>\n",
              "      <td>-0.192882</td>\n",
              "      <td>-2.674257</td>\n",
              "      <td>-1.673922</td>\n",
              "      <td>7.662305</td>\n",
              "      <td>8.645634</td>\n",
              "      <td>-1.500834</td>\n",
              "      <td>-1.451159</td>\n",
              "      <td>-1.664642</td>\n",
              "      <td>6.916553</td>\n",
              "      <td>4.439408</td>\n",
              "      <td>-3.769125</td>\n",
              "      <td>-2.669300</td>\n",
              "      <td>-3.656095</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-0.167907</td>\n",
              "      <td>-5.810746</td>\n",
              "      <td>-6.359710</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.519838</td>\n",
              "      <td>-2.391489</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.953836</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-0.651139</td>\n",
              "      <td>4.016591</td>\n",
              "      <td>0.239336</td>\n",
              "      <td>2.192282</td>\n",
              "      <td>2.811159</td>\n",
              "      <td>-0.796150</td>\n",
              "      <td>-0.004876</td>\n",
              "      <td>0.004617</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41523</th>\n",
              "      <td>-1.074138</td>\n",
              "      <td>0.819658</td>\n",
              "      <td>0.838407</td>\n",
              "      <td>2.230424</td>\n",
              "      <td>2.209143</td>\n",
              "      <td>-2.374790</td>\n",
              "      <td>-2.302082</td>\n",
              "      <td>-2.922942</td>\n",
              "      <td>-2.942924</td>\n",
              "      <td>-2.817463</td>\n",
              "      <td>-0.005007</td>\n",
              "      <td>-0.002744</td>\n",
              "      <td>0.821058</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.765152</td>\n",
              "      <td>-1.512094</td>\n",
              "      <td>4.156631</td>\n",
              "      <td>4.219358</td>\n",
              "      <td>-3.074998</td>\n",
              "      <td>-2.665702</td>\n",
              "      <td>-1.688153</td>\n",
              "      <td>5.169589</td>\n",
              "      <td>6.385363</td>\n",
              "      <td>-1.952184</td>\n",
              "      <td>0.332614</td>\n",
              "      <td>-3.680036</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-2.214653</td>\n",
              "      <td>-1.066068</td>\n",
              "      <td>-8.146378</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.519838</td>\n",
              "      <td>-2.317053</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.663842</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-2.405616</td>\n",
              "      <td>-1.113654</td>\n",
              "      <td>-2.015927</td>\n",
              "      <td>0.232477</td>\n",
              "      <td>0.500567</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41524</th>\n",
              "      <td>-1.074138</td>\n",
              "      <td>0.819658</td>\n",
              "      <td>0.801526</td>\n",
              "      <td>2.230424</td>\n",
              "      <td>2.188833</td>\n",
              "      <td>-2.374790</td>\n",
              "      <td>-2.302082</td>\n",
              "      <td>-2.927870</td>\n",
              "      <td>-2.918610</td>\n",
              "      <td>-2.812347</td>\n",
              "      <td>-0.005007</td>\n",
              "      <td>-0.002726</td>\n",
              "      <td>0.813572</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.765152</td>\n",
              "      <td>-1.593008</td>\n",
              "      <td>4.156631</td>\n",
              "      <td>4.219358</td>\n",
              "      <td>-3.074998</td>\n",
              "      <td>-2.665702</td>\n",
              "      <td>-1.688153</td>\n",
              "      <td>5.152462</td>\n",
              "      <td>6.385363</td>\n",
              "      <td>-1.952184</td>\n",
              "      <td>0.357630</td>\n",
              "      <td>-3.680036</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-2.214653</td>\n",
              "      <td>-1.005626</td>\n",
              "      <td>-8.195194</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.440600</td>\n",
              "      <td>-2.428707</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.663842</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-2.405616</td>\n",
              "      <td>-1.113654</td>\n",
              "      <td>-2.015927</td>\n",
              "      <td>0.232477</td>\n",
              "      <td>0.508720</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41525</th>\n",
              "      <td>-1.074138</td>\n",
              "      <td>0.819658</td>\n",
              "      <td>0.838407</td>\n",
              "      <td>2.230424</td>\n",
              "      <td>2.209143</td>\n",
              "      <td>-2.374790</td>\n",
              "      <td>-2.289029</td>\n",
              "      <td>-2.922942</td>\n",
              "      <td>-2.918610</td>\n",
              "      <td>-2.817463</td>\n",
              "      <td>-0.005007</td>\n",
              "      <td>-0.002726</td>\n",
              "      <td>0.821058</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.810600</td>\n",
              "      <td>-1.593008</td>\n",
              "      <td>4.156631</td>\n",
              "      <td>4.265226</td>\n",
              "      <td>-3.119031</td>\n",
              "      <td>-2.697005</td>\n",
              "      <td>-1.688153</td>\n",
              "      <td>5.101081</td>\n",
              "      <td>6.239417</td>\n",
              "      <td>-1.976848</td>\n",
              "      <td>0.340952</td>\n",
              "      <td>-3.638139</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-2.269233</td>\n",
              "      <td>-1.066068</td>\n",
              "      <td>-8.146378</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.519838</td>\n",
              "      <td>-2.354271</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.663842</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-2.405616</td>\n",
              "      <td>-1.113654</td>\n",
              "      <td>-2.009973</td>\n",
              "      <td>0.232477</td>\n",
              "      <td>0.515243</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41526</th>\n",
              "      <td>-1.074138</td>\n",
              "      <td>0.819658</td>\n",
              "      <td>0.838407</td>\n",
              "      <td>2.230424</td>\n",
              "      <td>2.209143</td>\n",
              "      <td>-2.380125</td>\n",
              "      <td>-2.302082</td>\n",
              "      <td>-2.922942</td>\n",
              "      <td>-2.918610</td>\n",
              "      <td>-2.817463</td>\n",
              "      <td>-0.005007</td>\n",
              "      <td>-0.002726</td>\n",
              "      <td>0.821058</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.765152</td>\n",
              "      <td>-1.673922</td>\n",
              "      <td>4.156631</td>\n",
              "      <td>4.219358</td>\n",
              "      <td>-3.119031</td>\n",
              "      <td>-2.697005</td>\n",
              "      <td>-1.688153</td>\n",
              "      <td>5.101081</td>\n",
              "      <td>6.190768</td>\n",
              "      <td>-1.993291</td>\n",
              "      <td>0.357630</td>\n",
              "      <td>-3.668065</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-2.323812</td>\n",
              "      <td>-1.066068</td>\n",
              "      <td>-8.195194</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.519838</td>\n",
              "      <td>-2.317053</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.663842</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-2.405616</td>\n",
              "      <td>-1.113654</td>\n",
              "      <td>-2.015927</td>\n",
              "      <td>0.226991</td>\n",
              "      <td>0.515243</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41527</th>\n",
              "      <td>-1.074138</td>\n",
              "      <td>0.819658</td>\n",
              "      <td>0.856848</td>\n",
              "      <td>2.230424</td>\n",
              "      <td>2.188833</td>\n",
              "      <td>-2.374790</td>\n",
              "      <td>-2.302082</td>\n",
              "      <td>-2.927870</td>\n",
              "      <td>-2.906453</td>\n",
              "      <td>-2.817463</td>\n",
              "      <td>-0.005007</td>\n",
              "      <td>-0.002726</td>\n",
              "      <td>0.821058</td>\n",
              "      <td>-2.629946</td>\n",
              "      <td>-2.297180</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>-2.441604</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.004657</td>\n",
              "      <td>-0.406357</td>\n",
              "      <td>-0.00491</td>\n",
              "      <td>-0.612948</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-17.841893</td>\n",
              "      <td>-17.992307</td>\n",
              "      <td>-2.765152</td>\n",
              "      <td>-1.633465</td>\n",
              "      <td>4.156631</td>\n",
              "      <td>4.219358</td>\n",
              "      <td>-3.074998</td>\n",
              "      <td>-2.665702</td>\n",
              "      <td>-1.688153</td>\n",
              "      <td>5.066827</td>\n",
              "      <td>6.239417</td>\n",
              "      <td>-1.993291</td>\n",
              "      <td>0.290921</td>\n",
              "      <td>-3.686021</td>\n",
              "      <td>-3.298069</td>\n",
              "      <td>-3.316266</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.004907</td>\n",
              "      <td>5.784954</td>\n",
              "      <td>-8.223565</td>\n",
              "      <td>-0.136262</td>\n",
              "      <td>-2.323812</td>\n",
              "      <td>-1.126509</td>\n",
              "      <td>-8.146378</td>\n",
              "      <td>-7.730458</td>\n",
              "      <td>-2.519838</td>\n",
              "      <td>-2.317053</td>\n",
              "      <td>-1.899376</td>\n",
              "      <td>-0.377668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.893398</td>\n",
              "      <td>1.663842</td>\n",
              "      <td>0.00694</td>\n",
              "      <td>-2.405616</td>\n",
              "      <td>-1.113654</td>\n",
              "      <td>-2.009973</td>\n",
              "      <td>0.221505</td>\n",
              "      <td>0.521765</td>\n",
              "      <td>-0.991336</td>\n",
              "      <td>-0.005054</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41528 rows × 67 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1         2  ...        64  severity  rorl\n",
              "0      1.335243  3.402779  3.327928  ...  0.002418         1     0\n",
              "1      1.335243  3.402779  3.364809  ...  0.002418         1     0\n",
              "2      1.335243  3.402779  3.383250  ...  0.004511         1     0\n",
              "3      1.335243  3.417711  3.493896  ...  0.004252         1     0\n",
              "4      1.335243  3.441974  3.493896  ...  0.004617         1     0\n",
              "...         ...       ...       ...  ...       ...       ...   ...\n",
              "41523 -1.074138  0.819658  0.838407  ...  0.002418         4     2\n",
              "41524 -1.074138  0.819658  0.801526  ...  0.002418         4     2\n",
              "41525 -1.074138  0.819658  0.838407  ...  0.002418         4     2\n",
              "41526 -1.074138  0.819658  0.838407  ...  0.002418         4     2\n",
              "41527 -1.074138  0.819658  0.856848  ...  0.002418         4     2\n",
              "\n",
              "[41528 rows x 67 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZuH-BS6N2B3"
      },
      "source": [
        "X2 = X.iloc[:,:-1]\n",
        "y_rorl = X.iloc[:,-2:]\n",
        "y_sev = X.iloc[:,-2]\n",
        "\n",
        "\n",
        "# train, test 분리\n",
        "#rorl\n",
        "X3_train, X3_test, rorl_train, rorl_test = ms.train_test_split(X2, y_rorl, \n",
        "                                                      test_size = 0.25, random_state = 100, stratify = y_sev )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X0fZ4HhN2B7"
      },
      "source": [
        "# train 셋 (2,3,4만)\n",
        "X3_train = X3_train.iloc[:,:-1]\n",
        "rorl_train = rorl_train.iloc[:,-1]\n",
        "\n",
        "X3_test = X3_test.iloc[:,:-1]\n",
        "rorl_test = rorl_test.iloc[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1efPT80sSQGZ"
      },
      "source": [
        "모델이양"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiVV7H_LSnj5",
        "outputId": "3d3d4364-c21f-4bc4-d839-3ed18f08f95e"
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn import metrics   \n",
        "\n",
        "#분류->object 회귀\n",
        "start = time.time()\n",
        "\n",
        "# 부스팅 타입은 default값인 gbdt로, 학습률은 0.01로 지정 하였음\n",
        "LGB = lgb.LGBMClassifier(objective=\"regression\", boosting_type='gbdt', learning_rate = 0.01)\n",
        "\n",
        "param_list = {\"n_estimators\": list(range(10, 300, 10)),\n",
        "              \"max_depth\": list(range(4, 21, 4)),\n",
        "              \"max_features\": list(range(3, 13, 2)),\n",
        "              \"min_samples_split\": list(range(3, 13, 2))}\n",
        "\n",
        "# 하이퍼파라미터 최적화\n",
        "LGB_random_search = RandomizedSearchCV(estimator = LGB,\n",
        "                                        param_distributions = param_list,\n",
        "                                        n_iter = 10,      # 10번반복하는 lightgbm 구현 : 성능개선 시도\n",
        "                                        cv = 3,           # cross-validation 3번 반복\n",
        "                                        n_jobs = 10,\n",
        "                                        random_state=42)\n",
        "\n",
        "LGB_random_search.fit(X3_train, rorl_train)\n",
        "y_pred = LGB_random_search.predict(X3_test)\n",
        "\n",
        "#성능평가\n",
        "print('정확도 :', metrics.accuracy_score(rorl_test, y_pred))\n",
        "print(confusion_matrix(rorl_test, y_pred))\n",
        "print(classification_report(rorl_test, y_pred))\n",
        "\n",
        "print( LGB_random_search.best_params_ )\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정확도 : 0.9805432479291081\n",
            "[[2504   53   39]\n",
            " [  52 3786   28]\n",
            " [  25    5 3890]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.97      2596\n",
            "           1       0.98      0.98      0.98      3866\n",
            "           2       0.98      0.99      0.99      3920\n",
            "\n",
            "    accuracy                           0.98     10382\n",
            "   macro avg       0.98      0.98      0.98     10382\n",
            "weighted avg       0.98      0.98      0.98     10382\n",
            "\n",
            "{'n_estimators': 290, 'min_samples_split': 9, 'max_features': 7, 'max_depth': 8}\n",
            "0:04:30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKE35rPYSvb1"
      },
      "source": [
        "DNN 이당"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBv7k6HBgA3i"
      },
      "source": [
        "# 원핫 인코딩 (One_Hot)\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(rorl_train)\n",
        "one_hot_test_labels = to_categorical(rorl_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeZ6qbefgCdZ",
        "outputId": "3a4f80c9-c85f-42b4-b508-ce2c9936b46d"
      },
      "source": [
        "one_hot_train_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31146, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nboFSIAnSnkA",
        "outputId": "6021ee3c-701d-44cf-e9a7-59598e361bde"
      },
      "source": [
        "start = time.time()\n",
        "# 라이브러리\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='RandomUniform',activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))   # relu\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(3, activation='sigmoid'))  # softmax\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='Adam', loss=\"binary_crossentropy\", metrics=['accuracy'])  # binary_crossentropy, categorical_crossentropy\n",
        "print(model.summary())\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "epochs = 100\n",
        "hist = model.fit(X3_train, one_hot_train_labels, epochs=epochs, batch_size=64)\n",
        "\n",
        "    \n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(X3_test, one_hot_test_labels, batch_size=64)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_56\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_321 (Dense)            (None, 16)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_246 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_322 (Dense)            (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dropout_247 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_323 (Dense)            (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_248 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_324 (Dense)            (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_249 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_325 (Dense)            (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_326 (Dense)            (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 6,371\n",
            "Trainable params: 6,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.3627 - accuracy: 0.7308\n",
            "Epoch 2/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.2103 - accuracy: 0.8670\n",
            "Epoch 3/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1900 - accuracy: 0.8806\n",
            "Epoch 4/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1763 - accuracy: 0.8913\n",
            "Epoch 5/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1686 - accuracy: 0.8940\n",
            "Epoch 6/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1633 - accuracy: 0.8965\n",
            "Epoch 7/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1590 - accuracy: 0.8984\n",
            "Epoch 8/100\n",
            "487/487 [==============================] - 2s 4ms/step - loss: 0.1521 - accuracy: 0.9033\n",
            "Epoch 9/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1501 - accuracy: 0.9042\n",
            "Epoch 10/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1468 - accuracy: 0.9065\n",
            "Epoch 11/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1461 - accuracy: 0.9033\n",
            "Epoch 12/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1433 - accuracy: 0.9074\n",
            "Epoch 13/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1423 - accuracy: 0.9074\n",
            "Epoch 14/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1411 - accuracy: 0.9106\n",
            "Epoch 15/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1374 - accuracy: 0.9102\n",
            "Epoch 16/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1383 - accuracy: 0.9095\n",
            "Epoch 17/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1348 - accuracy: 0.9122\n",
            "Epoch 18/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1360 - accuracy: 0.9111\n",
            "Epoch 19/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1315 - accuracy: 0.9145\n",
            "Epoch 20/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1327 - accuracy: 0.9111\n",
            "Epoch 21/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1331 - accuracy: 0.9106\n",
            "Epoch 22/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1291 - accuracy: 0.9143\n",
            "Epoch 23/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1316 - accuracy: 0.9128\n",
            "Epoch 24/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1288 - accuracy: 0.9158\n",
            "Epoch 25/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1269 - accuracy: 0.9159\n",
            "Epoch 26/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1281 - accuracy: 0.9128\n",
            "Epoch 27/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1282 - accuracy: 0.9143\n",
            "Epoch 28/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1255 - accuracy: 0.9173\n",
            "Epoch 29/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1262 - accuracy: 0.9145\n",
            "Epoch 30/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1263 - accuracy: 0.9159\n",
            "Epoch 31/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1241 - accuracy: 0.9169\n",
            "Epoch 32/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1242 - accuracy: 0.9185\n",
            "Epoch 33/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1246 - accuracy: 0.9155\n",
            "Epoch 34/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1236 - accuracy: 0.9184\n",
            "Epoch 35/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1253 - accuracy: 0.9146\n",
            "Epoch 36/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1245 - accuracy: 0.9182\n",
            "Epoch 37/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1231 - accuracy: 0.9186\n",
            "Epoch 38/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1221 - accuracy: 0.9185\n",
            "Epoch 39/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1195 - accuracy: 0.9191\n",
            "Epoch 40/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1228 - accuracy: 0.9183\n",
            "Epoch 41/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1206 - accuracy: 0.9194\n",
            "Epoch 42/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1222 - accuracy: 0.9204\n",
            "Epoch 43/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1183 - accuracy: 0.9206\n",
            "Epoch 44/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1190 - accuracy: 0.9213\n",
            "Epoch 45/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1184 - accuracy: 0.9194\n",
            "Epoch 46/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1197 - accuracy: 0.9203\n",
            "Epoch 47/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1179 - accuracy: 0.9207\n",
            "Epoch 48/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1186 - accuracy: 0.9203\n",
            "Epoch 49/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1179 - accuracy: 0.9214\n",
            "Epoch 50/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1186 - accuracy: 0.9208\n",
            "Epoch 51/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1149 - accuracy: 0.9233\n",
            "Epoch 52/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1173 - accuracy: 0.9212\n",
            "Epoch 53/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1177 - accuracy: 0.9210\n",
            "Epoch 54/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1186 - accuracy: 0.9202\n",
            "Epoch 55/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1163 - accuracy: 0.9213\n",
            "Epoch 56/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1150 - accuracy: 0.9237\n",
            "Epoch 57/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1170 - accuracy: 0.9214\n",
            "Epoch 58/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1193 - accuracy: 0.9184\n",
            "Epoch 59/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1163 - accuracy: 0.9215\n",
            "Epoch 60/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1151 - accuracy: 0.9213\n",
            "Epoch 61/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1172 - accuracy: 0.9231\n",
            "Epoch 62/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1133 - accuracy: 0.9222\n",
            "Epoch 63/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1128 - accuracy: 0.9246\n",
            "Epoch 64/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1142 - accuracy: 0.9214\n",
            "Epoch 65/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1161 - accuracy: 0.9217\n",
            "Epoch 66/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1162 - accuracy: 0.9226\n",
            "Epoch 67/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1111 - accuracy: 0.9236\n",
            "Epoch 68/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1183 - accuracy: 0.9220\n",
            "Epoch 69/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1145 - accuracy: 0.9238\n",
            "Epoch 70/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1193 - accuracy: 0.9186\n",
            "Epoch 71/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1126 - accuracy: 0.9249\n",
            "Epoch 72/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1139 - accuracy: 0.9216\n",
            "Epoch 73/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1153 - accuracy: 0.9224\n",
            "Epoch 74/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1104 - accuracy: 0.9248\n",
            "Epoch 75/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1121 - accuracy: 0.9235\n",
            "Epoch 76/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1122 - accuracy: 0.9234\n",
            "Epoch 77/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1135 - accuracy: 0.9221\n",
            "Epoch 78/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1129 - accuracy: 0.9212\n",
            "Epoch 79/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1158 - accuracy: 0.9236\n",
            "Epoch 80/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1124 - accuracy: 0.9248\n",
            "Epoch 81/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1150 - accuracy: 0.9219\n",
            "Epoch 82/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1127 - accuracy: 0.9222\n",
            "Epoch 83/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1137 - accuracy: 0.9220\n",
            "Epoch 84/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1152 - accuracy: 0.9231\n",
            "Epoch 85/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1107 - accuracy: 0.9237\n",
            "Epoch 86/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1118 - accuracy: 0.9267\n",
            "Epoch 87/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1118 - accuracy: 0.9240\n",
            "Epoch 88/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1109 - accuracy: 0.9251\n",
            "Epoch 89/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1125 - accuracy: 0.9231\n",
            "Epoch 90/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1131 - accuracy: 0.9245\n",
            "Epoch 91/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1131 - accuracy: 0.9210\n",
            "Epoch 92/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1111 - accuracy: 0.9248\n",
            "Epoch 93/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1109 - accuracy: 0.9250\n",
            "Epoch 94/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1127 - accuracy: 0.9261\n",
            "Epoch 95/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1095 - accuracy: 0.9251\n",
            "Epoch 96/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1101 - accuracy: 0.9252\n",
            "Epoch 97/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1106 - accuracy: 0.9256\n",
            "Epoch 98/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1101 - accuracy: 0.9255\n",
            "Epoch 99/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1106 - accuracy: 0.9254\n",
            "Epoch 100/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1117 - accuracy: 0.9250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "163/163 [==============================] - 0s 2ms/step - loss: 0.0829 - accuracy: 0.9405\n",
            "loss_and_metrics : [0.08287062495946884, 0.9404739141464233]\n",
            "0:02:27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgMhXBG4qofz"
      },
      "source": [
        "## 이 모델들은 그냥 ㅎㅎ :)\n",
        "\n",
        "\n",
        "*   LSTM\n",
        "*   RNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OINBXL9aqof0"
      },
      "source": [
        "LSTM 이당"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYZSdLmQqof1"
      },
      "source": [
        "# 데이터 변환\n",
        "# LTSM : 입력을 (샘플 수, 타입스텝, 특성 수)로 형태 변환\n",
        "x_train = np.array(X3_train).reshape((X3_train.shape[0], X3_train.shape[1], 1))\n",
        "y_train = np.array(rorl_train).reshape((rorl_train.shape[0], 1))\n",
        "x_test = np.array(X3_test).reshape((X3_test.shape[0], X3_test.shape[1],1))\n",
        "y_test = np.array(rorl_test).reshape((rorl_test.shape[0], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAR6ZYQTrWVN",
        "outputId": "9b9df8f5-a036-44ea-bb3f-74a8f5cd503e"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31146, 65, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSjur-aFq18R",
        "outputId": "f0fd0e05-79eb-42d4-f2fa-77bdccb5c654"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31146, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "Rx7jtymXqof4",
        "outputId": "54fee722-64e9-450a-af00-64b23f45cd36"
      },
      "source": [
        "start = time.time()\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Activation, Dropout, LSTM\n",
        "\n",
        "# RNN 모델 구성\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape = (65,1), return_sequences = True ,activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(64, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(128, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(256, return_sequences = True ,activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(128, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(64, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "hist = model.fit(x_train, y_train, epochs=10, batch_size=512 )\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "61/61 [==============================] - 5s 82ms/step - loss: -0.4113 - accuracy: 0.3738\n",
            "Epoch 2/10\n",
            "61/61 [==============================] - 5s 83ms/step - loss: -0.7305 - accuracy: 0.3759\n",
            "Epoch 3/10\n",
            "61/61 [==============================] - 5s 84ms/step - loss: -0.8774 - accuracy: 0.3759\n",
            "Epoch 4/10\n",
            "61/61 [==============================] - 5s 84ms/step - loss: -1.0122 - accuracy: 0.3759\n",
            "Epoch 5/10\n",
            "61/61 [==============================] - 5s 84ms/step - loss: -1.1404 - accuracy: 0.3759\n",
            "Epoch 6/10\n",
            "61/61 [==============================] - 5s 84ms/step - loss: -1.2649 - accuracy: 0.3759\n",
            "Epoch 7/10\n",
            "61/61 [==============================] - 5s 83ms/step - loss: -1.3881 - accuracy: 0.3759\n",
            "Epoch 8/10\n",
            "61/61 [==============================] - 5s 82ms/step - loss: -1.5042 - accuracy: 0.3759\n",
            "Epoch 9/10\n",
            "61/61 [==============================] - 5s 82ms/step - loss: -1.6059 - accuracy: 0.3759\n",
            "Epoch 10/10\n",
            "61/61 [==============================] - 5s 81ms/step - loss: -1.6823 - accuracy: 0.3759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAEWCAYAAAAtuzN2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAb3UlEQVR4nO3df5xVdb3v8dcbUEZGEhiqx72MxXTiFD+CIYTDuWhoZQfk5I8o0Rtlna6cHjfNbj08Umma1TlkVkYHy/FXph48pVl44mZhInWTkpAChQKVYps9JGImfoaDn/vH3oN7YGbYM+w1+zt73s/HYx7stdZ3rf2Z9YDvm7X2d3+XIgIzM7NUDKh0AWZmZsUcTGZmlhQHk5mZJcXBZGZmSXEwmZlZUhxMZmaWFAeTmZklxcFk1glJWyXtk7RLUrOkn0n6oKQBhe3fkBSSphXt81pJUbS8UtJ+SScXrXurpK29+suY9SEOJrOuvT0ihgKvBhYBVwC3Fm3/M/DZoxxjD3BVNuWZVR8Hk1kJIqIlIpYB84CLJE0obLoDmChpZhe7LwYulPQ3WddpVg0cTGbdEBG/AHLAaYVVe4F/BT7XxW7PAjcDn862OrPq4GAy674/ACOKlm8CXiVpdhf7/BvwdknjM63MrAo4mMy6bxT5z5YAiIi/Ap8p/HQoIrYD/w5cm3l1Zn2cg8msGyRNJR9MPz1s0+3AMOAdXez+BeAMYEo21ZlVBweTWQkkvUzSPwL3AHdFxPri7RHRClxNftRehyKiGfgi8C9Z1mrW1zmYzLr2gKRdwDbgk8CXgPd30nYp8NxRjvcV4GD5yjOrPvKDAs3MLCW+YjIzs6RkFkySbpP0vKQNnWyXpMWStkj6taQ3ZlWLmZl1LaU+O8srpm8As7rYPhsYU/hZAHwtw1rMzKxr3yCRPjuzYIqIVRR916MD5wDfjLzVwDBJ/y2reszMrHMp9dmDsjhoiUaRH+nUJldYd8SoJkkLyCc0wJQhQ4ZkX52ZWRXZu3dvAGuLVjVFRFM3DlFyn32sKhlMJSucvCaA2tra2LNnT4UrMjPrWyTti4hTKl1HKSo5Ku9Z4OSi5frCOjMzS0+v9dmVDKZlwHsLIz2mAy0RUfZLQjMzK4te67Mzu5UnaSlwOjBSUo78dC3HAUTE14HlwFnAFvKPDujs2/RmZpaxlPrsPjfzQ0efMb3wwgvkcjn2799foar6vpqaGurr6znuuOMqXYqZZUDS3oiorXQdpegTgx+OJpfLMXToUEaPHo2kSpfT50QEO3bsIJfL0dDQUOlyzKyfq4opifbv309dXZ1DqYckUVdX5ytOM0tCVQQT4FA6Rj5/ZpaKqgkmMzOrDg6mMmhububGG2/s0b5nnXUWzc3NJbe/5ppruP7663v0XmZmfYGDqQy6CqbW1tYu912+fDnDhg3Loiwzsz7JwVQGCxcu5KmnnqKxsZHLL7+clStXctppp3H22Wczbtw4AM4991ymTJnC+PHjaWp6aXqq0aNH86c//YmtW7cyduxYLr74YsaPH8/b3vY29u3b1+X7rlu3junTpzNx4kTOO+88du7cCcDixYsZN24cEydO5IILLgDgkUceobGxkcbGRiZPnsyuXbsyOhtmZsemKoaLF9u8+SPs3r2urMc88cRGxoy5odPtixYtYsOGDaxbl3/flStXsnbtWjZs2HBo+PVtt93GiBEj2LdvH1OnTmXu3LnU1dUdVvtmli5dys0338z555/Pfffdx/z58zt93/e+97189atfZebMmXzqU5/i05/+NDfccAOLFi3imWeeYfDgwYduE15//fUsWbKEGTNmsHv3bmpqao71tJiZZcJXTBmZNm1au+8ELV68mEmTJjF9+nS2bdvG5s2bj9inoaGBxsZGAKZMmcLWrVs7PX5LSwvNzc3MnDkTgIsuuohVq1YBMHHiRN797ndz1113MWhQ/v8eM2bM4KMf/SiLFy+mubn50Hozs9RUXe/U1ZVNb6qtfekL1itXrmTFihU8+uijDBkyhNNPP73D7wwNHjz40OuBAwce9VZeZ77//e+zatUqHnjgAT73uc+xfv16Fi5cyJw5c1i+fDkzZszgwQcf5PWvf32Pjm9mliVfMZXB0KFDu/zMpqWlheHDhzNkyBA2bdrE6tWrj/k9TzrpJIYPH85PfvITAO68805mzpzJiy++yLZt2zjjjDP4/Oc/T0tLC7t37+app57iDW94A1dccQVTp05l06ZNx1yDmVkWqu6KqRLq6uqYMWMGEyZMYPbs2cyZM6fd9lmzZvH1r3+dsWPH8rrXvY7p06eX5X3vuOMOPvjBD7J3715e85rXcPvtt3Pw4EHmz59PS0sLEcGHP/xhhg0bxlVXXcXDDz/MgAEDGD9+PLNnzy5LDWZm5VYVk7hu3LiRsWPHVqii6uHzaFa9+tIkrr6VZ2ZmSXEwmZlZUqommPraLcnU+PyZWSqqIphqamrYsWOHO9ceansek790a2YpqIpRefX19eRyObZv317pUvqstifYmplVWlWMyjMzs655VJ6ZmVkPOZjMzCwpDiYzM0uKg8nMzJLiYDIzs6Q4mMzMLCkOJjMzS4qDyczMkuJgMjOzpDiYzMwsKQ4mMzNLioPJzMyS4mAyM7OkOJjMzCwpDiYzM0uKg8nMzJKSaTBJmiXpN5K2SFrYwfZXSXpY0uOSfi3prCzrMTOzjqXUX2f2BFtJA4HfAmcCOeAx4MKIeLKoTRPweER8TdI4YHlEjO7quH6CrZlZ93X1BNus+uueyvKKaRqwJSKejogDwD3AOYe1CeBlhdcnAX/IsB4zM+tYUv31oKwODIwCthUt54C/O6zNNcAPJV0K1AJv7ehAkhYACwCOP/74shdqZtYPDJK0pmi5KSKaCq/L1l+XQ6UHP1wIfCMi6oGzgDslHVFTRDRFxCkRccqgQVlmqZlZ1Wpt60cLP01H36WdkvrrcsgymJ4FTi5ari+sK/YB4FsAEfEoUAOMzLAmMzM7UlL9dZbB9BgwRlKDpOOBC4Blh7X5PfAWAEljyf+i2zOsyczMjpRUf51ZMEVEK3AJ8CCwEfhWRDwh6VpJZxeafQy4WNKvgKXA+yKrYYJmZtah1PrrzIaLZ8XDxc3Muq+r4eKpqfTgBzMzs3YcTGZmlhQHk5mZJcXBZGZmSXEwmZlZUhxMZmaWFAeTmZklxcFkZmZJcTCZmVlSHExmZpYUB5OZmSXFwWRmZklxMJmZWVIcTGZmlhQHk5mZJcXBZGZmSXEwmZlZUhxMZmaWFAeTmZklxcFkZmZJcTCZmVlSHExmZpYUB5OZmSXFwWRmZklxMJmZWVIcTGZmlhQHk5mZJcXBZGZmSXEwmZlZUhxMZmaWFAeTmZklxcFkZmZJcTCZmVlSHExmZpaUTINJ0ixJv5G0RdLCTtqcL+lJSU9I+o8s6zEzs46l1F8rIrI5sDQQ+C1wJpADHgMujIgni9qMAb4FvDkidkp6RUQ839Vxa2trY8+ePZnUbGZWrSTtjYjaTrZl0l/3VJZXTNOALRHxdEQcAO4BzjmszcXAkojYCZDVL2lmZl1Kqr/OMphGAduKlnOFdcX+FvhbSf9P0mpJszo6kKQFktZIWtPa2ppRuWZmVW1QWz9a+FlQtK1s/XVZCs3qwN14/zHA6UA9sErSGyKiubhRRDQBTZC/ldfbRZqZVYHWiDjlGPYvqb8uhyyvmJ4FTi5ari+sK5YDlkXECxHxDPl7nGMyrMnMzI6UVH+dZTA9BoyR1CDpeOACYNlhbb5LPn2RNJL8peLTGdZkZmZHSqq/ziyYIqIVuAR4ENgIfCsinpB0raSzC80eBHZIehJ4GLg8InZkVZOZmR0ptf46s+HiWfFwcTOz7utquHhG7/eGiFjfk30984OZmWXhRkm/kPS/JZ3UnR0dTGZmVnYRcRrwbvKDKn4p6T8knVnKvr6VZ2bWD/T2rbyi9x0InAssBv4CCPhERHyns318xWRmZmUnaaKkL5MfTPFm4O0RMbbw+std7VvpL9iamVl1+ipwC/mro31tKyPiD5Ku7GpH38ozM+sHKnUrryd8xWRmZmVXmI3834BxQE3b+oh4zdH29WdMZmaWhduBrwGtwBnAN4G7StnRwWRmZlk4ISIeIv+R0e8i4hpgTik7+laemZll4a+SBgCbJV1CflLYE0vZsaQrJkmXSXqZ8m6VtFbS246hYDMzq26XAUOADwNTgPnARaXsWOqtvH+KiL8AbwOGA+8BFnW/TjMzq3aFL9XOi4jdEZGLiPdHxNyIWF3K/qUGkwp/ngXcGRFPFK0zMzM7JCIOAqf2dP9SP2P6paQfAg3AxyUNBV7s6ZuamVnVe1zSMuDbwKEvn3Y1FVGbUoPpA0Aj8HRE7JU0Anh/Tyo1M7N+oQbYQX4KojYBlC2Y/h5YFxF7JM0H3gh8pbtVmplZ/xARPb54KTWYvgZMkjQJ+Bj5+Y++Cczs6RubmVn1knQ7+SukdiLin462b6nB1BoRIekc4N8j4lZJH+hmnWZm1n/8V9HrGuA84A+l7FhqMO2S9HHyw8RPK3xp6rhulWhmZv1GRNxXvCxpKfDTUvYtdbj4POCv5L/P9EegHvhCd4o0M7N+bQzwilIalvzYC0mvBKYWFn8REc/3rLZj48demJl1X28/9kLSLtp/xvRH4OOHX0l1pKRbeZLOJ3+FtJL8F2u/KunyiLi3++WamVm1i4ihPd231M+YPglMbbtKkvRyYAXgYDIzsyNIOg/4cUS0FJaHAadHxHePtm+pnzENOOzW3Y5u7GtmZv3P1W2hBBARzcDVpexY6hXTDyQ9CCwtLM8DlnerRDMz6086ungp7eOjbgx+mAvMKCz+JCLuL6228vLgBzOz7qvA4IfbgGZgSWHVh4AREfG+o+5bajClwsFkZtZ9FQimWuAq4K3kR+f9CPhcRBy1A+8ymDoY7ndoExAR8bIeVXwMHExmZt3X28F0LLocwBARQyPiZR38DK1EKJmZWd8g6UeFkXhty8MLYxWOyiPrzMwsCyMLI/EAiIidlDjzg4PJzMyy8KKkV7UtSBpNxx8NHaHU4eJmZmbd8Ungp5IeIT8u4TRgQSk7elSemVk/UInBD5JeQT6MHgdOAJ6PiFVH289XTGZmVnaS/hdwGfmnUawDpgOP0v5R6x3yZ0xmZpaFy8g/keJ3EXEGMJn8F26PKtNgkjRL0m8kbZG0sIt2cyWFpFOyrMfMzDqWQX+9PyL2F/YZHBGbgNeVUktmt/IkDSQ/FcWZQA54TNKyiHjysHZDySfrz7OqxczMOpdRf50rfI/pu8CPJO0EfldKPVleMU0DtkTE0xFxALgHOKeDdp8BPg/sz7AWMzPrXNn764g4LyKaI+Ia8lMT3QqcW0oxWQbTKGBb0XKusO4QSW8ETo6I73d1IEkLJK2RtKa1tbX8lZqZVb9Bbf1o4ad46HbZ+uuORMQjEbGsEHpHL7S7b1AukgYAXwLed7S2EdEENEF+uHi2lZmZVaXWiOjR5/jd6a/LIcsrpmeBk4uW6wvr2gwFJgArJW0lP5RwmQdAmJn1uqT66yyD6TFgjKQGSccDFwDL2jZGREtEjIyI0RExGlgNnB0RazKsyczMjpRUf53ZrbyIaJV0CfAgMBC4LSKekHQtsCYilnV9hPJ69lnYuhVaW+HgwfxP2+vO/jzWNp1t62OTbZhZIv75n+Ef/qH8x02tv+43UxJddx1ccUX56xkwAAYOhEGD2v/Z0bq2Pwf4a81m1gNXXgnnn9+zffvS85j6TTA98wxs3tx5WJQSKB21kTL4Jc3MyszBlCFP4mpm1n19KZh8U8nMzJLiYDIzs6Q4mMzMLCkOJjMzS4qDyczMkuJgMjOzpDiYzMwsKQ4mMzNLioPJzMyS4mAyM7OkOJjMzCwpDiYzM0uKg8nMzJLiYDIzs6Q4mMzMLCkOJjMzS4qDyczMkuJgMjOzpDiYzMwsKQ4mMzNLioPJzMyS4mAyM7OkOJjMzCwpDiYzM0uKg8nMzJLiYDIzs6Q4mMzMLCkOJjMzS4qDyczMkuJgMjOzpDiYzMwsKQ4mMzNLSqbBJGmWpN9I2iJpYQfbPyrpSUm/lvSQpFdnWY+ZmXUspf46s2CSNBBYAswGxgEXShp3WLPHgVMiYiJwL3BdVvWYmVnHUuuvs7ximgZsiYinI+IAcA9wTnGDiHg4IvYWFlcD9RnWY2ZmHUuqv84ymEYB24qWc4V1nfkA8H872iBpgaQ1kta0traWsUQzs35jUFs/WvhZULStbP11OQzK6sDdIWk+cAows6PtEdEENAHU1tZGL5ZmZlYtWiPilGM9yNH663LIMpieBU4uWq4vrGtH0luBTwIzI+KvGdZjZmYdS6q/zvJW3mPAGEkNko4HLgCWFTeQNBm4CTg7Ip7PsBYzM+tcUv11ZsEUEa3AJcCDwEbgWxHxhKRrJZ1daPYF4ETg25LWSVrWyeHMzCwjqfXXiuhbH9nU1tbGnj17Kl2GmVmfImlvRNRWuo5SeOYHMzNLioPJzMyS4mAyM7OkOJjMzCwpDiYzM0uKg8nMzJLiYDIzs6Q4mMzMLCkOJjMzS4qDyczMkuJgMjOzpDiYzMwsKQ4mMzNLioPJzMyS4mAyM7OkZPlo9V7zwgsvkMvl2L9/f6VL6XNqamqor6/nuOOOq3QpZmZAlQRTLpdj6NChjB49GkmVLqfPiAh27NhBLpejoaGh0uWYmQFVcitv//791NXVOZS6SRJ1dXW+0jSzpFRFMAEOpR7yeTOz1FRNMJmZWXVwMJVBc3MzN954Y4/2Peuss2hubi5zRWZmfZeDqQy6CqbW1tYu912+fDnDhg3Loiwzsz6pKkblFfvIR2DduvIes7ERbrih8+0LFy7kqaeeorGxkTPPPJM5c+Zw1VVXMXz4cDZt2sRvf/tbzj33XLZt28b+/fu57LLLWLBgAQCjR49mzZo17N69m9mzZ3Pqqafys5/9jFGjRvG9732PE044od17PfDAA3z2s5/lwIED1NXVcffdd/PKV76S3bt3c+mll7JmzRokcfXVVzN37lx+8IMf8IlPfIKDBw8ycuRIHnroofKeHDOzMqu6YKqERYsWsWHDBtYVEnHlypWsXbuWDRs2HBqGfdtttzFixAj27dvH1KlTmTt3LnV1de2Os3nzZpYuXcrNN9/M+eefz3333cf8+fPbtTn11FNZvXo1krjlllu47rrr+OIXv8hnPvMZTjrpJNavXw/Azp072b59OxdffDGrVq2ioaGBP//5z71wNszMjk3VBVNXVza9adq0ae2+G7R48WLuv/9+ALZt28bmzZuPCKaGhgYaGxsBmDJlClu3bj3iuLlcjnnz5vHcc89x4MCBQ++xYsUK7rnnnkPthg8fzgMPPMCb3vSmQ21GjBhR1t/RzCwL/owpI7W1tYder1y5khUrVvDoo4/yq1/9ismTJ3f43aHBgwcfej1w4MAOP5+69NJLueSSS1i/fj033XSTv4NkZlXHwVQGQ4cOZdeuXZ1ub2lpYfjw4QwZMoRNmzaxevXqHr9XS0sLo0aNAuCOO+44tP7MM89kyZIlh5Z37tzJ9OnTWbVqFc888wyAb+WZWZ/gYCqDuro6ZsyYwYQJE7j88suP2D5r1ixaW1sZO3YsCxcuZPr06T1+r2uuuYZ3vetdTJkyhZEjRx5af+WVV7Jz504mTJjApEmTePjhh3n5y19OU1MT73jHO5g0aRLz5s3r8fuamfUWRUSla+iW2tra2LNnT7t1GzduZOzYsRWqqO/z+TOrfpL2RkTt0VtWnq+YzMwsKQ4mMzNLStUEU1+7JZkKnzczS01VBFNNTQ07duxwJ9tNbc9jqqmpqXQpZmaHVMUXbOvr68nlcmzfvr3SpfQ5bU+wNTNLRVWMyjMzs655VF6BpFmSfiNpi6SFHWwfLOk/C9t/Lml0lvWYmVnHUuqvMwsmSQOBJcBsYBxwoaRxhzX7ALAzIl4LfBn4fFb1mJlZx1Lrr7O8YpoGbImIpyPiAHAPcM5hbc4B2ubVuRd4i/ysbzOz3pZUf53l4IdRwLai5Rzwd521iYhWSS1AHfCn4kaSFgALCoshaV8PaxoEdP3kvv7F56M9n4+X+Fy0Vw3n4wRJa4qWmyKiqfC6bP11OfSJUXmFk9d01IZHIWlNRJxShpKqgs9Hez4fL/G5aM/no3dleSvvWeDkouX6wroO20gaBJwE7MiwJjMzO1JS/XWWwfQYMEZSg6TjgQuAZYe1WQZcVHj9TuDH0dfGr5uZ9X1J9deZ3cor3IO8BHgQGAjcFhFPSLoWWBMRy4BbgTslbQH+TP5kZOmYbwdWGZ+P9nw+XuJz0V5Vn4/U+us+9wVbMzOrblUxV56ZmVUPB5OZmSWl3wTT0abb6C8knSzpYUlPSnpC0mWVrikFkgZKelzSf1W6lkqTNEzSvZI2Sdoo6e8rXVOlSPo/hX8nGyQtleSp+HtBvwimEqfb6C9agY9FxDhgOvChfnwuil0GbKx0EYn4CvCDiHg9MIl+el4kjQI+DJwSERPIDwrIeoCW0U+CidKm2+gXIuK5iFhbeL2LfKczqrJVVZakemAOcEula6k0SScBbyI/AouIOBARzZWtqqIGkZ8xYRAwBPhDhevpF/pLMHU03Ua/7owBCrMDTwZ+XtlKKu4G4F+AFytdSAIagO3A7YVbm7dI6hOPSii3iHgWuB74PfAc0BIRP6xsVf1DfwkmO4ykE4H7gI9ExF8qXU+lSPpH4PmI+GWla0nEIOCNwNciYjKwB+iXn8lKGk7+zkoD8N+BWknzK1tV/9BfgqmU6Tb6DUnHkQ+luyPiO5Wup8JmAGdL2kr+Fu+bJd1V2ZIqKgfkIqLtKvpe8kHVH70VeCYitkfEC8B3gP9R4Zr6hf4STKVMt9EvFKapvxXYGBFfqnQ9lRYRH4+I+ogYTf7vxY8jot/+rzgi/ghsk/S6wqq3AE9WsKRK+j0wXdKQwr+bt9BPB4L0tj4xu/ix6my6jQqXVSkzgPcA6yWtK6z7REQsr2BNlpZLgbsL/4l7Gnh/heupiIj4uaR7gbXkR7M+TpVPTZQKT0lkZmZJ6S+38szMrI9wMJmZWVIcTGZmlhQHk5mZJcXBZGZmSXEwmfUiSad7BnOzrjmYzMwsKQ4msw5Imi/pF5LWSbqp8Lym3ZK+XHg+z0OSXl5o2yhptaRfS7q/MMcakl4raYWkX0laK+lvCoc/seh5R3cXZhUwswIHk9lhJI0F5gEzIqIROAi8G6gF1kTEeOAR4OrCLt8EroiIicD6ovV3A0siYhL5OdaeK6yfDHyE/LPBXkN+Ng4zK+gXUxKZddNbgCnAY4WLmROA58k/FuM/C23uAr5TeH7RsIh4pLD+DuDbkoYCoyLifoCI2A9QON4vIiJXWF4HjAZ+mv2vZdY3OJjMjiTgjoj4eLuV0lWHtevpfF5/LXp9EP87NGvHt/LMjvQQ8E5JrwCQNELSq8n/e3lnoc3/BH4aES3ATkmnFda/B3ik8HTgnKRzC8cYLGlIr/4WZn2U/6dmdpiIeFLSlcAPJQ0AXgA+RP6hedMK254n/zkUwEXA1wvBUzwb93uAmyRdWzjGu3rx1zDrszy7uFmJJO2OiBMrXYdZtfOtPDMzS4qvmMzMLCm+YjIzs6Q4mMzMLCkOJjMzS4qDyczMkuJgMjOzpPx/z3YF1zwvRQwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "325/325 [==============================] - 4s 12ms/step - loss: -1.8046 - accuracy: 0.3724\n",
            "loss_and_metrics : [-1.8046051263809204, 0.3723752796649933]\n",
            "0:01:05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm4NmvxFqof7"
      },
      "source": [
        "RNN 이당"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHzM_P_-qof7"
      },
      "source": [
        "# 데이터 변환\n",
        "# LTSM : 입력을 (샘플 수, 타입스텝, 특성 수)로 형태 변환\n",
        "x_train = np.array(X3_train).reshape((X3_train.shape[0], X3_train.shape[1], 1))\n",
        "y_train = np.array(rorl_train).reshape((rorl_train.shape[0], 1))\n",
        "x_test = np.array(X3_test).reshape((X3_test.shape[0], X3_test.shape[1],1))\n",
        "y_test = np.array(rorl_test).reshape((rorl_test.shape[0], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "V_-ZCaSAqof-",
        "outputId": "779a2433-e1f8-47de-ab55-9a4cacaee863"
      },
      "source": [
        "start = time.time()\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Activation, Dropout, LSTM\n",
        "\n",
        "# RNN 모델 구성\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(32,input_shape = (65,1), return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(SimpleRNN(128, return_sequences = True ,activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(SimpleRNN(256, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(SimpleRNN(128, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(SimpleRNN(64, return_sequences = True, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "hist = model.fit(x_train, y_train, epochs=10, batch_size=512 )\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "61/61 [==============================] - 12s 197ms/step - loss: -0.3861 - accuracy: 0.3694\n",
            "Epoch 2/10\n",
            "61/61 [==============================] - 12s 195ms/step - loss: -0.7649 - accuracy: 0.3759\n",
            "Epoch 3/10\n",
            "61/61 [==============================] - 12s 205ms/step - loss: -0.9105 - accuracy: 0.3759\n",
            "Epoch 4/10\n",
            "61/61 [==============================] - 12s 201ms/step - loss: -1.0423 - accuracy: 0.3759\n",
            "Epoch 5/10\n",
            "61/61 [==============================] - 12s 199ms/step - loss: -1.1706 - accuracy: 0.3759\n",
            "Epoch 6/10\n",
            "61/61 [==============================] - 12s 205ms/step - loss: -1.2954 - accuracy: 0.3759\n",
            "Epoch 7/10\n",
            "61/61 [==============================] - 13s 206ms/step - loss: -1.4162 - accuracy: 0.3759\n",
            "Epoch 8/10\n",
            "61/61 [==============================] - 13s 205ms/step - loss: -1.5300 - accuracy: 0.3759\n",
            "Epoch 9/10\n",
            "61/61 [==============================] - 12s 200ms/step - loss: -1.6257 - accuracy: 0.3759\n",
            "Epoch 10/10\n",
            "61/61 [==============================] - 12s 202ms/step - loss: -1.6988 - accuracy: 0.3759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "325/325 [==============================] - 9s 26ms/step - loss: -1.8279 - accuracy: 0.3724\n",
            "loss_and_metrics : [-1.827858805656433, 0.3723752796649933]\n",
            "0:02:17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzKp2ZdPYHIw"
      },
      "source": [
        "# 4단계로 나누어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S7LiRDeZlRM",
        "outputId": "71a77a1b-a7aa-4064-87c6-7308de42e336"
      },
      "source": [
        "# 데이터 로드\n",
        "# Refrigerant overcharge\n",
        "start = time.time()\n",
        "ro10 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro10.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro20.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro30.xls\",sheet_name=\"Complete Data Set\")\n",
        "ro40 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/3_Refrigerant overcharge/ro40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"ro\",  clock(start) )\n",
        "\n",
        "# Refrigerant leak\n",
        "start = time.time()\n",
        "rl10 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl10.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl20 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl20.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl30 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl30.xls\",sheet_name=\"Complete Data Set\")\n",
        "rl40 = pd.read_excel(\"/content/drive/My Drive/1043-RP/FDD Data/8_Refrigerant leak/rl40.xls\",sheet_name=\"Complete Data Set\")\n",
        "print(\"rl\",  clock(start) )\n",
        "\n",
        "# 심각도 수준 적기\n",
        "ro10['severity'] = 1\n",
        "ro20['severity'] = 2\n",
        "ro30['severity'] = 3\n",
        "ro40['severity'] = 4\n",
        "\n",
        "rl10['severity'] = 1\n",
        "rl20['severity'] = 2\n",
        "rl30['severity'] = 3\n",
        "rl40['severity'] = 4\n",
        "\n",
        "#\n",
        "ro = pd.concat([ro20,ro30,ro40], axis=0)\n",
        "rl= pd.concat([rl20,rl30,rl40], axis=0)\n",
        "\n",
        "# 1은 ro, 2는 rl\n",
        "ro['rorl'] = 2  \n",
        "rl['rorl'] = 3  \n",
        "\n",
        "# 0은 심각도 1단계들만 묶어서\n",
        "ro10['rorl'] = 0  \n",
        "rl10['rorl'] = 1 \n",
        "\n",
        "#\n",
        "ro = pd.concat([ro10,ro], axis=0)\n",
        "rl= pd.concat([rl10,rl], axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ro 0:00:03\n",
            "rl 0:00:03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "hDyPV3jvZlRT",
        "outputId": "e0159ee0-c986-4cf9-b3ae-8626b92f9742"
      },
      "source": [
        "data1 = pd.concat([ro, rl], axis=0)\n",
        "data1 = data1.drop(['Time'],axis=1)\n",
        "data1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TWE_set</th>\n",
              "      <th>TEI</th>\n",
              "      <th>TWEI</th>\n",
              "      <th>TEO</th>\n",
              "      <th>TWEO</th>\n",
              "      <th>TCI</th>\n",
              "      <th>TWCI</th>\n",
              "      <th>TCO</th>\n",
              "      <th>TWCO</th>\n",
              "      <th>TSI</th>\n",
              "      <th>TSO</th>\n",
              "      <th>TBI</th>\n",
              "      <th>TBO</th>\n",
              "      <th>Cond Tons</th>\n",
              "      <th>Cooling Tons</th>\n",
              "      <th>Shared Cond Tons</th>\n",
              "      <th>Cond Energy Balance</th>\n",
              "      <th>Evap Tons</th>\n",
              "      <th>Shared Evap Tons</th>\n",
              "      <th>Building Tons</th>\n",
              "      <th>Evap Energy Balance</th>\n",
              "      <th>kW</th>\n",
              "      <th>COP</th>\n",
              "      <th>kW/Ton</th>\n",
              "      <th>FWC</th>\n",
              "      <th>FWE</th>\n",
              "      <th>TEA</th>\n",
              "      <th>TCA</th>\n",
              "      <th>TRE</th>\n",
              "      <th>PRE</th>\n",
              "      <th>TRC</th>\n",
              "      <th>PRC</th>\n",
              "      <th>TRC_sub</th>\n",
              "      <th>T_suc</th>\n",
              "      <th>Tsh_suc</th>\n",
              "      <th>TR_dis</th>\n",
              "      <th>Tsh_dis</th>\n",
              "      <th>P_lift</th>\n",
              "      <th>Amps</th>\n",
              "      <th>RLA%</th>\n",
              "      <th>Heat Balance (kW)</th>\n",
              "      <th>Heat Balance%</th>\n",
              "      <th>Tolerance%</th>\n",
              "      <th>Unit Status</th>\n",
              "      <th>Active Fault</th>\n",
              "      <th>TO_sump</th>\n",
              "      <th>TO_feed</th>\n",
              "      <th>PO_feed</th>\n",
              "      <th>PO_net</th>\n",
              "      <th>TWCD</th>\n",
              "      <th>TWED</th>\n",
              "      <th>VSS</th>\n",
              "      <th>VSL</th>\n",
              "      <th>VH</th>\n",
              "      <th>VM</th>\n",
              "      <th>VC</th>\n",
              "      <th>VE</th>\n",
              "      <th>VW</th>\n",
              "      <th>TWI</th>\n",
              "      <th>TWO</th>\n",
              "      <th>THI</th>\n",
              "      <th>THO</th>\n",
              "      <th>FWW</th>\n",
              "      <th>FWH</th>\n",
              "      <th>FWB</th>\n",
              "      <th>severity</th>\n",
              "      <th>rorl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>70.67</td>\n",
              "      <td>70.6</td>\n",
              "      <td>71.76</td>\n",
              "      <td>71.7</td>\n",
              "      <td>71.73</td>\n",
              "      <td>71.5</td>\n",
              "      <td>72.59</td>\n",
              "      <td>72.5</td>\n",
              "      <td>72.24</td>\n",
              "      <td>73.22</td>\n",
              "      <td>70.89</td>\n",
              "      <td>71.37</td>\n",
              "      <td>1.383000e-46</td>\n",
              "      <td>5.720000e-47</td>\n",
              "      <td>1.590000e-46</td>\n",
              "      <td>2.400000e-46</td>\n",
              "      <td>1.407000e-46</td>\n",
              "      <td>-1.122000e-46</td>\n",
              "      <td>6.212000e-47</td>\n",
              "      <td>9.067000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>4.949000e-46</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.6</td>\n",
              "      <td>73.5</td>\n",
              "      <td>71.2</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>84.7</td>\n",
              "      <td>13.1</td>\n",
              "      <td>75.2</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>114.7</td>\n",
              "      <td>90.7</td>\n",
              "      <td>69.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>66.70</td>\n",
              "      <td>71.87</td>\n",
              "      <td>70.65</td>\n",
              "      <td>71.54</td>\n",
              "      <td>2.655000e-46</td>\n",
              "      <td>1.491000e-45</td>\n",
              "      <td>1.175000e-45</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>70.67</td>\n",
              "      <td>70.8</td>\n",
              "      <td>71.76</td>\n",
              "      <td>71.7</td>\n",
              "      <td>71.73</td>\n",
              "      <td>71.7</td>\n",
              "      <td>72.55</td>\n",
              "      <td>72.3</td>\n",
              "      <td>72.28</td>\n",
              "      <td>73.27</td>\n",
              "      <td>70.89</td>\n",
              "      <td>71.37</td>\n",
              "      <td>1.317000e-46</td>\n",
              "      <td>4.399000e-47</td>\n",
              "      <td>1.590000e-46</td>\n",
              "      <td>2.466000e-46</td>\n",
              "      <td>1.407000e-46</td>\n",
              "      <td>-1.122000e-46</td>\n",
              "      <td>6.212000e-47</td>\n",
              "      <td>9.067000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>4.949000e-46</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.6</td>\n",
              "      <td>73.5</td>\n",
              "      <td>71.2</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>84.7</td>\n",
              "      <td>13.1</td>\n",
              "      <td>75.2</td>\n",
              "      <td>3.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>114.5</td>\n",
              "      <td>90.8</td>\n",
              "      <td>69.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>66.70</td>\n",
              "      <td>71.87</td>\n",
              "      <td>70.65</td>\n",
              "      <td>71.58</td>\n",
              "      <td>2.042000e-46</td>\n",
              "      <td>1.491000e-45</td>\n",
              "      <td>1.175000e-45</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50</td>\n",
              "      <td>70.67</td>\n",
              "      <td>70.9</td>\n",
              "      <td>71.76</td>\n",
              "      <td>71.8</td>\n",
              "      <td>71.73</td>\n",
              "      <td>72.0</td>\n",
              "      <td>72.59</td>\n",
              "      <td>72.0</td>\n",
              "      <td>72.28</td>\n",
              "      <td>73.27</td>\n",
              "      <td>70.89</td>\n",
              "      <td>71.37</td>\n",
              "      <td>8.520000e+00</td>\n",
              "      <td>3.118000e+00</td>\n",
              "      <td>9.796000e+00</td>\n",
              "      <td>1.520000e+01</td>\n",
              "      <td>6.133000e+00</td>\n",
              "      <td>-4.889000e+00</td>\n",
              "      <td>2.707000e+00</td>\n",
              "      <td>3.951000e+00</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>2.156000e+01</td>\n",
              "      <td>2.742000e-46</td>\n",
              "      <td>2.383000e+02</td>\n",
              "      <td>1.350000e+02</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.1</td>\n",
              "      <td>72.5</td>\n",
              "      <td>71.9</td>\n",
              "      <td>74.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>84.7</td>\n",
              "      <td>13.6</td>\n",
              "      <td>75.4</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-8.392</td>\n",
              "      <td>-28.01</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>114.5</td>\n",
              "      <td>90.8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>66.70</td>\n",
              "      <td>71.87</td>\n",
              "      <td>70.69</td>\n",
              "      <td>71.54</td>\n",
              "      <td>1.447000e+01</td>\n",
              "      <td>6.496000e+01</td>\n",
              "      <td>1.210000e+02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50</td>\n",
              "      <td>70.75</td>\n",
              "      <td>71.5</td>\n",
              "      <td>71.76</td>\n",
              "      <td>71.4</td>\n",
              "      <td>71.82</td>\n",
              "      <td>71.7</td>\n",
              "      <td>72.51</td>\n",
              "      <td>72.0</td>\n",
              "      <td>72.24</td>\n",
              "      <td>73.06</td>\n",
              "      <td>70.97</td>\n",
              "      <td>71.37</td>\n",
              "      <td>7.644000e+00</td>\n",
              "      <td>3.006000e+00</td>\n",
              "      <td>9.060000e+00</td>\n",
              "      <td>1.370000e+01</td>\n",
              "      <td>8.448000e+00</td>\n",
              "      <td>-6.594000e+00</td>\n",
              "      <td>3.345000e+00</td>\n",
              "      <td>5.198000e+00</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>2.970000e+01</td>\n",
              "      <td>1.991000e-46</td>\n",
              "      <td>2.643000e+02</td>\n",
              "      <td>2.010000e+02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.4</td>\n",
              "      <td>73.3</td>\n",
              "      <td>71.6</td>\n",
              "      <td>73.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>84.8</td>\n",
              "      <td>13.4</td>\n",
              "      <td>75.4</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.824</td>\n",
              "      <td>10.51</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>113.8</td>\n",
              "      <td>90.8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>66.70</td>\n",
              "      <td>71.83</td>\n",
              "      <td>71.59</td>\n",
              "      <td>71.41</td>\n",
              "      <td>1.406000e+01</td>\n",
              "      <td>8.028000e+01</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>70.88</td>\n",
              "      <td>71.5</td>\n",
              "      <td>71.68</td>\n",
              "      <td>71.5</td>\n",
              "      <td>71.86</td>\n",
              "      <td>71.8</td>\n",
              "      <td>72.43</td>\n",
              "      <td>71.8</td>\n",
              "      <td>72.20</td>\n",
              "      <td>72.73</td>\n",
              "      <td>71.02</td>\n",
              "      <td>71.41</td>\n",
              "      <td>6.385000e+00</td>\n",
              "      <td>2.593000e+00</td>\n",
              "      <td>5.991000e+00</td>\n",
              "      <td>9.783000e+00</td>\n",
              "      <td>7.115000e+00</td>\n",
              "      <td>-5.881000e+00</td>\n",
              "      <td>3.534000e+00</td>\n",
              "      <td>4.768000e+00</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>2.502000e+01</td>\n",
              "      <td>2.363000e-46</td>\n",
              "      <td>2.682000e+02</td>\n",
              "      <td>2.123000e+02</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.1</td>\n",
              "      <td>72.8</td>\n",
              "      <td>71.6</td>\n",
              "      <td>73.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>84.9</td>\n",
              "      <td>13.8</td>\n",
              "      <td>75.2</td>\n",
              "      <td>3.8</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.566</td>\n",
              "      <td>11.43</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>113.8</td>\n",
              "      <td>90.8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>56</td>\n",
              "      <td>30.0</td>\n",
              "      <td>66.46</td>\n",
              "      <td>71.75</td>\n",
              "      <td>71.47</td>\n",
              "      <td>71.37</td>\n",
              "      <td>1.176000e+01</td>\n",
              "      <td>8.481000e+01</td>\n",
              "      <td>1.271000e+02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5186</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.1</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.40</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.85</td>\n",
              "      <td>57.00</td>\n",
              "      <td>5.248000e-47</td>\n",
              "      <td>2.419000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>4.042000e-48</td>\n",
              "      <td>1.984000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.4</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>57.3</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.7</td>\n",
              "      <td>17.8</td>\n",
              "      <td>97.3</td>\n",
              "      <td>39.8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>106.3</td>\n",
              "      <td>106.5</td>\n",
              "      <td>49.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.60</td>\n",
              "      <td>57.18</td>\n",
              "      <td>57.20</td>\n",
              "      <td>5.806000e-46</td>\n",
              "      <td>4.762000e-46</td>\n",
              "      <td>3.771000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5187</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>56.9</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.8</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.36</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.29</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>56.96</td>\n",
              "      <td>4.588000e-47</td>\n",
              "      <td>1.100000e-47</td>\n",
              "      <td>4.557000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>9.272000e-48</td>\n",
              "      <td>1.671000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>57.3</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.6</td>\n",
              "      <td>17.8</td>\n",
              "      <td>97.3</td>\n",
              "      <td>40.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>106.3</td>\n",
              "      <td>106.7</td>\n",
              "      <td>49.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.60</td>\n",
              "      <td>57.18</td>\n",
              "      <td>57.25</td>\n",
              "      <td>2.641000e-46</td>\n",
              "      <td>2.225000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5188</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.1</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.2</td>\n",
              "      <td>57.40</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>57.00</td>\n",
              "      <td>5.248000e-47</td>\n",
              "      <td>2.419000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>1.456000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.7</td>\n",
              "      <td>56.9</td>\n",
              "      <td>53.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.3</td>\n",
              "      <td>17.5</td>\n",
              "      <td>97.0</td>\n",
              "      <td>39.9</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>106.1</td>\n",
              "      <td>106.5</td>\n",
              "      <td>49.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.64</td>\n",
              "      <td>57.18</td>\n",
              "      <td>57.29</td>\n",
              "      <td>5.806000e-46</td>\n",
              "      <td>3.494000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5189</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.1</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.9</td>\n",
              "      <td>57.04</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.40</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>57.00</td>\n",
              "      <td>5.907000e-47</td>\n",
              "      <td>2.419000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-4.109000e-48</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>1.456000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>56.9</td>\n",
              "      <td>53.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.3</td>\n",
              "      <td>17.4</td>\n",
              "      <td>96.8</td>\n",
              "      <td>40.1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>105.9</td>\n",
              "      <td>106.5</td>\n",
              "      <td>49.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.60</td>\n",
              "      <td>57.14</td>\n",
              "      <td>57.29</td>\n",
              "      <td>5.806000e-46</td>\n",
              "      <td>3.494000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5190</th>\n",
              "      <td>40</td>\n",
              "      <td>56.83</td>\n",
              "      <td>57.2</td>\n",
              "      <td>56.82</td>\n",
              "      <td>56.8</td>\n",
              "      <td>57.08</td>\n",
              "      <td>57.1</td>\n",
              "      <td>57.36</td>\n",
              "      <td>57.2</td>\n",
              "      <td>57.25</td>\n",
              "      <td>57.01</td>\n",
              "      <td>56.89</td>\n",
              "      <td>57.00</td>\n",
              "      <td>4.588000e-47</td>\n",
              "      <td>1.759000e-47</td>\n",
              "      <td>3.898000e-47</td>\n",
              "      <td>-1.070000e-47</td>\n",
              "      <td>1.890000e-48</td>\n",
              "      <td>9.327000e-48</td>\n",
              "      <td>1.456000e-47</td>\n",
              "      <td>2.199000e-47</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>6.646000e-48</td>\n",
              "      <td>1.682000e-45</td>\n",
              "      <td>3.868000e-45</td>\n",
              "      <td>3.097000e-45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>56.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>57.3</td>\n",
              "      <td>54.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.1</td>\n",
              "      <td>17.5</td>\n",
              "      <td>96.8</td>\n",
              "      <td>39.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.27</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>105.9</td>\n",
              "      <td>106.3</td>\n",
              "      <td>49.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "      <td>100.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.98</td>\n",
              "      <td>56.64</td>\n",
              "      <td>57.10</td>\n",
              "      <td>57.33</td>\n",
              "      <td>4.222000e-46</td>\n",
              "      <td>3.494000e-46</td>\n",
              "      <td>3.644000e-45</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41528 rows × 67 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      TWE_set    TEI  TWEI    TEO  ...           FWH           FWB  severity  rorl\n",
              "0          50  70.67  70.6  71.76  ...  1.491000e-45  1.175000e-45         1     0\n",
              "1          50  70.67  70.8  71.76  ...  1.491000e-45  1.175000e-45         1     0\n",
              "2          50  70.67  70.9  71.76  ...  6.496000e+01  1.210000e+02         1     0\n",
              "3          50  70.75  71.5  71.76  ...  8.028000e+01  1.060000e+02         1     0\n",
              "4          50  70.88  71.5  71.68  ...  8.481000e+01  1.271000e+02         1     0\n",
              "...       ...    ...   ...    ...  ...           ...           ...       ...   ...\n",
              "5186       40  56.83  57.1  56.82  ...  4.762000e-46  3.771000e-45         4     3\n",
              "5187       40  56.83  56.9  56.82  ...  2.225000e-46  3.644000e-45         4     3\n",
              "5188       40  56.83  57.1  56.82  ...  3.494000e-46  3.644000e-45         4     3\n",
              "5189       40  56.83  57.1  56.82  ...  3.494000e-46  3.644000e-45         4     3\n",
              "5190       40  56.83  57.2  56.82  ...  3.494000e-46  3.644000e-45         4     3\n",
              "\n",
              "[41528 rows x 67 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yVURhGXZlRa"
      },
      "source": [
        "# 표준화\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(data1.iloc[:,:-2])\n",
        "X_scaled = scaler.transform(data1.iloc[:,:-2])\n",
        "X_scaled = pd.DataFrame(X_scaled).reset_index(drop=True)\n",
        "\n",
        "y_rorl_sev = pd.DataFrame(data1.iloc[:,-2:]).reset_index(drop=True)\n",
        "\n",
        "X = pd.concat([X_scaled, y_rorl_sev], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmPENNWbZlRj"
      },
      "source": [
        "X2 = X.iloc[:,:-1]\n",
        "y_rorl = X.iloc[:,-2:]\n",
        "y_sev = X.iloc[:,-2]\n",
        "\n",
        "\n",
        "# train, test 분리\n",
        "#rorl\n",
        "X3_train, X3_test, rorl_train, rorl_test = ms.train_test_split(X2, y_rorl, \n",
        "                                                      test_size = 0.25, random_state = 100, stratify = y_sev )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nnLI8uXZlRo"
      },
      "source": [
        "# train 셋 (2,3,4만)\n",
        "X3_train = X3_train.iloc[:,:-1]\n",
        "rorl_train = rorl_train.iloc[:,-1]\n",
        "\n",
        "X3_test = X3_test.iloc[:,:-1]\n",
        "rorl_test = rorl_test.iloc[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_mRrxxgaUtC"
      },
      "source": [
        "모델이양"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CrwXCPkaUtc",
        "outputId": "38e65334-0533-4f71-af93-1c89f9d6e2e7"
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn import metrics   \n",
        "\n",
        "#분류->object 회귀\n",
        "start = time.time()\n",
        "\n",
        "# 부스팅 타입은 default값인 gbdt로, 학습률은 0.01로 지정 하였음\n",
        "LGB = lgb.LGBMClassifier(objective=\"regression\", boosting_type='gbdt', learning_rate = 0.01)\n",
        "\n",
        "param_list = {\"n_estimators\": list(range(10, 300, 10)),\n",
        "              \"max_depth\": list(range(4, 21, 4)),\n",
        "              \"max_features\": list(range(3, 13, 2)),\n",
        "              \"min_samples_split\": list(range(3, 13, 2))}\n",
        "\n",
        "# 하이퍼파라미터 최적화\n",
        "LGB_random_search = RandomizedSearchCV(estimator = LGB,\n",
        "                                        param_distributions = param_list,\n",
        "                                        n_iter = 10,      # 10번반복하는 lightgbm 구현 : 성능개선 시도\n",
        "                                        cv = 3,           # cross-validation 3번 반복\n",
        "                                        n_jobs = 10,\n",
        "                                        random_state=42)\n",
        "\n",
        "LGB_random_search.fit(X3_train, rorl_train)\n",
        "y_pred = LGB_random_search.predict(X3_test)\n",
        "\n",
        "#성능평가\n",
        "print('정확도 :', metrics.accuracy_score(rorl_test, y_pred))\n",
        "print(confusion_matrix(rorl_test, y_pred))\n",
        "print(classification_report(rorl_test, y_pred))\n",
        "\n",
        "print( LGB_random_search.best_params_ )\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정확도 : 0.9822770179156232\n",
            "[[1234    0   49   26]\n",
            " [   3 1266    6   12]\n",
            " [  20    1 3801   44]\n",
            " [  16    0    7 3897]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.94      0.96      1309\n",
            "           2       1.00      0.98      0.99      1287\n",
            "           3       0.98      0.98      0.98      3866\n",
            "           4       0.98      0.99      0.99      3920\n",
            "\n",
            "    accuracy                           0.98     10382\n",
            "   macro avg       0.98      0.98      0.98     10382\n",
            "weighted avg       0.98      0.98      0.98     10382\n",
            "\n",
            "{'n_estimators': 280, 'min_samples_split': 3, 'max_features': 11, 'max_depth': 20}\n",
            "0:05:28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe8BpP86aUt-"
      },
      "source": [
        "DNN 이당"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4p4FCWxb7Yc",
        "outputId": "2d7d4792-b77c-4f13-ed74-60b117d79504"
      },
      "source": [
        "rorl_train.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    11707\n",
              "3    11653\n",
              "1     3904\n",
              "0     3882\n",
              "Name: rorl, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pNOYsAdcCR-",
        "outputId": "1fc801df-2386-4813-b414-e22e57058780"
      },
      "source": [
        "rorl_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31146,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3irakaDOaUuC"
      },
      "source": [
        "# 원핫 인코딩 (One_Hot)\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(rorl_train)\n",
        "one_hot_test_labels = to_categorical(rorl_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0ueRL4FaUuQ",
        "outputId": "cc5bdd32-1d85-451c-8bc8-7574850c9e77"
      },
      "source": [
        "one_hot_train_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IGISrvUIaUuk",
        "outputId": "1a068aee-0c5a-4768-cefb-b9dc6bccf34d"
      },
      "source": [
        "start = time.time()\n",
        "# 라이브러리\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# 다중퍼셉트론2 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=65, kernel_initializer='RandomUniform',activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32, activation='tanh'))   # relu\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dense(4, activation='sigmoid'))  # softmax\n",
        "\n",
        "# 모델 학습과정 설정\n",
        "model.compile(optimizer='Adam', loss=\"binary_crossentropy\", metrics=['accuracy'])  # binary_crossentropy, categorical_crossentropy\n",
        "print(model.summary())\n",
        "\n",
        "##### 모델 학습(여기에 Train 파일 넣으세용) #####\n",
        "epochs = 20\n",
        "hist = model.fit(X3_train, one_hot_train_labels, epochs=epochs, batch_size=64)\n",
        "\n",
        "    \n",
        "# 5. 학습과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots() # Figure와 axes.Axes, 두 개의 오브젝트를 리턴\n",
        "\n",
        "acc_ax = loss_ax.twinx() # x축을 하나 더 만든다\n",
        "\n",
        "loss_ax.set_ylim([0.0, 1.0])\n",
        "acc_ax.set_ylim([0.0, 1.0])\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.title(\"DNN\")\n",
        "plt.show()\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(X3_test, one_hot_test_labels, batch_size=64)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
        "print( clock(start) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 16)                1056      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4)                 68        \n",
            "=================================================================\n",
            "Total params: 6,388\n",
            "Trainable params: 6,388\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.3328 - accuracy: 0.7240\n",
            "Epoch 2/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.2347 - accuracy: 0.8042\n",
            "Epoch 3/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.2146 - accuracy: 0.8233\n",
            "Epoch 4/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1971 - accuracy: 0.8413\n",
            "Epoch 5/20\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1847 - accuracy: 0.8512\n",
            "Epoch 6/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1775 - accuracy: 0.8572\n",
            "Epoch 7/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1711 - accuracy: 0.8635\n",
            "Epoch 8/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1636 - accuracy: 0.8708\n",
            "Epoch 9/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1608 - accuracy: 0.8726\n",
            "Epoch 10/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1580 - accuracy: 0.8750\n",
            "Epoch 11/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1521 - accuracy: 0.8790\n",
            "Epoch 12/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1479 - accuracy: 0.8831\n",
            "Epoch 13/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1463 - accuracy: 0.8853\n",
            "Epoch 14/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1403 - accuracy: 0.8927\n",
            "Epoch 15/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1380 - accuracy: 0.8940\n",
            "Epoch 16/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1365 - accuracy: 0.8958\n",
            "Epoch 17/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1335 - accuracy: 0.8981\n",
            "Epoch 18/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1332 - accuracy: 0.9005\n",
            "Epoch 19/20\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1310 - accuracy: 0.9021\n",
            "Epoch 20/20\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.1281 - accuracy: 0.9046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "163/163 [==============================] - 0s 2ms/step - loss: 0.1133 - accuracy: 0.9202\n",
            "loss_and_metrics : [0.11330248415470123, 0.9201502799987793]\n",
            "0:00:36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxSPrHqBMOvo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psAh_UKDMOqa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}